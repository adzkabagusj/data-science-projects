{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BMbuHJ-BtM7x",
        "dvkfQNoptQjB",
        "ulTrSKof0dmR",
        "ef8l2yRf0j7_",
        "C84fqRxzp0zT",
        "F_7y63q-6zk7",
        "koBHRxhvLlN_",
        "zGf9D7PI_ngs",
        "Bf4CfbxuAdJt",
        "wZi-wYNkfTFg",
        "hyoJjvAKNT1q",
        "9zYPs67GNYcN"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7321e12292c54600b43d7448f8350149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eacf49ded09344a8b07dd99fa7bd2546",
              "IPY_MODEL_6036c7f968aa4ca78a8796237394024b",
              "IPY_MODEL_2a47134d17eb483298643dd486bf6af5"
            ],
            "layout": "IPY_MODEL_861c6e27ba34454896951b414aedf013"
          }
        },
        "eacf49ded09344a8b07dd99fa7bd2546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_212c439007ef43369bd589722502b03c",
            "placeholder": "​",
            "style": "IPY_MODEL_8109c3f0e6394e6db6885049f6ee2e5e",
            "value": "Loading Checkpoints: 100%"
          }
        },
        "6036c7f968aa4ca78a8796237394024b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00656e191c14deebc84dae1aa1b8d79",
            "max": 4354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2669458128a4985ad1dc44ee72742e5",
            "value": 4354
          }
        },
        "2a47134d17eb483298643dd486bf6af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11a29a1cfcef4f0ba09b39801b415ed8",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d127b4cef84e57af690dfed7951c2e",
            "value": " 4354/4354 [00:00&lt;00:00, 4717.91it/s]"
          }
        },
        "861c6e27ba34454896951b414aedf013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "212c439007ef43369bd589722502b03c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8109c3f0e6394e6db6885049f6ee2e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00656e191c14deebc84dae1aa1b8d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2669458128a4985ad1dc44ee72742e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11a29a1cfcef4f0ba09b39801b415ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d127b4cef84e57af690dfed7951c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b869caa9e24d74a81728fbcde45fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e556058fcbcb4a4294e59df5886e64fe",
              "IPY_MODEL_724eddd00dcd4f27a50bd91ec3c5e2ad",
              "IPY_MODEL_115b2cdcf094423689ee540a4236ce4b"
            ],
            "layout": "IPY_MODEL_e24a8af0801142ac8e4669bd7253f032"
          }
        },
        "e556058fcbcb4a4294e59df5886e64fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ac3407855e4fe18c191d0e6ef7817d",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5c068d1c7049e68be250da04672946",
            "value": "Calculating Embeddings: 100%"
          }
        },
        "724eddd00dcd4f27a50bd91ec3c5e2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6879585291a44506bc6973e0759a8e08",
            "max": 4354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6dd455975654929b902f11b0bf488d2",
            "value": 4354
          }
        },
        "115b2cdcf094423689ee540a4236ce4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43faa93273e4e0e8f3562e7fb1d0b88",
            "placeholder": "​",
            "style": "IPY_MODEL_23089bc31b504118b7f421238f62a301",
            "value": " 4354/4354 [44:08&lt;00:00,  1.15s/it]"
          }
        },
        "e24a8af0801142ac8e4669bd7253f032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ac3407855e4fe18c191d0e6ef7817d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5c068d1c7049e68be250da04672946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6879585291a44506bc6973e0759a8e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6dd455975654929b902f11b0bf488d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d43faa93273e4e0e8f3562e7fb1d0b88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23089bc31b504118b7f421238f62a301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "776b938694e34fe1943468fbf91a8d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ab9e036a383408ab49700f8d974f927",
              "IPY_MODEL_bfd50cacbbbd4dffaa2f2abe96343be9",
              "IPY_MODEL_1e6a3aa77fe44182a43f679576eac340"
            ],
            "layout": "IPY_MODEL_1915bc31b7244f32bf5bfba8850bba98"
          }
        },
        "9ab9e036a383408ab49700f8d974f927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81e7ea6a3acf49f98db6c7a004def16f",
            "placeholder": "​",
            "style": "IPY_MODEL_a870cce3712444b4807f66de9e0e2e56",
            "value": "Calculating Similarity: 100%"
          }
        },
        "bfd50cacbbbd4dffaa2f2abe96343be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63047605e914969aa13a75a0aaa3108",
            "max": 410691,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afcb4fb49cc8409fab577e286fb55c61",
            "value": 410691
          }
        },
        "1e6a3aa77fe44182a43f679576eac340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bc821acf9847ecb90d5c8924661925",
            "placeholder": "​",
            "style": "IPY_MODEL_ca34e723e83b45fe9bffcfdefe43eba8",
            "value": " 410691/410691 [04:28&lt;00:00, 1305.87it/s]"
          }
        },
        "1915bc31b7244f32bf5bfba8850bba98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e7ea6a3acf49f98db6c7a004def16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a870cce3712444b4807f66de9e0e2e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b63047605e914969aa13a75a0aaa3108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afcb4fb49cc8409fab577e286fb55c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2bc821acf9847ecb90d5c8924661925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca34e723e83b45fe9bffcfdefe43eba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9bb0472b5a47dfa59f0a563cf2c696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_134276b04eed45bf9d4fab6b1c28bfb8",
              "IPY_MODEL_e6ec0a52db2946dfb728768332e3ba64",
              "IPY_MODEL_bb428071b94e4f75a353b4e31c923a85"
            ],
            "layout": "IPY_MODEL_528974af71ab4cc6a9d54ccba66557fb"
          }
        },
        "134276b04eed45bf9d4fab6b1c28bfb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc38cafe8d314083a5dc3e5b524113ff",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9592c02e1f4977b234dcb0a0325fab",
            "value": "Calculating Similarity: 100%"
          }
        },
        "e6ec0a52db2946dfb728768332e3ba64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96940619509a4146a70fa6fe10666f00",
            "max": 336021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_526ebefcddec45b499bd16c7924cf1f8",
            "value": 336021
          }
        },
        "bb428071b94e4f75a353b4e31c923a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585ab793cb9c4825be357e350ffdd3f9",
            "placeholder": "​",
            "style": "IPY_MODEL_dd75de9c8f3a49a18a2eeec9e60f65a4",
            "value": " 336021/336021 [03:39&lt;00:00, 1282.97it/s]"
          }
        },
        "528974af71ab4cc6a9d54ccba66557fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc38cafe8d314083a5dc3e5b524113ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9592c02e1f4977b234dcb0a0325fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96940619509a4146a70fa6fe10666f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526ebefcddec45b499bd16c7924cf1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "585ab793cb9c4825be357e350ffdd3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd75de9c8f3a49a18a2eeec9e60f65a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c9b32e8fe7a4b54bd3e0aab54f42d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a115d28e445644b68a9922e293fe9338",
              "IPY_MODEL_94fe5cb76756460f8aa13a04aa64735b",
              "IPY_MODEL_d8db7b6758f44a3b9ce66795c3159095"
            ],
            "layout": "IPY_MODEL_00404e5f36f5439e80aa7aef19f0a6c0"
          }
        },
        "a115d28e445644b68a9922e293fe9338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_610d46faeeaf43c09ecaded942d1df26",
            "placeholder": "​",
            "style": "IPY_MODEL_e96f48865c01439fa2dc3505cb5a6f93",
            "value": "config.json: 100%"
          }
        },
        "94fe5cb76756460f8aa13a04aa64735b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b77e8179c254ff5827a37eddd7f98a5",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed06a49d83d5490ebed92b1468280191",
            "value": 385
          }
        },
        "d8db7b6758f44a3b9ce66795c3159095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8c8f48ef834978b4b27074ef89b632",
            "placeholder": "​",
            "style": "IPY_MODEL_68fc96c230234143a0a07ae762e5daf3",
            "value": " 385/385 [00:00&lt;00:00, 19.3kB/s]"
          }
        },
        "00404e5f36f5439e80aa7aef19f0a6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610d46faeeaf43c09ecaded942d1df26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96f48865c01439fa2dc3505cb5a6f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b77e8179c254ff5827a37eddd7f98a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed06a49d83d5490ebed92b1468280191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef8c8f48ef834978b4b27074ef89b632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68fc96c230234143a0a07ae762e5daf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7e42a88752949ab9b879f870674cb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e02afe75b64f4e359864c65c82b891ca",
              "IPY_MODEL_f9b057b39ecc40158b6c2b2c6befffd5",
              "IPY_MODEL_af5c5a33eb6048918d7abccfad415dfd"
            ],
            "layout": "IPY_MODEL_ba1abad662b74ae5b096171ad939c0f9"
          }
        },
        "e02afe75b64f4e359864c65c82b891ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a0952637b947aa9e907b1ae6e892b3",
            "placeholder": "​",
            "style": "IPY_MODEL_a831f3f64ce0484e924dd618581ecf48",
            "value": "vocab.txt: 100%"
          }
        },
        "f9b057b39ecc40158b6c2b2c6befffd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa7828a5d0546598f799f3b7dea2b8e",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af232549f3a54425a7a21a7ac9e29313",
            "value": 227845
          }
        },
        "af5c5a33eb6048918d7abccfad415dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a9945503b94117ab4ed426813a08d9",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e197d7863c438896a130560de3e176",
            "value": " 228k/228k [00:00&lt;00:00, 4.41MB/s]"
          }
        },
        "ba1abad662b74ae5b096171ad939c0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a0952637b947aa9e907b1ae6e892b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a831f3f64ce0484e924dd618581ecf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa7828a5d0546598f799f3b7dea2b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af232549f3a54425a7a21a7ac9e29313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66a9945503b94117ab4ed426813a08d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e197d7863c438896a130560de3e176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09357792143c40bfacf2309e2348966a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e35a5cb6506247e291f71c0a24ff5a26",
              "IPY_MODEL_586463985b364cb6a780f1004c8723a3",
              "IPY_MODEL_1bdc26e3de304cdf80f75ebac00e28c3"
            ],
            "layout": "IPY_MODEL_ffeda570f3c745f48d0a6af33a5b03cf"
          }
        },
        "e35a5cb6506247e291f71c0a24ff5a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec50b02c01d4b49a4984203689b7250",
            "placeholder": "​",
            "style": "IPY_MODEL_eaac03911e3644078686986f1772d4fe",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "586463985b364cb6a780f1004c8723a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6df432fc6e4c94ba7a61826799f11e",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2f06f31cf464edcabf6a2f6900e057d",
            "value": 442221694
          }
        },
        "1bdc26e3de304cdf80f75ebac00e28c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf2c82cb2b341279dc96a69331c73ba",
            "placeholder": "​",
            "style": "IPY_MODEL_fb8ecd1b94d04c3a968d7328294625c7",
            "value": " 442M/442M [00:05&lt;00:00, 155MB/s]"
          }
        },
        "ffeda570f3c745f48d0a6af33a5b03cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec50b02c01d4b49a4984203689b7250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaac03911e3644078686986f1772d4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b6df432fc6e4c94ba7a61826799f11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2f06f31cf464edcabf6a2f6900e057d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdf2c82cb2b341279dc96a69331c73ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb8ecd1b94d04c3a968d7328294625c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aabe8b4ecafe4852a9f8099f154ec11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_624f7256b53948ee8848e77fbf04a6c3",
              "IPY_MODEL_4b372fda4ddd490fb3b529449035467f",
              "IPY_MODEL_c4a5d33d9d78411eb4cb4c61bec09a7a"
            ],
            "layout": "IPY_MODEL_ff0cc3e043ab40cf844b4cbcff06ed42"
          }
        },
        "624f7256b53948ee8848e77fbf04a6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475df0b23b43408cbdc582e62d4e4bf4",
            "placeholder": "​",
            "style": "IPY_MODEL_a7c07b11ed1b4c4795608115a52e3220",
            "value": "model.safetensors: 100%"
          }
        },
        "4b372fda4ddd490fb3b529449035467f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64853a7c4d21491ebf15c9178e1e1613",
            "max": 442197096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_874fe5be17ed4f2c91fc98847f79544b",
            "value": 442197096
          }
        },
        "c4a5d33d9d78411eb4cb4c61bec09a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e0f07841adc4be9ba040af2ec0fcb4f",
            "placeholder": "​",
            "style": "IPY_MODEL_a431d2e0e3fb4e64953b5c7501166445",
            "value": " 442M/442M [00:03&lt;00:00, 145MB/s]"
          }
        },
        "ff0cc3e043ab40cf844b4cbcff06ed42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475df0b23b43408cbdc582e62d4e4bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7c07b11ed1b4c4795608115a52e3220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64853a7c4d21491ebf15c9178e1e1613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "874fe5be17ed4f2c91fc98847f79544b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e0f07841adc4be9ba040af2ec0fcb4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a431d2e0e3fb4e64953b5c7501166445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79762e6e48fd4b5f871a5317d90f7f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3989a1a29f3b48299f33f6bc57c3bde0",
              "IPY_MODEL_df15ab1a65a546dc870e7c37f89e2637",
              "IPY_MODEL_c6e38ce681ff49dd9a39c815eaaedb86"
            ],
            "layout": "IPY_MODEL_03ad2b2a2e5e4c019b194ac91ea847aa"
          }
        },
        "3989a1a29f3b48299f33f6bc57c3bde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f473c095635e47648e15c369c0f9a146",
            "placeholder": "​",
            "style": "IPY_MODEL_a2e8362380e84fbf922108bc3b703f60",
            "value": "Loading SciBERT Checkpoints: 100%"
          }
        },
        "df15ab1a65a546dc870e7c37f89e2637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f4696080ca42c98905a05b9a3dfd90",
            "max": 4354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb20adbf2b3945cfab9beee3cf6b7bb6",
            "value": 4354
          }
        },
        "c6e38ce681ff49dd9a39c815eaaedb86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b517e704754b405c9be23eb0c48d60f9",
            "placeholder": "​",
            "style": "IPY_MODEL_e490c8e19e3c45f5b5d5ddcd4cfb0be1",
            "value": " 4354/4354 [00:00&lt;00:00, 4936.13it/s]"
          }
        },
        "03ad2b2a2e5e4c019b194ac91ea847aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f473c095635e47648e15c369c0f9a146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e8362380e84fbf922108bc3b703f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f4696080ca42c98905a05b9a3dfd90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb20adbf2b3945cfab9beee3cf6b7bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b517e704754b405c9be23eb0c48d60f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e490c8e19e3c45f5b5d5ddcd4cfb0be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad272040b83545959a980c4e87130ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a34fb6016514e21b99e2deee33116a9",
              "IPY_MODEL_adcb57d49af54a899bab46564740f2b4",
              "IPY_MODEL_abc6b8e6cd814545b2509e96ce87ffb2"
            ],
            "layout": "IPY_MODEL_d2d19fe4af164c48be6c25230786920b"
          }
        },
        "6a34fb6016514e21b99e2deee33116a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b2340468124960a6814fa0d5763c80",
            "placeholder": "​",
            "style": "IPY_MODEL_70471ccd64114dfcb7edc1836f6d2f67",
            "value": "Calculating SciBERT Embeddings: 100%"
          }
        },
        "adcb57d49af54a899bab46564740f2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1409fd1d1ab4440fbb9ec2597a345c78",
            "max": 4354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_365f1e2ed47f40baa5401e8f3ca08870",
            "value": 4354
          }
        },
        "abc6b8e6cd814545b2509e96ce87ffb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1850aecc37bf4383951e1900e0c75ff5",
            "placeholder": "​",
            "style": "IPY_MODEL_64358a8811094c699c8c64d1261eeff7",
            "value": " 4354/4354 [47:59&lt;00:00,  1.29s/it]"
          }
        },
        "d2d19fe4af164c48be6c25230786920b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b2340468124960a6814fa0d5763c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70471ccd64114dfcb7edc1836f6d2f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1409fd1d1ab4440fbb9ec2597a345c78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365f1e2ed47f40baa5401e8f3ca08870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1850aecc37bf4383951e1900e0c75ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64358a8811094c699c8c64d1261eeff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8609da6e0e554904b89eccb0228691fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d0ed2129ce9408c9c413b873ee48192",
              "IPY_MODEL_d27204351de24d6d8ebfb92efbf19d75",
              "IPY_MODEL_405a4d2c3fdb46a3918b36d63e7035d7"
            ],
            "layout": "IPY_MODEL_d8de2e3603c34d83b00b379ba91f05f3"
          }
        },
        "3d0ed2129ce9408c9c413b873ee48192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89f241962bb04fb7b8ac2e78d2541889",
            "placeholder": "​",
            "style": "IPY_MODEL_186cddc2b5594928843f83c59b10a62c",
            "value": "Calculating Similarity: 100%"
          }
        },
        "d27204351de24d6d8ebfb92efbf19d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02d39b9f9fa4c4da8c4bf49d4f74a19",
            "max": 410691,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfc0d27d67314138a1d7a970ead95e58",
            "value": 410691
          }
        },
        "405a4d2c3fdb46a3918b36d63e7035d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd9716269c3343fda48d630cf0b0d6aa",
            "placeholder": "​",
            "style": "IPY_MODEL_6a6f76535b16465cb75eaadf9996400b",
            "value": " 410691/410691 [04:19&lt;00:00, 1678.73it/s]"
          }
        },
        "d8de2e3603c34d83b00b379ba91f05f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f241962bb04fb7b8ac2e78d2541889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186cddc2b5594928843f83c59b10a62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d02d39b9f9fa4c4da8c4bf49d4f74a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc0d27d67314138a1d7a970ead95e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd9716269c3343fda48d630cf0b0d6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6f76535b16465cb75eaadf9996400b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32ce1b82b03542c48e7dd65ece2cd978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba107ad84ca14c7eb0f520334bf1dc9c",
              "IPY_MODEL_3bb471cff5f141458e115ce864c032df",
              "IPY_MODEL_15b40ee28004423f9898d0ff1b46baa4"
            ],
            "layout": "IPY_MODEL_28021b14b12743a3a2c8d9726fb1c8b0"
          }
        },
        "ba107ad84ca14c7eb0f520334bf1dc9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7187e902e2ed4a60b93c2be267dcb1bf",
            "placeholder": "​",
            "style": "IPY_MODEL_bd6a40134863496a9b95d00db25acb42",
            "value": "Calculating Similarity: 100%"
          }
        },
        "3bb471cff5f141458e115ce864c032df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5fcfc3ec80464c95f173527b98087f",
            "max": 336021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6391222275d4a31adc50a56cab6877c",
            "value": 336021
          }
        },
        "15b40ee28004423f9898d0ff1b46baa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe367f4c3ee540aeae8430034976b3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_ce315401820943aa9f8df1537ea4baf1",
            "value": " 336021/336021 [03:31&lt;00:00, 1792.63it/s]"
          }
        },
        "28021b14b12743a3a2c8d9726fb1c8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7187e902e2ed4a60b93c2be267dcb1bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd6a40134863496a9b95d00db25acb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c5fcfc3ec80464c95f173527b98087f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6391222275d4a31adc50a56cab6877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe367f4c3ee540aeae8430034976b3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce315401820943aa9f8df1537ea4baf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78bee6887585478a826a3292a1b804e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ec070e8995a405191a5234b1fbdea6b",
              "IPY_MODEL_60b18c577989444e9543eb9656bd402b",
              "IPY_MODEL_e5de8e4d14e54d3d90f758f83029af79"
            ],
            "layout": "IPY_MODEL_385ba62231c540308ffc5b7a31fa6cff"
          }
        },
        "9ec070e8995a405191a5234b1fbdea6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93ffc7ab2e484abd975c81104e15423b",
            "placeholder": "​",
            "style": "IPY_MODEL_e5d6a808cf5242038442f8e2a5401f54",
            "value": "Processing papers: "
          }
        },
        "60b18c577989444e9543eb9656bd402b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4003155337745e480320b432ce6d1a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d7b3d27914349e6af99de2ca9c9bb26",
            "value": 0
          }
        },
        "e5de8e4d14e54d3d90f758f83029af79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a9cd7d24674322a4688ee578e7459f",
            "placeholder": "​",
            "style": "IPY_MODEL_04fc9282265c4ab799828b5b5bcce274",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "385ba62231c540308ffc5b7a31fa6cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ffc7ab2e484abd975c81104e15423b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d6a808cf5242038442f8e2a5401f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4003155337745e480320b432ce6d1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5d7b3d27914349e6af99de2ca9c9bb26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7a9cd7d24674322a4688ee578e7459f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fc9282265c4ab799828b5b5bcce274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b5605b8d6974722b0b662823e472f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164980e770df4155ac6a079835567e23",
              "IPY_MODEL_b31222af777d4423a84b2dd2a9a1f45e",
              "IPY_MODEL_bd552b16f8a444aea4d5e441f2e171e0"
            ],
            "layout": "IPY_MODEL_1a787ecac1604b54ba680c09dfe8aa95"
          }
        },
        "164980e770df4155ac6a079835567e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_889bcceaabe94b249647f18d6641cdfa",
            "placeholder": "​",
            "style": "IPY_MODEL_96179168aac74229964ad8c96b694f59",
            "value": "Cleaning paper content: 100%"
          }
        },
        "b31222af777d4423a84b2dd2a9a1f45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca735f5743d447c681611f72973d7e0f",
            "max": 4354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b935033e8d74583991f826af5835f62",
            "value": 4354
          }
        },
        "bd552b16f8a444aea4d5e441f2e171e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c5d2866c804bc48a5b0bb0d27fdafa",
            "placeholder": "​",
            "style": "IPY_MODEL_1edcd1d94aa14d1eaace0fed7304349e",
            "value": " 4354/4354 [00:49&lt;00:00, 59.31it/s]"
          }
        },
        "1a787ecac1604b54ba680c09dfe8aa95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889bcceaabe94b249647f18d6641cdfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96179168aac74229964ad8c96b694f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca735f5743d447c681611f72973d7e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b935033e8d74583991f826af5835f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86c5d2866c804bc48a5b0bb0d27fdafa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edcd1d94aa14d1eaace0fed7304349e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "BMbuHJ-BtM7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFRF0GnviWqJ",
        "outputId": "0f52f25e-e154-498c-c50a-76d7bb44f1a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdefGD3Eq7IM",
        "outputId": "e60923a8-78c0-487c-b306-4f0500684bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import lightgbm as lgb\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import catboost as cb\n",
        "import xgboost as xgb\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sentence_transformers import CrossEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "dvkfQNoptQjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Path\n",
        "base_path = '/content/drive/MyDrive/gammafest25/'\n",
        "\n",
        "train_file = base_path + 'train.csv'\n",
        "test_file = base_path + 'test.csv'\n",
        "metadata_file = base_path + 'papers_metadata.csv'\n",
        "submission_file = base_path + 'sample_submission.csv'\n",
        "paper_folder = base_path + 'Paper Database/'\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    metadata_df = pd.read_csv(metadata_file)\n",
        "    sample_submission_df = pd.read_csv(submission_file)\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(f\"Train shape: {train_df.shape}\")\n",
        "    print(f\"Test shape: {test_df.shape}\")\n",
        "    print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading files: {e}\")\n",
        "    print(\"Pastikan path file sudah benar dan file sudah diupload/tersedia.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2VT0HsNrVSA",
        "outputId": "b4c0acdf-ed9f-4b3a-8dbf-5d937bbf949a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "Train shape: (410691, 3)\n",
            "Test shape: (336021, 3)\n",
            "Metadata shape: (4354, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging dataset with metadata_df"
      ],
      "metadata": {
        "id": "w8ZBG3AQZFGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_df"
      ],
      "metadata": {
        "id": "ulTrSKof0dmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Merging metadata onto train_df...\")\n",
        "# Merge untuk paper_id (p1)\n",
        "train_merged = pd.merge(train_df, metadata_df, left_on='paper', right_on='paper_id', how='left', suffixes=('', '_p1_meta'))\n",
        "# Merge untuk referenced_paper_id (p2)\n",
        "train_merged = pd.merge(train_merged, metadata_df, left_on='referenced_paper', right_on='paper_id', how='left', suffixes=('_p1', '_p2'))\n",
        "# Drop kolom paper_id_p1 dan paper_id_p2 yang duplikat dari merge kedua\n",
        "if 'paper_id_p1' in train_merged.columns:\n",
        "    train_merged = train_merged.drop(columns=['paper_id_p1'])\n",
        "if 'paper_id_p2' in train_merged.columns:\n",
        "    train_merged = train_merged.drop(columns=['paper_id_p2'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTtNqStby7az",
        "outputId": "af4d9b29-3081-469d-c371-3614fbd9eaef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging metadata onto train_df...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train merged shape: {train_merged.shape}\")\n",
        "train_merged.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "iiUMrlGhz8IX",
        "outputId": "531e4080-5de6-4e0a-f617-e09656a60ab9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train merged shape: (410691, 19)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   paper referenced_paper  is_referenced  \\\n",
              "0  p2128            p3728              0   \n",
              "1  p0389            p3811              0   \n",
              "2  p1298            p3760              0   \n",
              "3  p0211            p1808              0   \n",
              "4  p0843            p2964              0   \n",
              "\n",
              "                                              doi_p1  \\\n",
              "0   https://doi.org/10.18653/v1/2021.findings-acl.84   \n",
              "1  https://doi.org/10.1016/b978-1-55860-377-6.500...   \n",
              "2         https://doi.org/10.1109/tpami.2016.2644615   \n",
              "3         https://doi.org/10.1109/tpami.2017.2699184   \n",
              "4         https://doi.org/10.1007/s11831-021-09694-4   \n",
              "\n",
              "                                            title_p1  publication_year_p1  \\\n",
              "0   A Survey of Data Augmentation Approaches for NLP                 2021   \n",
              "1  Residual Algorithms: Reinforcement Learning wi...                 1995   \n",
              "2  SegNet: A Deep Convolutional Encoder-Decoder A...                 2017   \n",
              "3  DeepLab: Semantic Image Segmentation with Deep...                 2017   \n",
              "4  Particle Swarm Optimization Algorithm and Its ...                 2022   \n",
              "\n",
              "  publication_date_p1  cited_by_count_p1       type_p1  \\\n",
              "0            1/1/2021                357       article   \n",
              "1            1/1/1995                981  book-chapter   \n",
              "2            1/2/2017              16255       article   \n",
              "3           4/27/2017              18641       article   \n",
              "4           4/19/2022                799        review   \n",
              "\n",
              "                                          authors_p1  \\\n",
              "0  Steven Y. Feng; Varun Gangal; Jason Wei; Sarat...   \n",
              "1                                    Leemon C. Baird   \n",
              "2  Vijay Badrinarayanan; A. C. Kendall; Roberto C...   \n",
              "3  Liang-Chieh Chen; George Papandreou; Iasonas K...   \n",
              "4                                       Ahmed G. Gad   \n",
              "\n",
              "                                         concepts_p1  \\\n",
              "0  Computer science; Popularity; Artificial intel...   \n",
              "1  Residual; Algorithm; Reinforcement learning; C...   \n",
              "2  Computer science; Artificial intelligence; Ups...   \n",
              "3  Conditional random field; Artificial intellige...   \n",
              "4  Particle swarm optimization; Swarm intelligenc...   \n",
              "\n",
              "                                     doi_p2  \\\n",
              "0        https://doi.org/10.1137/16m1080173   \n",
              "1   https://doi.org/10.1109/cvpr.2019.00447   \n",
              "2    https://doi.org/10.1002/pmic.201500396   \n",
              "3  https://doi.org/10.1609/aimag.v31i3.2303   \n",
              "4        https://doi.org/10.1007/bf00114723   \n",
              "\n",
              "                                            title_p2  publication_year_p2  \\\n",
              "0  Optimization Methods for Large-Scale Machine L...                 2018   \n",
              "1  Filter Pruning via Geometric Median for Deep C...                 2019   \n",
              "2  Integrative methods for analyzing big data in ...                 2015   \n",
              "3  Building Watson: An Overview of the DeepQA Pro...                 2010   \n",
              "4  Linear Least-Squares algorithms for temporal d...                 1996   \n",
              "\n",
              "  publication_date_p2  cited_by_count_p2  type_p2  \\\n",
              "0            1/1/2018               2492  article   \n",
              "1            6/1/2019               1078  article   \n",
              "2          12/17/2015                182   review   \n",
              "3            9/1/2010               1479  article   \n",
              "4            1/1/1996                645  article   \n",
              "\n",
              "                                          authors_p2  \\\n",
              "0        Léon Bottou; Frank E. Curtis; Jorge Nocedal   \n",
              "1  Yang He; Ping Liu; Ziwei Wang; Zhilan Hu; Yi Yang   \n",
              "2  Vladimir Gligorijević; Noël Malod‐Dognin; Nata...   \n",
              "3  David Ferrucci; Eric W. Brown; Jennifer Chu‐Ca...   \n",
              "4                 Steven J. Bradtke; Andrew G. Barto   \n",
              "\n",
              "                                         concepts_p2  \n",
              "0  Computer science; Machine learning; Artificial...  \n",
              "1  FLOPS; Computer science; Convolutional neural ...  \n",
              "2  Big data; Data science; Precision medicine; Re...  \n",
              "3  Watson; Champion; IBM; Computer science; Archi...  \n",
              "4  Recursive least squares filter; Algorithm; Tem...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ab47f40-bd4c-45a2-a187-e33060579a33\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>is_referenced</th>\n",
              "      <th>doi_p1</th>\n",
              "      <th>title_p1</th>\n",
              "      <th>publication_year_p1</th>\n",
              "      <th>publication_date_p1</th>\n",
              "      <th>cited_by_count_p1</th>\n",
              "      <th>type_p1</th>\n",
              "      <th>authors_p1</th>\n",
              "      <th>concepts_p1</th>\n",
              "      <th>doi_p2</th>\n",
              "      <th>title_p2</th>\n",
              "      <th>publication_year_p2</th>\n",
              "      <th>publication_date_p2</th>\n",
              "      <th>cited_by_count_p2</th>\n",
              "      <th>type_p2</th>\n",
              "      <th>authors_p2</th>\n",
              "      <th>concepts_p2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p2128</td>\n",
              "      <td>p3728</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.18653/v1/2021.findings-acl.84</td>\n",
              "      <td>A Survey of Data Augmentation Approaches for NLP</td>\n",
              "      <td>2021</td>\n",
              "      <td>1/1/2021</td>\n",
              "      <td>357</td>\n",
              "      <td>article</td>\n",
              "      <td>Steven Y. Feng; Varun Gangal; Jason Wei; Sarat...</td>\n",
              "      <td>Computer science; Popularity; Artificial intel...</td>\n",
              "      <td>https://doi.org/10.1137/16m1080173</td>\n",
              "      <td>Optimization Methods for Large-Scale Machine L...</td>\n",
              "      <td>2018</td>\n",
              "      <td>1/1/2018</td>\n",
              "      <td>2492</td>\n",
              "      <td>article</td>\n",
              "      <td>Léon Bottou; Frank E. Curtis; Jorge Nocedal</td>\n",
              "      <td>Computer science; Machine learning; Artificial...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p0389</td>\n",
              "      <td>p3811</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1016/b978-1-55860-377-6.500...</td>\n",
              "      <td>Residual Algorithms: Reinforcement Learning wi...</td>\n",
              "      <td>1995</td>\n",
              "      <td>1/1/1995</td>\n",
              "      <td>981</td>\n",
              "      <td>book-chapter</td>\n",
              "      <td>Leemon C. Baird</td>\n",
              "      <td>Residual; Algorithm; Reinforcement learning; C...</td>\n",
              "      <td>https://doi.org/10.1109/cvpr.2019.00447</td>\n",
              "      <td>Filter Pruning via Geometric Median for Deep C...</td>\n",
              "      <td>2019</td>\n",
              "      <td>6/1/2019</td>\n",
              "      <td>1078</td>\n",
              "      <td>article</td>\n",
              "      <td>Yang He; Ping Liu; Ziwei Wang; Zhilan Hu; Yi Yang</td>\n",
              "      <td>FLOPS; Computer science; Convolutional neural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>p1298</td>\n",
              "      <td>p3760</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1109/tpami.2016.2644615</td>\n",
              "      <td>SegNet: A Deep Convolutional Encoder-Decoder A...</td>\n",
              "      <td>2017</td>\n",
              "      <td>1/2/2017</td>\n",
              "      <td>16255</td>\n",
              "      <td>article</td>\n",
              "      <td>Vijay Badrinarayanan; A. C. Kendall; Roberto C...</td>\n",
              "      <td>Computer science; Artificial intelligence; Ups...</td>\n",
              "      <td>https://doi.org/10.1002/pmic.201500396</td>\n",
              "      <td>Integrative methods for analyzing big data in ...</td>\n",
              "      <td>2015</td>\n",
              "      <td>12/17/2015</td>\n",
              "      <td>182</td>\n",
              "      <td>review</td>\n",
              "      <td>Vladimir Gligorijević; Noël Malod‐Dognin; Nata...</td>\n",
              "      <td>Big data; Data science; Precision medicine; Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p0211</td>\n",
              "      <td>p1808</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1109/tpami.2017.2699184</td>\n",
              "      <td>DeepLab: Semantic Image Segmentation with Deep...</td>\n",
              "      <td>2017</td>\n",
              "      <td>4/27/2017</td>\n",
              "      <td>18641</td>\n",
              "      <td>article</td>\n",
              "      <td>Liang-Chieh Chen; George Papandreou; Iasonas K...</td>\n",
              "      <td>Conditional random field; Artificial intellige...</td>\n",
              "      <td>https://doi.org/10.1609/aimag.v31i3.2303</td>\n",
              "      <td>Building Watson: An Overview of the DeepQA Pro...</td>\n",
              "      <td>2010</td>\n",
              "      <td>9/1/2010</td>\n",
              "      <td>1479</td>\n",
              "      <td>article</td>\n",
              "      <td>David Ferrucci; Eric W. Brown; Jennifer Chu‐Ca...</td>\n",
              "      <td>Watson; Champion; IBM; Computer science; Archi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p0843</td>\n",
              "      <td>p2964</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1007/s11831-021-09694-4</td>\n",
              "      <td>Particle Swarm Optimization Algorithm and Its ...</td>\n",
              "      <td>2022</td>\n",
              "      <td>4/19/2022</td>\n",
              "      <td>799</td>\n",
              "      <td>review</td>\n",
              "      <td>Ahmed G. Gad</td>\n",
              "      <td>Particle swarm optimization; Swarm intelligenc...</td>\n",
              "      <td>https://doi.org/10.1007/bf00114723</td>\n",
              "      <td>Linear Least-Squares algorithms for temporal d...</td>\n",
              "      <td>1996</td>\n",
              "      <td>1/1/1996</td>\n",
              "      <td>645</td>\n",
              "      <td>article</td>\n",
              "      <td>Steven J. Bradtke; Andrew G. Barto</td>\n",
              "      <td>Recursive least squares filter; Algorithm; Tem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ab47f40-bd4c-45a2-a187-e33060579a33')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ab47f40-bd4c-45a2-a187-e33060579a33 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ab47f40-bd4c-45a2-a187-e33060579a33');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-15c4070d-81a4-4f7e-ad12-ab2f49a468b7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15c4070d-81a4-4f7e-ad12-ab2f49a468b7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-15c4070d-81a4-4f7e-ad12-ab2f49a468b7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_merged"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test_df"
      ],
      "metadata": {
        "id": "ef8l2yRf0j7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Merging metadata onto test_df...\")\n",
        "# Merge untuk paper_id (p1)\n",
        "test_merged = pd.merge(test_df, metadata_df, left_on='paper', right_on='paper_id', how='left', suffixes=('', '_p1_meta'))\n",
        "# Merge untuk referenced_paper_id (p2)\n",
        "test_merged = pd.merge(test_merged, metadata_df, left_on='referenced_paper', right_on='paper_id', how='left', suffixes=('_p1', '_p2'))\n",
        "# Drop kolom paper_id_p1 dan paper_id_p2 yang duplikat dari merge kedua\n",
        "if 'paper_id_p1' in test_merged.columns:\n",
        "    test_merged = test_merged.drop(columns=['paper_id_p1'])\n",
        "if 'paper_id_p2' in test_merged.columns:\n",
        "    test_merged = test_merged.drop(columns=['paper_id_p2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEcXgwe0k8S",
        "outputId": "23f1b4d4-93bb-4fac-95c2-9e633392e71e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging metadata onto test_df...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"test merged shape: {test_merged.shape}\")\n",
        "test_merged.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "hZZ6qU8Z0qsY",
        "outputId": "b0acf5e8-7d93-4ea1-d3ea-945a0e57763b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test merged shape: (336021, 19)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  paper referenced_paper                                        doi_p1  \\\n",
              "0   0  p0913            p3488     https://doi.org/10.19173/irrodl.v11i1.769   \n",
              "1   1  p2971            p4337             https://doi.org/10.1063/1.5136351   \n",
              "2   2  p2237            p1610  https://doi.org/10.1371/journal.pcbi.1003919   \n",
              "3   3  p2876            p3212  https://doi.org/10.1371/journal.pone.0225015   \n",
              "4   4  p2939            p1901          https://doi.org/10.18653/v1/s16-1001   \n",
              "\n",
              "                                            title_p1  publication_year_p1  \\\n",
              "0  Profiles in self-regulated learning in the onl...                 2010   \n",
              "1  Feature engineering and symbolic regression me...                 2020   \n",
              "2  Bayesian Inference of Sampled Ancestor Trees f...                 2014   \n",
              "3  Towards implementation of AI in New Zealand na...                 2020   \n",
              "4  SemEval-2016 Task 4: Sentiment Analysis in Twi...                 2016   \n",
              "\n",
              "  publication_date_p1  cited_by_count_p1  type_p1  \\\n",
              "0            3/5/2010                401  article   \n",
              "1            1/1/2020                 74  article   \n",
              "2           12/4/2014                349  article   \n",
              "3           4/10/2020                 21  article   \n",
              "4            1/1/2016                592  article   \n",
              "\n",
              "                                          authors_p1  \\\n",
              "0  Lucy Barnard‐Brak; Valerie Osland Paton; Willi...   \n",
              "1  Harsha Vaddireddy; Adil Rasheed; Anne Staples;...   \n",
              "2  Alexandra Gavryushkina; David Welch; Tanja Sta...   \n",
              "3  Li Xie; Song Yang; David Squirrell; Ehsan Vaghefi   \n",
              "4  Preslav Nakov; Alan Ritter; Sara Rosenthal; Fa...   \n",
              "\n",
              "                                         concepts_p1  \\\n",
              "0  Self-regulated learning; Likert scale; Psychol...   \n",
              "1  Physics; Statistical physics; Feature (linguis...   \n",
              "2  Ancestor; Phylogenetic tree; Inference; Markov...   \n",
              "3  Computer science; Convolutional neural network...   \n",
              "4  SemEval; Sentiment analysis; Computer science;...   \n",
              "\n",
              "                                              doi_p2  \\\n",
              "0          https://doi.org/10.19173/irrodl.v12i3.890   \n",
              "1          https://doi.org/10.3102/0013189x018001032   \n",
              "2  https://doi.org/10.1163/2210-7975_hrd-9927-201...   \n",
              "3                 https://doi.org/10.1007/bfb0103798   \n",
              "4         https://doi.org/10.1109/icarcv.2006.345351   \n",
              "\n",
              "                                            title_p2  publication_year_p2  \\\n",
              "0   Three generations of distance education pedagogy                 2011   \n",
              "1     Situated Cognition and the Culture of Learning                 1989   \n",
              "2            The Millennium Development Goals Report                 2018   \n",
              "3  Branching and interacting particle systems app...                 2000   \n",
              "4  Decentralized Reinforcement Learning Control o...                 2006   \n",
              "\n",
              "  publication_date_p2  cited_by_count_p2       type_p2  \\\n",
              "0           3/25/2011                933       article   \n",
              "1            1/1/1989              12629       article   \n",
              "2           8/17/2018               3729       dataset   \n",
              "3            1/1/2000                274  book-chapter   \n",
              "4            1/1/2006                 51       article   \n",
              "\n",
              "                                         authors_p2  \\\n",
              "0                          Terry Anderson; Jon Dron   \n",
              "1      John Seely Brown; Allan Collins; Paul Duguid   \n",
              "2                                               NaN   \n",
              "3                   Pierre Del Moral; Laurent Miclo   \n",
              "4  Lucian Buşoniu; Bart De Schutter; Robert Babuška   \n",
              "\n",
              "                                         concepts_p2  \n",
              "0  Distance education; Pedagogy; Community of inq...  \n",
              "1  Situated; Situated cognition; Cognitive appren...  \n",
              "2  Millennium Development Goals; Geography; Envir...  \n",
              "3  Mathematics; Feynman diagram; Particle system;...  \n",
              "4  Reinforcement learning; Variety (cybernetics);...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba8a237d-f007-4768-b188-3e31eaa146fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>paper</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>doi_p1</th>\n",
              "      <th>title_p1</th>\n",
              "      <th>publication_year_p1</th>\n",
              "      <th>publication_date_p1</th>\n",
              "      <th>cited_by_count_p1</th>\n",
              "      <th>type_p1</th>\n",
              "      <th>authors_p1</th>\n",
              "      <th>concepts_p1</th>\n",
              "      <th>doi_p2</th>\n",
              "      <th>title_p2</th>\n",
              "      <th>publication_year_p2</th>\n",
              "      <th>publication_date_p2</th>\n",
              "      <th>cited_by_count_p2</th>\n",
              "      <th>type_p2</th>\n",
              "      <th>authors_p2</th>\n",
              "      <th>concepts_p2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>p0913</td>\n",
              "      <td>p3488</td>\n",
              "      <td>https://doi.org/10.19173/irrodl.v11i1.769</td>\n",
              "      <td>Profiles in self-regulated learning in the onl...</td>\n",
              "      <td>2010</td>\n",
              "      <td>3/5/2010</td>\n",
              "      <td>401</td>\n",
              "      <td>article</td>\n",
              "      <td>Lucy Barnard‐Brak; Valerie Osland Paton; Willi...</td>\n",
              "      <td>Self-regulated learning; Likert scale; Psychol...</td>\n",
              "      <td>https://doi.org/10.19173/irrodl.v12i3.890</td>\n",
              "      <td>Three generations of distance education pedagogy</td>\n",
              "      <td>2011</td>\n",
              "      <td>3/25/2011</td>\n",
              "      <td>933</td>\n",
              "      <td>article</td>\n",
              "      <td>Terry Anderson; Jon Dron</td>\n",
              "      <td>Distance education; Pedagogy; Community of inq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>p2971</td>\n",
              "      <td>p4337</td>\n",
              "      <td>https://doi.org/10.1063/1.5136351</td>\n",
              "      <td>Feature engineering and symbolic regression me...</td>\n",
              "      <td>2020</td>\n",
              "      <td>1/1/2020</td>\n",
              "      <td>74</td>\n",
              "      <td>article</td>\n",
              "      <td>Harsha Vaddireddy; Adil Rasheed; Anne Staples;...</td>\n",
              "      <td>Physics; Statistical physics; Feature (linguis...</td>\n",
              "      <td>https://doi.org/10.3102/0013189x018001032</td>\n",
              "      <td>Situated Cognition and the Culture of Learning</td>\n",
              "      <td>1989</td>\n",
              "      <td>1/1/1989</td>\n",
              "      <td>12629</td>\n",
              "      <td>article</td>\n",
              "      <td>John Seely Brown; Allan Collins; Paul Duguid</td>\n",
              "      <td>Situated; Situated cognition; Cognitive appren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>p2237</td>\n",
              "      <td>p1610</td>\n",
              "      <td>https://doi.org/10.1371/journal.pcbi.1003919</td>\n",
              "      <td>Bayesian Inference of Sampled Ancestor Trees f...</td>\n",
              "      <td>2014</td>\n",
              "      <td>12/4/2014</td>\n",
              "      <td>349</td>\n",
              "      <td>article</td>\n",
              "      <td>Alexandra Gavryushkina; David Welch; Tanja Sta...</td>\n",
              "      <td>Ancestor; Phylogenetic tree; Inference; Markov...</td>\n",
              "      <td>https://doi.org/10.1163/2210-7975_hrd-9927-201...</td>\n",
              "      <td>The Millennium Development Goals Report</td>\n",
              "      <td>2018</td>\n",
              "      <td>8/17/2018</td>\n",
              "      <td>3729</td>\n",
              "      <td>dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Millennium Development Goals; Geography; Envir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>p2876</td>\n",
              "      <td>p3212</td>\n",
              "      <td>https://doi.org/10.1371/journal.pone.0225015</td>\n",
              "      <td>Towards implementation of AI in New Zealand na...</td>\n",
              "      <td>2020</td>\n",
              "      <td>4/10/2020</td>\n",
              "      <td>21</td>\n",
              "      <td>article</td>\n",
              "      <td>Li Xie; Song Yang; David Squirrell; Ehsan Vaghefi</td>\n",
              "      <td>Computer science; Convolutional neural network...</td>\n",
              "      <td>https://doi.org/10.1007/bfb0103798</td>\n",
              "      <td>Branching and interacting particle systems app...</td>\n",
              "      <td>2000</td>\n",
              "      <td>1/1/2000</td>\n",
              "      <td>274</td>\n",
              "      <td>book-chapter</td>\n",
              "      <td>Pierre Del Moral; Laurent Miclo</td>\n",
              "      <td>Mathematics; Feynman diagram; Particle system;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>p2939</td>\n",
              "      <td>p1901</td>\n",
              "      <td>https://doi.org/10.18653/v1/s16-1001</td>\n",
              "      <td>SemEval-2016 Task 4: Sentiment Analysis in Twi...</td>\n",
              "      <td>2016</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>592</td>\n",
              "      <td>article</td>\n",
              "      <td>Preslav Nakov; Alan Ritter; Sara Rosenthal; Fa...</td>\n",
              "      <td>SemEval; Sentiment analysis; Computer science;...</td>\n",
              "      <td>https://doi.org/10.1109/icarcv.2006.345351</td>\n",
              "      <td>Decentralized Reinforcement Learning Control o...</td>\n",
              "      <td>2006</td>\n",
              "      <td>1/1/2006</td>\n",
              "      <td>51</td>\n",
              "      <td>article</td>\n",
              "      <td>Lucian Buşoniu; Bart De Schutter; Robert Babuška</td>\n",
              "      <td>Reinforcement learning; Variety (cybernetics);...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba8a237d-f007-4768-b188-3e31eaa146fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba8a237d-f007-4768-b188-3e31eaa146fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba8a237d-f007-4768-b188-3e31eaa146fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a69f585-c298-4b36-8efe-6c4db72440f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a69f585-c298-4b36-8efe-6c4db72440f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a69f585-c298-4b36-8efe-6c4db72440f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_merged"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### metadata_df"
      ],
      "metadata": {
        "id": "C84fqRxzp0zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "_TXgosu2p2zC",
        "outputId": "d534d26c-bf2a-40a4-b44b-6944774690d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  paper_id                                                doi  \\\n",
              "0    p0000  https://doi.org/10.1161/circulationaha.115.001593   \n",
              "1    p0001         https://doi.org/10.1504/ijmmno.2013.055204   \n",
              "2    p0002          https://doi.org/10.1109/icip.2017.8296547   \n",
              "3    p0003                https://doi.org/10.3115/v1/p15-1001   \n",
              "4    p0004            https://doi.org/10.1109/tpami.2007.1167   \n",
              "\n",
              "                                               title  publication_year  \\\n",
              "0                       Machine Learning in Medicine              2015   \n",
              "1  A literature survey of benchmark functions for...              2013   \n",
              "2  Abnormal event detection in videos using gener...              2017   \n",
              "3  On Using Very Large Target Vocabulary for Neur...              2015   \n",
              "4  Gaussian Process Dynamical Models for Human Mo...              2007   \n",
              "\n",
              "  publication_date  cited_by_count     type  \\\n",
              "0       11/16/2015            2662   review   \n",
              "1         1/1/2013            1138  article   \n",
              "2         9/1/2017             486  article   \n",
              "3         1/1/2015             916  article   \n",
              "4       12/20/2007            1016  article   \n",
              "\n",
              "                                             authors  \\\n",
              "0                                       Rahul C. Deo   \n",
              "1                          Momin Jamil; Xin‐She Yang   \n",
              "2  Mahdyar Ravanbakhsh; Moin Nabi; Enver Sanginet...   \n",
              "3  Sébastien Jean; Kyunghyun Cho; Roland Memisevi...   \n",
              "4  Jonathan M. Wang; David J. Fleet; Aaron Hertzmann   \n",
              "\n",
              "                                            concepts  \n",
              "0  Medicine; Medical physics; Medical education; ...  \n",
              "1  Benchmark (surveying); Set (abstract data type...  \n",
              "2  Abnormality; Computer science; Artificial inte...  \n",
              "3  Machine translation; Computer science; Vocabul...  \n",
              "4  Gaussian process; Artificial intelligence; Lat...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7cd2e747-df2b-4456-ba57-41b44e2c907f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>doi</th>\n",
              "      <th>title</th>\n",
              "      <th>publication_year</th>\n",
              "      <th>publication_date</th>\n",
              "      <th>cited_by_count</th>\n",
              "      <th>type</th>\n",
              "      <th>authors</th>\n",
              "      <th>concepts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p0000</td>\n",
              "      <td>https://doi.org/10.1161/circulationaha.115.001593</td>\n",
              "      <td>Machine Learning in Medicine</td>\n",
              "      <td>2015</td>\n",
              "      <td>11/16/2015</td>\n",
              "      <td>2662</td>\n",
              "      <td>review</td>\n",
              "      <td>Rahul C. Deo</td>\n",
              "      <td>Medicine; Medical physics; Medical education; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p0001</td>\n",
              "      <td>https://doi.org/10.1504/ijmmno.2013.055204</td>\n",
              "      <td>A literature survey of benchmark functions for...</td>\n",
              "      <td>2013</td>\n",
              "      <td>1/1/2013</td>\n",
              "      <td>1138</td>\n",
              "      <td>article</td>\n",
              "      <td>Momin Jamil; Xin‐She Yang</td>\n",
              "      <td>Benchmark (surveying); Set (abstract data type...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>p0002</td>\n",
              "      <td>https://doi.org/10.1109/icip.2017.8296547</td>\n",
              "      <td>Abnormal event detection in videos using gener...</td>\n",
              "      <td>2017</td>\n",
              "      <td>9/1/2017</td>\n",
              "      <td>486</td>\n",
              "      <td>article</td>\n",
              "      <td>Mahdyar Ravanbakhsh; Moin Nabi; Enver Sanginet...</td>\n",
              "      <td>Abnormality; Computer science; Artificial inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p0003</td>\n",
              "      <td>https://doi.org/10.3115/v1/p15-1001</td>\n",
              "      <td>On Using Very Large Target Vocabulary for Neur...</td>\n",
              "      <td>2015</td>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>916</td>\n",
              "      <td>article</td>\n",
              "      <td>Sébastien Jean; Kyunghyun Cho; Roland Memisevi...</td>\n",
              "      <td>Machine translation; Computer science; Vocabul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p0004</td>\n",
              "      <td>https://doi.org/10.1109/tpami.2007.1167</td>\n",
              "      <td>Gaussian Process Dynamical Models for Human Mo...</td>\n",
              "      <td>2007</td>\n",
              "      <td>12/20/2007</td>\n",
              "      <td>1016</td>\n",
              "      <td>article</td>\n",
              "      <td>Jonathan M. Wang; David J. Fleet; Aaron Hertzmann</td>\n",
              "      <td>Gaussian process; Artificial intelligence; Lat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cd2e747-df2b-4456-ba57-41b44e2c907f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7cd2e747-df2b-4456-ba57-41b44e2c907f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7cd2e747-df2b-4456-ba57-41b44e2c907f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5c366468-2044-49b0-8e0d-03a2beaa8d86\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c366468-2044-49b0-8e0d-03a2beaa8d86')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5c366468-2044-49b0-8e0d-03a2beaa8d86 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metadata_df",
              "summary": "{\n  \"name\": \"metadata_df\",\n  \"rows\": 4354,\n  \"fields\": [\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4354,\n        \"samples\": [\n          \"p0505\",\n          \"p1611\",\n          \"p2547\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4319,\n        \"samples\": [\n          \"https://doi.org/10.1109/cvpr.2016.91\",\n          \"https://doi.org/10.1109/iros40897.2019\",\n          \"https://doi.org/10.1109/tsmca.2009.2029559\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4241,\n        \"samples\": [\n          \"DisSent: Learning Sentence Representations from Explicit Discourse Relations\",\n          \"Convolutional 2D Knowledge Graph Embeddings\",\n          \"Explainable Reasoning over Knowledge Graphs for Recommendation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publication_year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 1887,\n        \"max\": 2025,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          2014,\n          1987,\n          2019\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publication_date\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1804,\n        \"samples\": [\n          \"3/19/2008\",\n          \"8/20/2014\",\n          \"12/31/2008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cited_by_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4649,\n        \"min\": 1,\n        \"max\": 138239,\n        \"num_unique_values\": 1864,\n        \"samples\": [\n          1564,\n          2043,\n          500\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"dissertation\",\n          \"other\",\n          \"review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4134,\n        \"samples\": [\n          \"Ali Imran; Iryna Posokhova; Haneya Naeem Qureshi; Usama Masood; Muhammad Sajid Riaz\",\n          \"Jean Charles Gilbert; Claude Lemar\\u00e9chal\",\n          \"Victor M. Corman; Olfert Landt; Marco Kaiser; Richard Molenkamp; Adam Meijer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"concepts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4299,\n        \"samples\": [\n          \"Bench to bedside; Medicine; Biomarker discovery; Personalization; Genomics\",\n          \"Beneficence; Mainstream; Engineering ethics; Autonomy; Typology\",\n          \"Discriminator; Artificial intelligence; Computer science; Similarity (geometry); Image (mathematics)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "lKV0jmQhV7yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### paper_df"
      ],
      "metadata": {
        "id": "koBHRxhvLlN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_paper(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "paper_data = []\n",
        "\n",
        "txt_files = [f for f in os.listdir(paper_folder) if f.endswith('.txt')]\n",
        "\n",
        "for filename in tqdm(txt_files, desc=\"Processing papers\"):\n",
        "    file_path = os.path.join(paper_folder, filename)\n",
        "    paper_content = process_paper(file_path)\n",
        "    if paper_content:\n",
        "        paper_data.append({'paper_id': filename[:-4], 'content': paper_content})\n",
        "\n",
        "paper_df = pd.DataFrame(paper_data)\n",
        "print(paper_df.head())\n",
        "paper_df.to_parquet(f'{base_path}/paper_df.csv', index=False)"
      ],
      "metadata": {
        "id": "UOYwhZm8qRRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "78bee6887585478a826a3292a1b804e5",
            "9ec070e8995a405191a5234b1fbdea6b",
            "60b18c577989444e9543eb9656bd402b",
            "e5de8e4d14e54d3d90f758f83029af79",
            "385ba62231c540308ffc5b7a31fa6cff",
            "93ffc7ab2e484abd975c81104e15423b",
            "e5d6a808cf5242038442f8e2a5401f54",
            "e4003155337745e480320b432ce6d1a0",
            "5d7b3d27914349e6af99de2ca9c9bb26",
            "b7a9cd7d24674322a4688ee578e7459f",
            "04fc9282265c4ab799828b5b5bcce274"
          ]
        },
        "outputId": "0348d790-6232-421b-ec22-d7505f218be3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing papers: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78bee6887585478a826a3292a1b804e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_df = pd.read_parquet(f'{base_path}/paper_df.parquet')\n",
        "paper_df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "45Sgb_RRLuTu",
        "outputId": "9f917f41-ed56-4501-c84f-54022b3685eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  paper_id                                            content\n",
              "0    p3360  Dynamical studies of transitions B y V. Dobros..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53d85267-037f-42b0-afe6-422ab6682923\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p3360</td>\n",
              "      <td>Dynamical studies of transitions B y V. Dobros...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53d85267-037f-42b0-afe6-422ab6682923')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53d85267-037f-42b0-afe6-422ab6682923 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53d85267-037f-42b0-afe6-422ab6682923');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "paper_df",
              "summary": "{\n  \"name\": \"paper_df\",\n  \"rows\": 4354,\n  \"fields\": [\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4354,\n        \"samples\": [\n          \"p3859\",\n          \"p2966\",\n          \"p1904\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4264,\n        \"samples\": [\n          \"Unifying Human and Statistical Evaluation for Natural Language Generation Tatsunori B.,2 Hugh Percy Liang1,2 (* equal contribution) 1Department of Computer Science 2Department of Statistics Stanford University {thashim,hughz}@stanford.edu How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE. On summarization and chitchat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity. Introduction Generating text is a core part of many NLP tasks such as image captioning, opendomain dialogue, story generation, and summarization. However, proper evaluation of natural language generation has proven difficult. A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, tasks like dialogue or story generation. Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity. As an example, for language Model Probability (pmodel) Agassi bows out of Australian open Agassi withdraws from Australian open Sharon has stroke for stroke Cleared coach facing another grilling from British swim bosses Model Generations Human Judgment   HUSE is twice the classification error of distinguishing reference and generated text based on human judgment scores and model probabilities. HUSE identifies samples with defects in quality (Sharon has stroke...) and diversity (Cleared coach facing...). modeling, a model that directly plagiarizes sentences from the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity. On the other hand, statistical evaluation-i.e., perplexity on a reference test set-captures diversity, as it ensures a model must assign reasonable probability to novel sentences, but perplexity provides an inadequate measure of quality. For example, modifying a perfect model by removing its ability to generate even a single test sentence results in infinite perplexity even though the model is still. Automatic metrics such as BLEU and ROUGE capture quality better than perplexity but still correlate poorly with human evaluation and fail to capture diversity. Existing approaches to combining statistical and human evaluation have been, leading to misleading performance measures. A common approach is to measure diversity through the perplexity of a probabilistic model and quality through human evaluation on outputs. This gives the illusion that a single model is and diverse, while the reality is that it shows we can have either a diverse model (when sampling from the distribution used to compute perplexity) or a model (when beamsearching). In this paper, we define the idealized evaluation metric as twice the error of the optimal discriminator for classifying sentences as coming from the reference distribution or the model (Section 2). If a model generates gibberish (low quality), the optimal discriminator can classify these accurately as coming from the model. If the reference distribution contains sentences the model cannot generate (low diversity), the optimal discriminator can classify these accurately as coming from the reference. Unfortunately, the optimal discriminator is unavailable. Human discriminators cannot capture diversity effectively, and learned discriminators- e.g., from a Generative Adversarial Network or one trained on human judgments -are too unreliable to use for rigorous evaluation. Our key result (Section 3) is based on the observation that the optimal classifier depends only on two numbers: the probability of a sentence under the model and the probability under the reference distribution. The former can be computed directly from the model, and we show that the latter can be by human judgment scores. The resulting space is illustrated in   We apply a simple neighbor classifier in this space and define Human Uni- fied with Statistical Evaluation (HUSE) as twice the -out error of this classifier. We apply HUSE to four natural language generation tasks (Section 5): language modeling, chitchat dialogue, story generation, and summarization. First, we show that human evaluation alone is insufficient to discriminate model generations from the references, leading to inflated estimates of model performance. In contrast, HUSE is able to reveal deficiencies of current models. We also show that common techniques for improving sample quality such as annealing actually increase distinguishability between the model and reference due to losses in diversity. Optimal Discriminator Consider a natural language generation task where the model is given a context x (e.g., a dialogue history) drawn from some prior p(x) and must output a distribution over possible sentences pmodel(y | x). We define an idealized evaluation metric based on whether pmodel is close to a reference distribution pref, which is generally.1 Specifically, consider a random variable y drawn from either the reference or the model based on an indicator z \\u223cBernoulli y | x, z \\u223c pref(y | x) pmodel(y | x) Define L\\u2217to be twice the lowest possible error over any discriminator f that attempts to determine z based on x and y: f P[f(x, y) \\u0338= z]. L\\u2217measures similarity between pmodel and pref; it is 0 if pmodel and pref are disjoint and 1 if they are identical.2 Obstacles. Unfortunately, L\\u2217is unattainable because it requires computing the optimal discriminator. In the spirit of the Turing Test, we could consider using the error rate of a human discriminator fhum instead, often considered the gold standard for evaluation. However, while humans might have knowledge of pref, they do not have full knowledge of pmodel and thus would have difficulties determining which sentences a model cannot As a concrete example, suppose pref placed a uniform distribution over some set S. knowledge of pmodel the most sensible discriminator is to predict z = 1 (reference) when y \\u2208S. This discriminator achieves the same classification error of 0.5 for both the perfect model pmodel = pref and one which can only return a single y \\u2208S. We could try to reveal pmodel to humans by showing multiple samples simultaneously, but this is expensive and, as we will later see, unnecessary. Another option is to learn f over an expressive class of functions such as neural networks on data 1 While some tasks only care about quality and thus only require pmodel to place mass on some high quality y, we demand that pmodel places mass on all high quality y as given by pref. This diversity is important for tasks such as dialogue or story generation. Also note that pref need not be the human distribution, or match the training distribution. It can be defined as the distribution given by experts. variational divergence: x,y p(x) |pmodel(y | x) -pref(y | x)| Appendix A.1 for details. sampled from pmodel and pref. This is analogous to learning the discriminator in a Generative Adversarial Network (GAN) or learning an evaluation metric from human judgments. However, as (x, y) are objects, training a good classi- fier is extremely difficult (and perhaps not significantly easier than solving the original generation problem). Indeed, learned evaluation metrics do not generalize very well. Unlike these approaches which seek to replace human evaluation, our focus will instead be on combining human and automatic statistical evaluation to estimate the optimal classifier Human Unified with Statistical Evaluation (HUSE) Our key result is that the optimal discriminator depends on (x, y) only through a sufficient statistic (Section 3.1), motivating an approximation which we call HUSE (Section 3.2). For any feature map \\u03c6 that maps (x, y) to \\u03c6(x, y) \\u2208Rd, define the evaluation score L(\\u03c6) to be twice the error rate of the optimal discriminator that depends on (x, y) only through \\u03c6: f P[f(\\u03c6(x, y)) \\u0338= z]. Note that the evaluation score L(\\u03c6) given by a feature map \\u03c6 optimizes over all functions that depend on \\u03c6 (3). Thus, the more information \\u03c6 contains, the lower L(\\u03c6) is. This has two implications: First, any feature map \\u03c6 yields an (optimistic) upper bound on L\\u2217(2), meaning that L(\\u03c6) might be able detect when a model is poor but cannot certify that it is good. Second, adding features to \\u03c6 can only improve this bound. Two features suffice Let us consider the following feature map: \\u03c6opt(x, y) def = [pref(y | x), pmodel(y | x)]. From the arguments above, it is clear that L(\\u03c6opt) \\u2265L\\u2217, but perhaps more surprisingly, we actually have equality: Proposition 1. The feature map \\u03c6opt achieves the optimal discriminator score: L(\\u03c6opt) = L\\u2217. We compute the true posterior over z given x, y. Since p(z = 1) = p(z = 0) = p(y | x, z = 1) = pref(y | x) and p(y | x, z = 0) = pmodel(y | x), by Bayes\\u2019 rule: p(z = 1 | x, y) = pref(y | x) pref(y | x) + pmodel(y | x). The optimal discriminator simply predicts z = 1 if pref(y | x) > pmodel(y | x) and z = 0 otherwise. In other words, the decision boundary is given by \\u03c6opt(x, y)1 > \\u03c6opt(x, y)2. More generally, we can obtain this equality with a wider class of \\u03c6. It will hold exactly for any invertible transformation of \\u03c6opt (Appendix Corollary 1), and approximately for any \\u03c6 which has high mutual information with \\u03c6opt (Appendix Theorem 1). This means that we can substitute pref with noisy, possibly estimates and still obtain accurate estimates of L\\u2217. HUSE features While we can directly compute pmodel(y | x) for many probabilistic models, pref(y | x) is unattainable, so L(\\u03c6opt) is not computable. However, the wisdom of the crowds suggests that pooling together the judgments of many humans can often produce surprisingly reliable estimates of probabilities such as pref(y | x), even if no individual human is particularly reliable. With this motivation, we ask Amazon Mechanical Turk workers to rate a sentence from 1-5 based on how \\u201ctypical\\u201d it is as a way to estimate pref(y | x). (see Appendix A.3 for more details). We define HJ(x, y) to be the average response over 20 crowdworkers.   shows that for a language modeling task on the Reddit corpus,3 HJ(x, y) strongly correlates with the actual of y in the corpus. The high correlation suggests that human judgments HJ(x, y) are a good surrogate for log pref. In addition, we found that rather than using the model probability pmodel(y | x) directly as a feature, normalizing by sentence length len(y) yielded lower (tighter) scores. We therefore define the HUSE features as follows: \\u03c6huse(x, y) def \\u0014log pmodel(y | x), HJ(x, y) 3We used the Reddit corpus due to crowdworker familiarity, corpus size, and short average sentence length, which results in a wide range of sentence frequencies. Frequency in Reddit corpus (log scale) Human Judgment (HJ) Score   On the Reddit corpus, human judgment (HJ) of the \\u201ctypicality\\u201d of a sentence y correlates strongly (r = 0.92) with its frequency in the corpus, suggesting that HJ is a good surrogate for log pref. Error bars at the 90% confidence interval. and define the (population) HUSE score as Guarantees derived from HUSE We now show that the HUSE score satisfies two nice properties: (i) HUSE does at least as well as human evaluation and (ii) a low HUSE score is sufficient to show that a model is far from the reference distribution. To show (i), consider a feature map that only includes human evaluation: \\u03c6hj(x, y) def = [HJ(x, y)]. Because \\u03c6huse also incorporates human evaluation, L(\\u03c6huse) is always tighter (lower) than the human discriminator error L(\\u03c6hj): Proposition 1 (Relationship between HUSE, human evaluation, and optimal scores). L(\\u03c6hj) \\u2265L(\\u03c6huse) \\u2265L\\u2217. Furthermore, the main difference between L(\\u03c6huse) and L\\u2217is that the former uses HJ(x, y) and the latter uses pref. But as we argued using, HJ(x, y) is strongly correlated with pref, and good approximations to pref provide approximation guarantees for L(\\u03c6huse) (Appendix Theorem 1). Evaluating models with HUSE In this section, we show how we can estimate the error rate L(\\u03c6) from finite data (Section 4.1). We then show how the HUSE estimate (\\u02c6L(\\u03c6huse)) can be decomposed into a score that measures quality ( ) and a score that measures diversity ( ), which allows us to study qualitydiversity tradeoffs (Section 4.2). Learning a discriminator For any feature map \\u03c6, we show how to produce an estimate of L(\\u03c6). Fix n contexts x1,..., xn. First, we draw n examples y1,..., yn from the reference distribution pref(y | x), which are usually sentences from a test set. We also draw n examples y\\u2032 1,..., y\\u2032 n from the model pmodel(y | x) we wish to evaluate. Next, for each of the 2n examples (x, y), we compute the feature map \\u03c6(x, y), which might involve evaluating the model probability pmodel(y | x) as well as collecting human judgments HJ(x, y) from crowdworkers. Finally, we compute the -out error of a classifier that tries to predict whether a given example (x, y) comes from the reference distribution (z = 1) or the model (z = 0). The classification problems for HUSE are twodimensional, which allows us to accurately estimate error rates using a neighbors classifier. We opt to use nearest neighbors classifiers as they are simple, require no training, and can asymptotically capture arbitrary continuous decision boundaries. Specifically, we set k = 16 and define neighbors using L2 distances over the feature vectors \\u03c6(x, y) scaled componentwise to have unit variance. The overall procedure for computing the estimate \\u02c6L(\\u03c6) is formally defined in Algorithm 1. Algorithm 1 Estimating error rates under \\u03c6 Require: Feature map \\u03c6, number of neighbors k Contexts x1,..., xn Reference outputs y1,..., yn Model outputs y\\u2032 1,..., y\\u2032 1: Construct dataset: {(\\u03c6(xi, yi), 1), (\\u03c6(xi, y\\u2032 2: \\u02c6L(\\u03c6) def = -out error of on D decomposition We now define the (empirical) HUSE score using the feature map \\u03c6huse: = \\u02c6L(\\u03c6huse). We define the quality component of HUSE ( ) similarly using human judgments def = \\u02c6L(\\u03c6hj). Since humans can detect quality defects in a model, any increase in error from removing pmodel must come from a model\\u2019s lack of diversity. Therefore, we define the diversity component ( ) as follows: def = 1 + HUSE -, which implies the decomposition (1- )+ (1 - ) = 1 -HUSE. As long as the discriminators are (obtaining better performance than chance and HUSE > ), all scores are contained in. Here, = 1 implies that the model suffers no diversity defects, while = 0 indicates that the examples could be discriminated perfectly due to a lack of diversity. Experiments Experimental setup We use HUSE to evaluate three different types of natural language generation tasks: (i) unconditional and high entropy (language modeling); (ii) conditional and high entropy (story generation, dialogue); and (iii) conditional and low entropy (summarization). We show that HUSE provides a direct and interpretable measure of diversity on tasks, while also serving as a useful model diagnostic on lowentropy ones. The four tasks along with the datasets and models are as follows: \\u2022 Summarization: Giganews story to headline dataset and the model from Gehrmann et al.. The dataset consists of 3.8 million news pairs. Examples from this dataset are shown in   \\u2022 Story generation: Last sentence generation for ROC stories consisting of 96,198 examples of partially written stories as input, and a single sentence which completes the story as the target. We use a standard OpenNMT model with global attention. The task consists of generating a single sentence from the one billion word newswire text distribution. \\u2022 dialogue: dialogue dataset consisting of 37.3 million pairs from Reddit (Appendix A.4). Comments are generally short (5-15 tokens) and cover a single topic (e.g. given \\u201cwow how did i not notice that\\u201d, the response is \\u201cyou were focusing on other things its understandable\\u201d). We train a convolutional model using fairseq (Gehring et al., For all the tasks, we train neural models and evaluate their tradeoffs as we change the decoding scheme for generation. Our primary evaluation concerns diversity involving temperature annealing which is a generation technique applicable to any probabilistic model that generates words sequentially. temperature annealed models, we sample a word w proportional to (w) where p is the model probability of w given previous words and t is the temperature parameter. We excluded beam search since it qualitatively behaves similarly to temperature annealing with low temperatures and HUSE \\u22480 due to beam search being extremely under diverse. As a baseline, we also consider retrieval based models based on Apache solr on a few tasks. For this approach, we retrieve the single most relevant response from the training set using the BM25 similarity metric on inputs. Such models are known to perform well in tasks with complex outputs such as program generation and style transfer. For cost reasons, we did not measure certain combinations of task and generation mechanisms. We did not measure retrieval for dialogue, as we observed its outputs were lower quality than a neural model. We also did not anneal language models, as the generation quality from the language model was already high, and our goal was to show that they achieved high HUSE. Our set of measurements, while not comprehensive, generally covers the available qualitydiversity tradeoffs for conditional tasks. Summarization Story generation dialogue   Performance achieved by the best models on the four tasks, as measured by overall -fit (HUSE), sample quality ( ) and diversity ( ). The scale for HUSE and ranges from 0.0 (completely distinguishable from reference) to 1.0 (indistinguishable from reference) where the implied classification error is. may exceed 1.0 with small sample sizes when > HUSE. Quality ( ) Diversity ( ) Tradeoffs between and Points are models and color indicates task. Neural models (circle) generate using temperature annealing (point labels indicate temperature). Models closer to the top right are superior, and gray diagonal lines indicate equivalent HUSE. A shaded region for a task indicates models which are strictly dominated (worse HUSE with the same - proportion). Annealing can between diversity and quality but cannot easily increase the underlying model performance (HUSE). Finally, we collect human judgments HJ(x, y) as per Section 4.1 where we query 20 Amazon Mechanical Turk crowdworkers for typicality ratings on 100 reference and 100 model sentences. Since our models generate UNK (unknown and -vocabulary) tokens, we instructed crowdworkers to treat UNK tokens as rare, but appropriate words for the context. Overall results The HUSE scores across the four tasks vary widely.   shows that language models are nearly indistinguishable, with HUSE = 0.86 and implied discriminator error of In contrast, both summarization and dialogue are highly distinguishable (HUSE \\u22480.5) with relatively low quality when sampled from t = 1.0. Human evaluation alone ( ) would suggest that using temperature annealing (t = 0.7) to emphasize outputs substantially improves the model ( goes from 0.58 to 0.92 for summarization and 0.56 to 0.92 for dialogue). However, we find that this increase in sample quality comes at the cost of diversity ( goes from 0.95 to 0.34 for summarization and 1.0 to 0.57 for dialogue). Examining the achievable HUSE and diversity tradeoffs in   shows that mechanisms such as annealing which improve sample quality actually degrade HUSE due to severe losses in diversity. We find that all generation schemes and models are inadequate for story generation on ROC stories. The original model (t = 1.0) is very easily distinguishable by a human ( = 0.15), corresponding to a discriminator error of 7%. The retrieval models can improve this to = 0.47, but this comes at the expense of diversity. Finally, we observe that directly sampling from the model (t = 1.0) is always diverse. suggests that human evaluation is an appropriate evaluation for generation systems that are directly sampled (rather than ). Model error analysis with HUSE Since HUSE is estimated from a classification problem, we can directly visualize the classification problem to understand defects in both model quality and diversity. \\u03c6huse(xi, yi) (blue squares) and model points \\u03c6huse(xi, y\\u2032 i) (red circles) for the summarization The shaded areas indicate the decision boundary of the neighbor classifier. At temperature t = 1.0, we find that the classification boundary is mostly horizontal, implying that human judgment alone can distinguish model outputs from references. There is a cluster of sentences with high HJ and high pmodel which are essentially indistinguishable. Examining the samples in this region reveals that these are news stories with short headlines such as \\u201cNadal pulls out of Sydney International\\u201d which can be   The classification problem in Algorithm 1 on the summarization task with different softmax temperatures (three panels). Each point represents a reference sentence \\u03c6huse(xi, yi) or sentence \\u03c6huse(xi, y\\u2032 i). The color denotes the source of the sentence (z), shading is the classification confidence of the nearest neighbor classifier. reliably generated even at t = 1.0. the model frequently generates low quality samples that can easily be distinguished such as \\u201ctwo new vaccines in the poor countries were effective against -alone study says\\u201d ( ). At lower temperatures of t = 0.9 and t = 0.7, the boundary shifts towards becoming diagonal. Although the distribution is no longer directly separable on human judgment, the two distributions are clearly separable with the inclusion of pmodel. Using, we can identify individual examples which were correctly and incorrectly classified based on pmodel and HJ.   shows examples of both quality failures and diversity failures identified by HUSE. For example, the \\u201cdiversity failure\\u201d table shows that the summarization model (t = 0.7) has an extremely low probability of generating some reference sentences (\\u201cNFL\\u2019s bills shake up front office\\u201d) and is thus underdiverse. Closer examination of the model shows that the probability of generating \\u201cfront office\\u201d is low, since it is an unusual way to refer to the president and general manager. Improving these models on the diversity failures will require that the model understand more subtle paraphrases. We can also identify model successes, where the model outputs are indistinguishable from the reference in terms of quality (\\u201cAgassi bows out of Australian Open after injury\\u201d), and the model assigns high probability to the reference (\\u201cAgassi withdraws from Australian Open\\u201d). HUSE stability Since HUSE depends on human crowdworker annotations, one might ask if it is possible to reduce either the number of annotated examples, or number of distinct crowdworkers for each example. We show that for models, substantially fewer annotations are needed.   Estimates of HUSE are robust to small test set size, but generally require \\u224820 crowdworker measurements for each example.   shows the result of subsampling our original data of 200 sentences and 20 crowdworkers and estimating HUSE. First, we find that using 50 test set examples (, left) is often sufficient to give accurate estimates of HUSE. Next, we find that the necessary number of crowdworkers per example depends heavily on the task. Easily distinguishable tasks (story generation), require only 10 crowdworkers, while less distinguishable tasks (summarization) require more than 20 crowdworkers to obtain accurate estimates. Related work The current state of NLG evaluation. approaches to NLG evaluation use a hodgepodge mix of quality and diversity measures. Out of the 26 NLG papers at ACL 2018, six perform only human evaluation, fourteen measure human evaluation and a diversity metric such as perplexity or diversity, and six do not evaluate using human judgments. While perplexity and counts can in principle evaluate diversity, their practical implementations suffer from serious drawbacks. When human evaluation and perplexity are both evaluated, they are almost always done on separate Quality failure log pmodel Two new vaccines have been shown effective against rotavirus, which is responsible for a infant deaths in poor countries each year, research studies published Wednesday said. Two new vaccines in the poor countries were effective against -alone study says New vaccines for key <UNK> virus shown effective Diversity failure The Buffalo Bills sacked Tom Donahoe as president and general manager on Wednesday, fulfilling expectations of a after another failure to make the National Football League playoffs. Bills sack <UNK> as president GM and general manager NFL\\u2019s Bills shake up front office. Model is indistinguishable US veteran and Grand Slam winner Andre Agassi has withdrawn from this month\\u2019s Australian Open due to a nagging ankle injury, his management team announced Agassi bows out of Australian Open after injury. Agassi withdraws from Australian Open.   Example reference and model outputs (capitalization added for readability) corresponding to   (summarization task) that were shown to crowdworkers (left column). Crowdworkers were shown samples from the model (including the <UNK> token) and returned human judgments (right column). Using human judgments and the model probability, we can identify several types of failures. Quality failures are examples that are classified by human judgment. Diversity failures are examples that are classified by model probabilities. Finally some examples are not easily classified, as they have similar human judgment and model probability scores. models-human evaluations are done on beamsearched output, while perplexity is computed on the softmax outputs. This makes it appear as if the models can simultaneously generate high quality outputs while also being diverse, when in fact they can only be one at a time based on whether they sample or run beam search. On the other hand, diversity was proposed by Li et al. to identify models with the generic utterance problem where models repeat phrases such as \\u2018I don\\u2019t know\\u2019. Unfortunately, diversity is computed across contexts by counting the number of unique generated, and so does not measure a model\\u2019s ability to generate multiple valid utterances at any single context. In particular, a model which only outputs a single memorized utterance per context (e.g., via memorization or retrieval) can still have high diversity as long as the memorized sentences differ across contexts. Finally, all existing diversity measures are computed separately from human evaluation. results in two incomparable evaluation metrics, which prevent us from reasoning about tradeoffs between diversity and quality. In contrast, HUSE allows us to make precise statements about the tradeoffs between model quality and diversity because it is a single metric which decomposes into diversity and quality terms. Related evaluations of diversity. The importance of diverse responses has previously been acknowledged for summarization and information retrieval. Our work differs in considering a single evaluation measure that captures quality and diversity applicable to any generation task. Automated metrics based on overlap such as BLEU, METEOR, ROUGE work well for machine translation but do not generalize well to domains with a diverse spectrum of correct responses. While variants have adapted such metrics to high entropy generative environments, they are still significantly inferior to the human judgments they attempt to mimic. Caccia et al. recently examined the diversity and quality tradeoffs for different language model architectures on synthetic datasets. However, as their approach relies on measuring loglikelihoods under both the model and reference distributions, it cannot be applied to real data where pref is unavailable. Our main conceptual contribution overcomes this by showing that HJ is an acceptable proxy for pref. Sajjadi et al. also examines diversity and quality (which they call precision and recall) in the context of generative image models. However, they rely on assuming that pref and pmodel can be estimated accurately using the Fr'echet Inception Distance (FID). HUSE avoids such assumptions and instead directly leverages human judgments, resulting in a simple and reliable metric more suitable for use as a. Estimating optimal classification error. Evaluating a model by estimating its optimal classification error has been considered by several earlier works. However, these methods have focused on classifying sentences directly, which is quite challenging to do reliably. Existing adversarial evaluation methods do not yet reliably outperform human classification. We propose the use of both human evaluation and model probabilities as part of the adversarial evaluation framework, and demonstrate that the resulting classifier reliably outperforms humans and captures both the sample quality and diversity of a Distributional divergence estimation. proposed evaluation metric is closely related to the total variation distance which has been studied extensively in the distribution testing literature. It is known that total variation distance estimates have pessimistic minimax estimation rates in high dimensions (Balakrishnan and Wasserman, Our work overcomes this by utilizing pmodel and an estimate of pref. Other approaches to distributional testing include the maximum mean discrepancy (MMD) and Wasserstein distances, but these approaches require knowledge of a ground truth metric or kernel space. Although such divergences are easier to estimate than the total variation distance from samples, the implied convergence rates are still too slow to be practically Discussion In this paper, we demonstrate that the current gold standard of human evaluation does not penalize models. To remedy this, we propose HUSE, a general purpose evaluation strategy which can be applied to any model for which we can calculate a model\\u2019s sampling probabilities. HUSE is an upper bound on the optimal classification error of distinguishing reference and modelgenerated text, and never does worse than human classification. HUSE leverages both model probabilities and human judgments, ensuring that models which do well on the metric are both highquality and diverse. Our work can be viewed as a \\u201csuperhuman version\\u201d of the classic Turing Test. Instead of relying on just a human classifier, we approximate the optimal classifier, which can utilize information about the model in addition to the reference. We also modify the classification problem and seek to identify whether a sample comes from a (potentially superhuman) reference distribution, rather than the human distribution. These two changes lead to tractable, rigorous estimators which can quantify tradeoffs between model quality and diversity on a wide range of generation Acknowledgements. We would like to thank Arun Chaganty, Robin Jia, and Peng Qi for extensive comments and feedback on the paper. This work was funded by DARPA CwC program under ARO prime contract no. -. Reproducibility. All code, data, and experiments codalab. / 0x88644b5ee189402eb19d39d721d1005c.\",\n          \"Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5) Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik UC Berkeley {rbg,jdonahue,trevor,malik}@eecs.berkeley.edu Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The methods are complex ensemble systems that typically combine multiple image features with context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012-achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply convolutional neural networks (CNNs) to region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised for an auxiliary task, followed by, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method: Regions with CNN features. We also compare to OverFeat, a recently proposed detector based on a similar CNN architecture. We find that outperforms OverFeat by a large margin on the ILSVRC2013 detection dataset. Source code for the complete system is available at 1. Introduction Features matter. The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT and HOG. But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection, it is generally acknowledged that progress has been slow during, with small gains obtained by building ensemble systems and employing minor variants of successful methods. SIFT and HOG are blockwise orientation histograms, a representation we could associate roughly with complex cells in V1, the first cortical area in the primate visual pathway. But we also know that recognition occurs several stages downstream, which suggests that there might be hier- 2. Extract region proposals (~2k) 3. Compute CNN features aeroplane? no. person? yes. tvmonitor? no. 4. Classify warped region: Regions with CNN features: Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using linear SVMs. achieves a mean average precision (mAP) of 53.7% on PASCAL VOC 2010. For comparison, reports 35.1% mAP using the same region proposals, but with a spatial pyramid and - approach. The popular deformable part models perform at 33.4%. On the ILSVRC2013 detection dataset, \\u2019s mAP is 31.4%, a large improvement over OverFeat, which had the previous best result at 24.3%. archical, processes for computing features that are even more informative for visual recognition. Fukushima\\u2019s \\u201cneocognitron\\u201d biologicallyinspired hierarchical and model for pattern recognition, was an early attempt at just such a process. The neocognitron, however, lacked a supervised training algorithm. Building on Rumelhart et al., LeCun et al. showed that stochastic gradient descent via backpropagation was effective for training convolutional neural networks (CNNs), a class of models that extend the neocognitron. CNNs saw heavy use in the 1990s (e.g., ), but then fell out of fashion with the rise of support vector machines. In 2012, Krizhevsky et al. rekindled interest in CNNs by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on LeCun\\u2019s CNN (e.g., max(x, 0) rectifying and \\u201cdropout\\u201d regularization). The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? We answer this question by bridging the gap between image classification and object detection. This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler features. To achieve this result, we focused on two problems: localizing objects with a deep network and training a model with only a small quantity of annotated detection data. Unlike image classification, detection requires localizing (likely many) objects within an image. One approach frames localization as a regression problem. However, work from Szegedy et al., concurrent with our own, indicates that this strategy may not fare well in practice. An alternative is to build a detector. CNNs have been used in this way for at least two decades, typically on constrained object categories, such as faces and pedestrians. In order to maintain high spatial resolution, these CNNs typically only have two convolutional and pooling layers. We also considered adopting a approach. However, units high up in our network, which has five convolutional layers, have very large receptive fields (195 \\u00d7 195 pixels) and strides (32\\u00d732 pixels) in the input image, which makes precise localization within the paradigm an open technical challenge. Instead, we solve the CNN localization problem by operating within the \\u201crecognition using regions\\u201d paradigm, which has been successful for both object detection and semantic segmentation. At test time, our method generates around 2000 region proposals for the input image, extracts a feature vector from each proposal using a CNN, and then classifies each region with linear SVMs. We use a simple technique (affine image warping) to compute a CNN input from each region proposal, regardless of the region\\u2019s shape.   presents an overview of our method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method: Regions with CNN features. In this updated version of this paper, we provide a comparison of and the recently proposed OverFeat detection system by running on the ILSVRC2013 detection dataset. OverFeat uses a CNN for detection and until now was the best performing method on ILSVRC2013 detection. We show that significantly outperforms OverFeat, with a mAP of 31.4% versus 24.3%. A second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised, followed by supervised (e.g., ). The second principle contribution of this paper is to show that supervised on a large auxiliary dataset (ILSVRC), followed by domainspecific on a small dataset (PASCAL), is an effective paradigm for learning CNNs when data is scarce. In our experiments, for detection improves mAP performance by 8 percentage points. After, our system achieves a mAP of 54% on VOC 2010 compared to 33% for the, deformable part model (DPM). We also point readers to contemporaneous work by Donahue et al., who show that Krizhevsky\\u2019s CNN can be used (without finetuning) as a blackbox feature extractor, yielding excellent performance on several recognition tasks including scene classification,, and domain adaptation. Our system is also quite efficient. The only computations are a reasonably small product and greedy suppression. This computational property follows from features that are shared across all categories and that are also two orders of magnitude lowerdimensional than previously used region features (cf. ). Understanding the failure modes of our approach is also critical for improving it, and so we report results from the detection analysis tool of Hoiem et al.. As an immediate consequence of this analysis, we demonstrate that a simple regression method significantly reduces mislocalizations, which are the dominant error mode. Before developing technical details, we note that because operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, we also achieve competitive results on the PASCAL VOC segmentation task, with an average segmentation accuracy of 47.9% on the VOC 2011 test set. 2. Object detection with Our object detection system consists of three modules. The first generates region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a feature vector from each region. The third module is a set of classspecific linear SVMs. In this section, we present our design decisions for each module, describe their usage, detail how their parameters are learned, and show detection results on PASCAL VOC and on ILSVRC2013. 2.1. Module design Region proposals. A variety of recent papers offer methods for generating region proposals.: Warped training samples from VOC 2007 train. Examples include: objectness, selective search, object proposals, constrained parametric (CPMC), combinatorial grouping, and Cires\\u00b8an et al., who detect mitotic cells by applying a CNN to square crops, which are a special case of region proposals. While is agnostic to the particular region proposal method, we use selective search to enable a controlled comparison with prior detection work (e.g., ). Feature extraction. We extract a feature vector from each region proposal using the Caffe implementation of the CNN described by Krizhevsky et al.. Features are computed by forward propagating a 227 \\u00d7 227 RGB image through five convolutional layers and two fully connected layers. We refer readers to for more network architecture details. In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227 \\u00d7 227 pixel size). Of the many possible transformations of our regions, we opt for the simplest. Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16).   shows a random sampling of warped training regions. Alternatives to warping are discussed in Appendix A. 2.2. detection At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective search\\u2019s \\u201cfast mode\\u201d in all experiments). We warp each proposal and forward propagate it through the CNN in order to compute features. Then, for each class, we score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, we apply a greedy suppression (for each class independently) that rejects a region if it has an (IoU) overlap with a higher scoring selected region larger than a learned threshold. analysis. Two properties make detection efficient. First, all CNN parameters are shared across all categories. Second, the feature vectors computed by the CNN are when compared to other common approaches, such as spatial pyramids with - encodings. The features used in the UVA detection system, for example, are two orders of magnitude larger than ours (360k vs. ). The result of such sharing is that the time spent computing region proposals and features ( on a GPU or on a CPU) is amortized over all classes. The only computations are dot products between features and SVM weights and suppression. In practice, all dot products for an image are batched into a single product. The feature matrix is typically 2000\\u00d74096 and the SVM weight matrix is 4096\\u00d7N, where N is the number of classes. This analysis shows that can scale to thousands of object classes without resorting to approximate techniques, such as hashing. Even if there were 100k classes, the resulting matrix multiplication takes only 10 seconds on a modern CPU. This efficiency is not merely the result of using region proposals and shared features. The UVA system, due to its features, would be two orders of magnitude slower while requiring 134GB of memory just to store 100k linear predictors, compared to just 1.5GB for our features. It is also interesting to contrast with the recent work from Dean et al. on scalable detection using DPMs and hashing. They report a mAP of around 16% on VOC 2007 at a of 5 minutes per image when introducing 10k distractor classes. With our approach, 10k detectors can run in about a minute on a CPU, and because no approximations are made mAP would remain at 59% (Section 3.2). 2.3. Training Supervised. We discriminatively the CNN on a large auxiliary dataset (ILSVRC2012 classification) using annotations only (boundingbox labels are not available for this data). was performed using the open source Caffe CNN library. In brief, our CNN nearly matches the performance of Krizhevsky et al., obtaining a error rate 2.2 percentage points higher on the ILSVRC2012 classification validation set. This discrepancy is due to simplifications in the training process.. To adapt our CNN to the new task (detection) and the new domain (warped proposal windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals. Aside from replacing the CNN\\u2019s ImageNetspecific classification layer with a randomly initialized (N + 1)-way classification layer (where N is the number of object classes, plus 1 for background), the CNN architecture is unchanged. For VOC, N = 20 and for ILSVRC2013, N = 200. We treat all region proposals with \\u22650.5 IoU overlap with a box as positives for that box\\u2019s class and the rest as negatives. We start SGD at a learning rate of 0.001 ( of the initial rate), which allows to make progress while not clobbering the initialization. In each SGD iteration, we uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a of size 128. We bias the sampling towards positive windows because they are extremely rare compared to background. Object category classifiers. Consider training a binary classifier to detect cars. It\\u2019s clear that an image region tightly enclosing a car should be a positive example. Similarly, it\\u2019s clear that a background region, which has nothing to do with cars, should be a negative example. Less clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives. The overlap threshold, 0.3, was selected by a grid search over {0, 0.1,..., 0.5} on a validation set. We found that selecting this threshold carefully is important. Setting it to 0.5, as in, decreased mAP by 5 points. Similarly, setting it to 0 decreased mAP by 4 points. Positive examples are defined simply to be the bounding boxes for each class. Once features are extracted and training labels are applied, we optimize one linear SVM per class. Since the training data is too large to fit in memory, we adopt the standard hard negative mining method. Hard negative mining converges quickly and in practice mAP stops increasing after only a single pass over all images. In Appendix B we discuss why the positive and negative examples are defined differently in versus SVM training. We also discuss the involved in training detection SVMs rather than simply using the outputs from the final softmax layer of the CNN. 2.4. Results on PASCAL VOC Following the PASCAL VOC best practices, we validated all design decisions and hyperparameters on the VOC 2007 dataset (Section 3.2). For final results on the VOC datasets, we the CNN on VOC 2012 train and optimized our detection SVMs on VOC 2012 trainval. We submitted test results to the evaluation server only once for each of the two major algorithm variants (with and without regression).   shows complete results on VOC 2010. We compare our method against four strong baselines, including SegDPM, which combines DPM detectors with the output of a semantic segmentation system and uses additional context and rescoring. The most germane comparison is to the UVA system from Uijlings et al., since our systems use the same region proposal algorithm. To classify regions, their method builds a spatial pyramid and populates it with densely sampled SIFT, Extended OpponentSIFT, and RGB- SIFT descriptors, each vector quantized with codebooks. Classification is performed with a histogram intersection kernel SVM. Compared to their, kernel SVM approach, we achieve a large improvement in mAP, from 35.1% to 53.7% mAP, while also being much faster (Section 2.2). Our method achieves similar performance (53.3% mAP) on VOC test. 2.5. Results on ILSVRC2013 detection We ran on the ILSVRC2013 detection dataset using the same system hyperparameters that we used for PASCAL VOC. We followed the same protocol of submitting test results to the ILSVRC2013 evaluation server only twice, once with and once without regression.   compares to the entries in the ILSVRC 2013 competition and to the OverFeat result. achieves a mAP of 31.4%, which is significantly ahead of the result of 24.3% from To give a sense of the AP distribution over classes, box plots are also presented and a table of perclass APs follows at the end of the paper in   Most of the competing submissions (OverFeat,, UvA- Euvision, Toronto A, and ) used convolutional neural networks, indicating that there is significant nuance in how CNNs can be applied to object detection, leading to greatly varying outcomes. In Section 4, we give an overview of the ILSVRC2013 detection dataset and provide details about choices that we made when running on it. 3. Visualization, ablation, and modes of error 3.1. Visualizing learned features filters can be visualized directly and are easy to understand. They capture oriented edges and opponent colors. Understanding the subsequent layers is more challenging. Zeiler and Fergus present a visually attractive deconvolutional approach in. We propose a simple (and complementary) method that directly shows what the network learned. The idea is to single out a particular unit (feature) in the network and use it as if it were an object detector in its own right. That is, we compute the unit\\u2019s activations on a large set of region proposals (about 10 million), sort the proposals from highest to lowest activation, perform nonmaximum suppression, and then display the regions. Our method lets the selected unit \\u201cspeak for itself\\u201d by showing exactly which inputs it fires on. We avoid averaging in order to see different visual modes and gain insight into the invariances computed by the unit. VOC 2010 test aero bike bird boat bottle chair cow table horse mbike person plant sheep sofa train DPM v5 \\u2020 49.2 53.8 13.1 15.3 53.4 49.7 27.0 28.8 14.7 17.8 20.7 43.8 38.3 56.2 42.4 15.3 12.6 49.3 36.8 46.1 32.1 30.0 36.5 31.8 47.0 44.8 Regionlets 65.0 48.9 25.9 24.6 56.1 54.5 51.2 28.9 30.2 35.8 32.6 54.0 45.9 SegDPM \\u2020 61.4 53.4 25.6 25.2 51.7 50.6 50.8 33.8 26.8 40.4 35.0 52.8 43.1 67.1 64.1 46.7 32.0 56.4 57.2 65.9 47.3 40.9 66.6 38.1 52.8 50.2 71.8 65.8 53.0 36.8 59.7 60.0 69.9 50.6 41.4 70.0 39.3 61.2 52.4: Detection average precision (%) on VOC 2010 test. is most directly comparable to UVA and Regionlets since all methods use selective search region proposals. regression (BB) is described in Section C. At publication time, SegDPM was the on the PASCAL VOC leaderboard. \\u2020DPM and SegDPM use context rescoring not used by the other methods. SYSU_Vision *OverFeat (1) UvA-Euvision *OverFeat (2) mean average precision (mAP) in % ILSVRC2013 detection test set mAP competition result post competition result UvA-Euvision *OverFeat (1) SYSU_Vision average precision (AP) in % ILSVRC2013 detection test set class AP box plots: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data (images and labels from the ILSVRC classification dataset in all cases). (Right) Box plots for the 200 average precision values per method. A box plot for the OverFeat result is not shown because APs are not yet available ( APs for are in   and also included in the tech report source uploaded to arXiv.org; see -.txt). The red line marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each method. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).: Top regions for six pool5 units. Receptive fields and activation values are drawn in white. Some units are aligned to concepts, such as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reflections (6). VOC 2007 test aero bike bird boat bottle chair cow table horse mbike person plant sheep sofa train pool5 51.8 60.2 36.4 27.8 52.8 60.6 49.2 47.8 44.3 40.8 36.7 51.3 55.7 59.3 61.8 43.1 34.0 53.1 60.6 52.8 47.8 42.7 47.8 34.0 53.1 58.0 57.6 57.9 38.5 31.8 51.2 58.9 51.4 50.5 40.9 46.0 35.3 51.0 57.4 FT pool5 58.2 63.3 37.9 27.6 54.1 66.9 51.4 55.5 43.4 43.1 40.6 53.1 56.4 FT fc6 63.5 66.0 47.9 37.7 62.5 70.2 60.2 57.9 47.0 53.5 50.0 57.7 63.0 FT fc7 64.2 69.7 50.0 41.9 62.6 71.0 60.7 58.5 46.5 56.1 48.9 57.9 64.7 FT fc7 BB 68.1 72.8 56.8 43.0 66.3 74.2 67.6 63.5 54.5 61.2 51.1 62.5 64.8 DPM v5 33.2 60.3 10.2 16.1 54.3 58.2 23.0 24.1 26.7 12.7 36.1 46.0 43.5 DPM ST 23.8 58.2 10.5 22.8 46.2 44.9 DPM HSC 32.2 58.3 11.5 16.3 49.9 54.8 23.5 27.7 34.0 13.7 34.4 47.4 45.2: Detection average precision (%) on VOC 2007 test. Rows show performance without. Rows show results for the CNN on ILSVRC 2012 and then (FT) on VOC 2007 trainval. Row 7 includes a simple regression (BB) stage that reduces localization errors (Section C). Rows present DPM methods as a strong baseline. The first uses only HOG, while the next two use different feature learning approaches to augment or replace HOG. VOC 2007 test aero bike bird boat bottle chair cow table horse mbike person plant sheep sofa train 64.2 69.7 50.0 41.9 62.6 71.0 60.7 58.5 46.5 56.1 48.9 57.9 64.7 BB 68.1 72.8 56.8 43.0 66.3 74.2 67.6 63.5 54.5 61.2 51.1 62.5 64.8 71.6 73.5 58.1 42.2 70.7 76.0 74.5 71.0 56.9 74.5 64.0 66.5 71.2 BB 73.4 77.0 63.4 45.4 75.1 78.1 79.8 73.7 62.2 79.4 67.2 70.4 71.1: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The first two rows are results from   using Krizhevsky et al.\\u2019s architecture ( ). Rows three and four use the recently proposed architecture from Simonyan and Zisserman ( ). We visualize units from layer pool5, which is the maxpooled output of the network\\u2019s fifth and final convolutional The pool5 feature map is 6 \\u00d7 6 \\u00d7 256 = 9216dimensional. Ignoring boundary effects, each pool5 unit has a receptive field of 195\\u00d7195 pixels in the original 227\\u00d7227 pixel input. A central pool5 unit has a nearly global view, while one near the edge has a smaller, clipped support. Each row in   displays the top 16 activations for a pool5 unit from a CNN that we on VOC 2007 trainval. Six of the 256 functionally unique units are visualized (Appendix D includes more). These units were selected to show a representative sample of what the network learns. In the second row, we see a unit that fires on dog faces and dot arrays. The unit corresponding to the third row is a red blob detector. There are also detectors for human faces and more abstract patterns such as text and triangular structures with windows. The network appears to learn a representation that combines a small number of features together with a distributed representation of shape, texture, color, and material properties. The subsequent fully connected layer fc6 has the ability to model a large set of compositions of these rich features. 3.2. Ablation studies Performance -layer, without. To understand which layers are critical for detection performance, we analyzed results on the VOC 2007 dataset for each of the CNN\\u2019s last three layers. Layer pool5 was briefly described in Section 3.1. The final two layers are summarized below. Layer fc6 is fully connected to pool5. To compute features, it multiplies a 4096\\u00d79216 weight matrix by the pool5 feature map (reshaped as a vector) and then adds a vector of biases. This intermediate vector is rectified (x \\u2190max(0, x)). Layer fc7 is the final layer of the network. It is implemented by multiplying the features computed by fc6 by a 4096 \\u00d7 4096 weight matrix, and similarly adding a vector of biases and applying rectification. We start by looking at results from the CNN without on PASCAL, i.e. all CNN parameters were on ILSVRC 2012 only. Analyzing performance -layer (  rows ) reveals that features from fc7 generalize worse than features from fc6. This means that 29%, or about 16.8 million, of the CNN\\u2019s parameters can be removed without degrading mAP. More surprising is that removing both fc7 and fc6 produces quite good results even though pool5 features are computed using only 6% of the CNN\\u2019s parameters. Much of the CNN\\u2019s representational power comes from its convolutional layers, rather than from the much larger densely connected layers. This finding suggests potential utility in computing a dense feature map, in the sense of HOG, of an image by using only the convolutional layers of the CNN. This representation would enable experimentation with detectors, including DPM, on top of pool5 features. Performance -layer, with. We now look at results from our CNN after having its pa- rameters on VOC 2007 trainval. The improvement is striking (  rows ): increases mAP by 8.0 percentage points to 54.2%. The boost from is much larger for fc6 and fc7 than for pool5, which suggests that the pool5 features learned from ImageNet are general and that most of the improvement is gained from learning classifiers on top of them. Comparison to recent feature learning methods. Relatively few feature learning methods have been tried on PAS- CAL VOC detection. We look at two recent approaches that build on deformable part models. For reference, we also include results for the standard DPM. The first DPM feature learning method, DPM ST, augments HOG features with histograms of \\u201csketch token\\u201d probabilities. Intuitively, a sketch token is a tight distribution of contours passing through the center of an image patch. Sketch token probabilities are computed at each pixel by a random forest that was trained to classify 35\\u00d735 pixel patches into one of 150 sketch tokens or background. The second method, DPM HSC, replaces HOG with histograms of sparse codes (HSC). To compute an HSC, sparse code activations are solved for at each pixel using a learned dictionary of 100 7 \\u00d7 7 pixel (grayscale) atoms. The resulting activations are rectified in three ways (full and both ), spatially pooled, unit \\u21132 normalized, and then power transformed (x \\u2190sign(x)|x|\\u03b1). All variants strongly outperform the three DPM baselines (  rows ), including the two that use feature learning. Compared to the latest version of DPM, which uses only HOG features, our mAP is more than 20 percentage points higher: 54.2% vs. 33.7%-a 61% relative improvement. The combination of HOG and sketch tokens yields 2.5 mAP points over HOG alone, while HSC improves over HOG by 4 mAP points (when compared internally to their private DPM baselines-both use nonpublic implementations of DPM that underperform the open source version ). These methods achieve mAPs of 29.1% and 34.3%, respectively. 3.3. Network architectures Most results in this paper use the network architecture from Krizhevsky et al.. However, we have found that the choice of architecture has a large effect on detection performance. In   we show results on VOC 2007 test using the deep network recently proposed by Simonyan and Zisserman. This network was one of the top performers in the recent ILSVRC 2014 classification challenge. The network has a homogeneous structure consisting of 13 layers of 3 \\u00d7 3 convolution kernels, with five max pooling layers interspersed, and topped with three layers. We refer to this network as \\u201c \\u201d for OxfordNet and the baseline as \\u201c \\u201d for TorontoNet. To use in, we downloaded the publicly VGG ILSVRC 16 layers model from the Caffe Model Zoo.1 We then the network using the same protocol as we used for. The only difference was to use smaller minibatches (24 examples) as required in order to fit within GPU memory. The results in   show that R- CNN with substantially outperforms with T- Net, increasing mAP from 58.5% to 66.0%. However there is a considerable drawback in terms of compute time, with the forward pass of taking roughly 7 times longer than. 3.4. Detection error analysis We applied the excellent detection analysis tool from Hoiem et al. in order to reveal our method\\u2019s error modes, understand how changes them, and to see how our error types compare with DPM. A full summary of the analysis tool is beyond the scope of this paper and we encourage readers to consult to understand some finer details (such as \\u201cnormalized AP\\u201d). Since the analysis is best absorbed in the context of the associated plots, we present the discussion within the captions of   and   3.5. regression Based on the error analysis, we implemented a simple method to reduce localization errors. Inspired by the regression employed in DPM, we train a linear regression model to predict a new detection window given the pool5 features for a selective search region proposal. Full details are given in Appendix C. Results in,, and   show that this simple approach fixes a large number of mislocalized detections, boosting mAP by 3 to 4 points. 3.6. Qualitative results Qualitative detection results on ILSVRC2013 are presented in   and   at the end of the paper. Each image was sampled randomly from the val2 set and all detections from all detectors with a precision greater than 0.5 are shown. Note that these are not curated and give a realistic impression of the detectors in action. More qualitative results are presented in   and, but these have been curated. We selected each image because it contained interesting, surprising, or amusing results. Here, also, all detections at precision greater than 0.5 are shown. 4. The ILSVRC2013 detection dataset In Section 2 we presented results on the ILSVRC2013 detection dataset. This dataset is less homogeneous than 1 normalized AP R-CNN fc6: sensitivity and impact R-CNN FT fc7: sensitivity and impact R-CNN FT fc7 BB: sensitivity and impact DPM voc-release5: sensitivity and impact: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see ) for the highest and lowest performing subsets within six different object characteristics (occlusion, truncation, area, aspect ratio, viewpoint, part visibility). We show plots for our method ( ) with and without (FT) and regression (BB) as well as for DPM. Overall, does not reduce sensitivity (the difference between max and min), but does substantially improve both the highest and lowest performing subsets for nearly all characteristics. This indicates that does more than simply improve the lowest performing subsets for aspect ratio and area, as one might conjecture based on how we warp network inputs. Instead, improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility. total false positives percentage of each type R-CNN fc6: animals total false positives R-CNN FT fc7: animals total false positives R-CNN FT fc7 BB: animals total false positives percentage of each type R-CNN fc6: furniture total false positives R-CNN FT fc7: furniture total false positives R-CNN FT fc7 BB: furniture: Distribution of false positive (FP) types. Each plot shows the evolving distribution of FP types as more FPs are considered in order of decreasing score. Each FP is categorized into 1 of 4 types: Loc-poor localization (a detection with an IoU overlap with the correct class between 0.1 and 0.5, or a duplicate); Sim-confusion with a similar category; Oth-confusion with a dissimilar object category; BG-a FP that fired on background. Compared with DPM (see ), significantly more of our errors result from poor localization, rather than confusion with background or other object classes, indicating that the CNN features are much more discriminative than HOG. Loose localization likely results from our use of region proposals and the positional invariance learned from the CNN for classification. Column three shows how our simple regression method fixes many localization errors. PASCAL VOC, requiring choices about how to use it. Since these decisions are, we cover them in this section. 4.1. Dataset overview The ILSVRC2013 detection dataset is split into three sets: train (395,918), val (20,121), and test (40,152), where the number of images in each set is in parentheses. The val and test splits are drawn from the same image distribution. These images are and similar in complexity (number of objects, amount of clutter, pose variability, etc.) to PASCAL VOC images. The val and test splits are exhaustively annotated, meaning that in each image all instances from all 200 classes are labeled with bounding boxes. The train set, in contrast, is drawn from the ILSVRC2013 classification image distribution. These images have more variable complexity with a skew towards images of a single centered object. Unlike val and test, the train images (due to their large number) are not exhaustively annotated. In any given train image, instances from the 200 classes may or may not be labeled. In addition to these image sets, each class has an extra set of negative images. Negative images are manually checked to validate that they do not contain any instances of their associated class. The negative image sets were not used in this work. More information on how ILSVRC was collected and annotated can be found in The nature of these splits presents a number of choices for training. The train images cannot be used for hard negative mining, because annotations are not exhaustive. Where should negative examples come from? Also, the train images have different statistics than val and test. Should the train images be used at all, and if so, to what extent? While we have not thoroughly evaluated a large number of choices, we present what seemed like the most obvious path based on previous experience. Our general strategy is to rely heavily on the val set and use some of the train images as an auxiliary source of positive examples. To use val for both training and validation, we split it into roughly equally sized \\u201cval1\\u201d and \\u201cval2\\u201d sets. Since some classes have very few examples in val (the smallest has only 31 and half have fewer than 110), it is important to produce an approximately partition. To do this, a large number of candidate splits were generated and the one with the smallest maximum relative class imbalance was selected.2 Each candidate split was generated by clustering val images using their class counts as features, followed by a randomized local search that may improve the split balance. The particular split used here has a maximum relative imbalance of about 11% and a median relative imbalance of 4%. The split and code used to produce them will be publicly available to allow other researchers to compare their methods on the val splits used in this report. 4.2. Region proposals We followed the same region proposal approach that was used for detection on PASCAL. Selective search was run in \\u201cfast mode\\u201d on each image in val1, val2, and test (but not on images in train). One minor modification was required to deal with the fact that selective search is not scale invariant and so the number of regions produced depends on the image resolution. ILSVRC image sizes range from very small to a few that are several, and so we resized each image to a fixed width (500 pixels) before running selective search. On val, selective search resulted in an average of 2403 region proposals per image with a 91.6% recall of all bounding boxes (at 0.5 IoU threshold). This recall is notably lower than in PASCAL, where it is approximately 98%, indicating significant room for improvement in the region proposal stage. 4.3. Training data For training data, we formed a set of images and boxes that includes all selective search and boxes from val1 together with up to N boxes per class from train (if a class has fewer than N boxes in train, then we take all of them). We\\u2019ll call this dataset of images and boxes. In an ablation study, we show mAP on val2 for N \\u2208{0, 500, 1000} (Section 4.5). Training data is required for three procedures in: (1) CNN, (2) detector SVM training, and (3) regressor training. CNN was run for 50k SGD iteration on using the exact same settings as were used for PASCAL. on a single NVIDIA Tesla K20 took 13 hours using Caffe. SVM training, all boxes from were used as positive examples for their respective classes. Hard negative mining was performed on a randomly selected subset of 5000 images from val1. An initial experiment indicated that mining negatives from all of val1, versus a 5000 image subset (roughly half of it), resulted in only a 0.5 percentage point drop in mAP, while cutting SVM training time in half. No negative examples were taken from 2Relative imbalance is measured as |a -b|/(a + b) where a and b are class counts in each half of the split. train because the annotations are not exhaustive. The extra sets of verified negative images were not used. regressors were trained on val1. 4.4. Validation and evaluation Before submitting results to the evaluation server, we validated data usage choices and the effect of and regression on the val2 set using the training data described above. All system hyperparameters (e.g., SVM C hyperparameters, padding used in region warping, NMS thresholds, regression hyperparameters) were fixed at the same values used for PAS- CAL. Undoubtedly some of these hyperparameter choices are slightly suboptimal for ILSVRC, however the goal of this work was to produce a preliminary result on ILSVRC without extensive dataset tuning. After selecting the best choices on val2, we submitted exactly two result files to the ILSVRC2013 evaluation server. The first submission was without regression and the second submission was with regression. these submissions, we expanded the SVM and boundingbox regressor training sets to use and val, respectively. We used the CNN that was on to avoid and feature computation. 4.5. Ablation study   shows an ablation study of the effects of different amounts of training data,, and boundingbox regression. A first observation is that mAP on val2 matches mAP on test very closely. This gives us confidence that mAP on val2 is a good indicator of test set performance. The first result, 20.9%, is what achieves using a CNN on the ILSVRC2012 classification dataset (no ) and given access to the small amount of training data in val1 (recall that half of the classes in val1 have between 15 and 55 examples). the training set to improves performance to 24.1%, with essentially no difference between N = 500 and N = 1000. the CNN using examples from just val1 gives a modest improvement to 26.5%, however there is likely significant overfitting due to the small number of positive training examples. Expanding the set to, which adds up to 1000 positive examples per class from the train set, helps significantly, boosting mAP to 29.7%. regression improves results to 31.0%, which is a smaller relative gain that what was observed in PASCAL. 4.6. Relationship to OverFeat There is an interesting relationship between and OverFeat: OverFeat can be seen (roughly) as a special case of. If one were to replace selective search region SVM training set val1.5k CNN set bbox reg set CNN feature layer: ILSVRC2013 ablation study of data usage choices,, and regression. proposals with a pyramid of regular square regions and change the regressors to a single regressor, then the systems would be very similar (modulo some potentially significant differences in how they are trained: CNN detection, using SVMs, etc.). It is worth noting that OverFeat has a significant speed advantage over: it is about 9x faster, based on a figure of 2 seconds per image quoted from. This speed comes from the fact that OverFeat\\u2019s sliding windows (i.e., region proposals) are not warped at the image level and therefore computation can be easily shared between overlapping windows. Sharing is implemented by running the entire network in a convolutional fashion over inputs. Speeding up should be possible in a variety of ways and remains as future work. 5. Semantic segmentation Region classification is a standard technique for semantic segmentation, allowing us to easily apply to the PASCAL VOC segmentation challenge. To facilitate a direct comparison with the current leading semantic segmentation system (called O2P for \\u201c pooling\\u201d), we work within their open source framework. O2P uses CPMC to generate 150 region proposals per image and then predicts the quality of each region, for each class, using support vector regression (SVR). The high performance of their approach is due to the quality of the CPMC regions and the powerful pooling of multiple feature types (enriched variants of SIFT and LBP). We also note that Farabet et al. recently demonstrated good results on several dense scene labeling datasets (not including PAS- CAL) using a CNN as a classifier. We follow and extend the PASCAL segmentation training set to include the extra annotations made available by Hariharan et al.. Design decisions and hyperparameters were on the VOC 2011 validation set. Final test results were evaluated only once. CNN features for segmentation. We evaluate three strategies for computing features on CPMC regions, all of which begin by warping the rectangular window around the region to 227 \\u00d7 227. The first strategy (full) ignores the region\\u2019s shape and computes CNN features directly on the warped window, exactly as we did for detection. However, these features ignore the shape of the region. Two regions might have very similar bounding boxes while having very little overlap. Therefore, the second strategy (fg) computes CNN features only on a region\\u2019s foreground mask. We replace the background with the mean input so that background regions are zero after mean subtraction. The third strategy ( ) simply concatenates the full and fg features; our experiments validate their complementarity. full: Segmentation mean accuracy (%) on VOC 2011 validation. Column 1 presents O2P; use our CNN on ILSVRC 2012. Results on VOC 2011.   shows a summary of our results on the VOC 2011 validation set compared with O2P. (See Appendix E for complete results.) Within each feature computation strategy, layer fc6 always outperforms fc7 and the following discussion refers to the fc6 features. The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition. However, achieves an average accuracy of 47.9%, our best result by a margin of 4.2% (also modestly outperforming O2P), indicating that the context provided by the full features is highly informative even given the fg features. Notably, training the 20 SVRs on our features takes an hour on a single core, compared to 10+ hours for training on O2P features. In   we present results on the VOC 2011 test set, comparing our method, fc6 ( ), against two strong baselines. Our method achieves the highest segmentation accuracy for 11 out of 21 categories, and the highest overall segmentation accuracy of 47.9%, averaged across categories (but likely ties with the O2P result under any reasonable margin of error). Still better performance could likely be achieved by. VOC 2011 test aero bike bird boat bottle chair cow table horse mbike person plant sheep sofa train 83.4 46.8 18.9 36.6 31.2 57.3 47.4 44.1 39.4 36.1 36.3 22.1 42.0 43.2 85.4 69.7 22.3 45.2 44.4 66.7 57.8 56.2 46.1 32.3 41.2 27.8 46.9 44.6 ours ( fc6) 84.2 66.9 23.7 58.3 37.4 73.3 58.7 56.5 45.5 29.5 49.3 22.7 47.1 41.3: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the \\u201cRegions and Parts\\u201d (R&P) method of and the pooling (O2P) method of. Without any, our CNN achieves top segmentation performance, outperforming R&P and roughly matching O2P. 6. Conclusion In recent years, object detection performance had stagnated. The best performing systems were complex ensembles combining multiple image features with context from object detectors and scene classi- fiers. This paper presents a simple and scalable object detection algorithm that gives a 30% relative improvement over the best previous results on PASCAL VOC 2012. We achieved this performance through two insights. The first is to apply convolutional neural networks to region proposals in order to localize and segment objects. The second is a paradigm for training large CNNs when labeled training data is scarce. We show that it is highly effective to the network- with supervision-for a auxiliary task with abundant data (image classification) and then to the network for the target task where data is scarce (detection). We conjecture that the \\u201csupervised / finetuning\\u201d paradigm will be highly effective for a variety of vision problems. We conclude by noting that it is significant that we achieved these results by using a combination of classical tools from computer vision and deep learning (bottomup region proposals and convolutional neural networks). Rather than opposing lines of scientific inquiry, the two are natural and inevitable partners. Acknowledgments. This research was supported in part by DARPA Mind\\u2019s Eye and MSEE programs, by NSF,,, MURI -, and by support from Toyota. The GPUs used in this research were generously donated by the NVIDIA Corporation. A. Object proposal transformations The convolutional neural network used in this work requires a input of 227 \\u00d7 227 pixels. For detection, we consider object proposals that are arbitrary image rectangles. We evaluated two approaches for transforming object proposals into valid CNN inputs. The first method (\\u201ctightest square with context\\u201d) encloses each object proposal inside the tightest square and: Different object proposal transformations. (A) the original object proposal at its actual scale relative to the transformed CNN inputs; (B) tightest square with context; (C) tightest square without context; (D) warp. Within each column and example proposal, the top row corresponds to p = 0 pixels of context padding while the bottom row has p = 16 pixels of context then scales (isotropically) the image contained in that square to the CNN input size.   column (B) shows this transformation. A variant on this method (\\u201ctightest square without context\\u201d) excludes the image content that surrounds the original object proposal.   column (C) shows this transformation. The second method (\\u201cwarp\\u201d) anisotropically scales each object proposal to the CNN input size.   column (D) shows the warp transformation. For each of these transformations, we also consider including additional image context around the original object proposal. The amount of context padding (p) is defined as a border size around the original object proposal in the transformed input coordinate frame.   shows p = 0 pixels in the top row of each example and p = 16 pixels in the bottom row. In all methods, if the source rectangle extends beyond the image, the missing data is replaced with the image mean (which is then subtracted before inputing the image into the CNN). A pilot set of experiments showed that warping with context padding (p = 16 pixels) outperformed the alternatives by a large margin ( mAP points). Obviously more alternatives are possible, including using replication instead of mean padding. Exhaustive evaluation of these alternatives is left as future work. B. Positive vs. negative examples and softmax Two design choices warrant further discussion. The first is: Why are positive and negative examples defined differently for the CNN versus training the object detection SVMs? To review the definitions briefly, for finetuning we map each object proposal to the instance with which it has maximum IoU overlap (if any) and label it as a positive for the matched class if the IoU is at least 0.5. All other proposals are labeled \\u201cbackground\\u201d (i.e., negative examples for all classes). For training SVMs, in contrast, we take only the boxes as positive examples for their respective classes and label proposals with less than 0.3 IoU overlap with all instances of a class as a negative for that class. Proposals that fall into the grey zone (more than 0.3 IoU overlap, but are not ground truth) are ignored. Historically speaking, we arrived at these definitions because we started by training SVMs on features computed by the ImageNet CNN, and so was not a consideration at that point in time. In that setup, we found that our particular label definition for training SVMs was optimal within the set of options we evaluated (which included the setting we now use for ). When we started using, we initially used the same positive and negative example definition as we were using for SVM training. However, we found that results were much worse than those obtained using our current definition of positives and negatives. Our hypothesis is that this difference in how positives and negatives are defined is not fundamentally important and arises from the fact that data is limited. Our current scheme introduces many \\u201cjittered\\u201d examples (those proposals with overlap between 0.5 and 1, but not ground truth), which expands the number of positive examples by approximately 30x. We conjecture that this large set is needed when the entire network to avoid overfitting. However, we also note that using these jittered examples is likely suboptimal because the network is not being for precise localization. This leads to the second issue: Why, after, train SVMs at all? It would be cleaner to simply apply the last layer of the network, which is a softmax regression classifier, as the object detector. We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in does not emphasize precise localization and the softmax classi- fier was trained on randomly sampled negative examples rather than on the subset of \\u201chard negatives\\u201d used for SVM This result shows that it\\u2019s possible to obtain close to the same level of performance without training SVMs after. We conjecture that with some additional tweaks to the remaining performance gap may be closed. If true, this would simplify and speed up training with no loss in detection performance. C. regression We use a simple regression stage to improve localization performance. After scoring each selective search proposal with a detection SVM, we predict a new bounding box for the detection using a regressor. This is similar in spirit to the regression used in deformable part models. The primary difference between the two approaches is that here we regress from features computed by the CNN, rather than from geometric features computed on the inferred DPM part locations. The input to our training algorithm is a set of N training pairs {(P i, Gi)},...,N, where P i = (P i specifies the pixel coordinates of the center of proposal P i\\u2019s bounding box together with P i\\u2019s width and height in pixels. Hence forth, we drop the superscript i unless it is needed. Each bounding box G is specified in the same way: G = (Gx, Gy, Gw, Gh). Our goal is to learn a transformation that maps a proposed box P to a box We parameterize the transformation in terms of four functions dx(P), dy(P), dw(P), and dh(P). two specify a translation of the center of P\\u2019s bounding box, while the second two specify translations of the width and height of P\\u2019s bounding box. After learning these functions, we can transform an input proposal P into a predicted box \\u02c6G by applying the transformation \\u02c6Gx = Pwdx(P) + Px \\u02c6Gy = Phdy(P) + Py \\u02c6Gw = Pw exp(dw(P)) \\u02c6Gh = Ph exp(dh(P)). Each function d\\u22c6(P) (where \\u22c6is one of x, y, h, w) is modeled as a linear function of the pool5 features of proposal P, denoted by \\u03c65(P). (The dependence of \\u03c65(P) on the image data is implicitly assumed.) Thus we have d\\u22c6(P) = wT \\u22c6\\u03c65(P), where w\\u22c6is a vector of learnable model parameters. We learn w\\u22c6by optimizing the regularized least squares objective (ridge regression): w\\u22c6= argmin \\u22c6\\u03c65(P i))2 + \\u03bb \\u2225\\u02c6w\\u22c6\\u22252. The regression targets t\\u22c6for the training pair (P, G) are de- tx = (Gx -Px)/Pw ty = (Gy -Py)/Ph tw = log( ) th = log( ). As a standard regularized least squares problem, this can be solved efficiently in closed form. implementing regression. The first is that regularization is important: we set \\u03bb = 1000 based on a validation set. The second issue is that care must be taken when selecting which training pairs (P, G) to use. Intuitively, if P is far from all boxes, then the task of transforming P to a box G does not make sense. Using examples like P would lead to a hopeless learning problem. Therefore, we only learn from a proposal P if it is nearby at least one box. We implement \\u201cnearness\\u201d by assigning P to the box G with which it has maximum IoU overlap (in case it overlaps more than one) if and only if the overlap is greater than a threshold (which we set to 0.6 using a validation set). All unassigned proposals are discarded. We do this once for each object class in order to learn a set of regressors. At test time, we score each proposal and predict its new detection window only once. In principle, we could iterate this procedure (i.e., the newly predicted bounding box, and then predict a new bounding box from it, and so on). However, we found that iterating does not improve D. Additional feature visualizations   shows additional visualizations for 20 pool5 units. For each unit, we show the 24 region proposals that maximally activate that unit out of the full set of approximately 10 million regions in all of VOC 2007 test. We label each unit by its (y, x, channel) position in the 6 \\u00d7 6 \\u00d7 256 dimensional pool5 feature map. Within each channel, the CNN computes exactly the same function of the input region, with the (y, x) position changing only the receptive field. E. segmentation results In   we show the segmentation accuracy on VOC 2011 val for each of our six segmentation methods in addition to the O2P method. These results show which methods are strongest across each of the 20 PASCAL classes, plus the background class. F. Analysis of redundancy One concern when training on an auxiliary dataset is that there might be redundancy between it and the test set. Even though the tasks of object detection and classification are substantially different, making such redundancy much less worrisome, we still conducted a thorough investigation that quantifies the extent to which PAS- CAL test images are contained within the ILSVRC 2012 training and validation sets. Our findings may be useful to researchers who are interested in using ILSVRC 2012 as training data for the PASCAL image classification task. We performed two checks for duplicate (and nearduplicate) images. The first test is based on exact matches of flickr image IDs, which are included in the VOC 2007 test annotations (these IDs are intentionally kept secret for subsequent PASCAL test sets). All PASCAL images, and about half of ILSVRC, were collected from flickr.com. This check turned up 31 matches out of 4952 (0.63%). The second check uses GIST descriptor matching, which was shown in to have excellent performance at image detection in large (> 1 million) image collections. Following, we computed GIST descriptors on warped 32 \\u00d7 32 pixel versions of all ILSVRC 2012 trainval and PASCAL 2007 test images. Euclidean distance matching of GIST descriptors revealed 38 images (including all 31 found by flickr ID matching). The matches tend to vary slightly in JPEG compression level and resolution, and to a lesser extent cropping. These findings show that the overlap is small, less than 1%. For VOC 2012, because flickr IDs are not available, we used the GIST matching method only. Based on GIST matches, 1.5% of VOC 2012 test images are in ILSVRC 2012 trainval. The slightly higher rate for VOC 2012 is likely due to the fact that the two datasets were collected closer together in time than VOC 2007 and ILSVRC 2012 were. G. Document changelog This document tracks the progress of. To help readers understand how it has changed over time, here\\u2019s a brief changelog describing the revisions. v1 Initial version. v2 CVPR 2014 revision. Includes substantial improvements in detection performance brought about by (1) starting from a higher learning rate (0.001 instead of 0.0001), (2) using context padding when preparing CNN inputs, and (3) regression to fix localization errors. v3 Results on the ILSVRC2013 detection dataset and comparison with OverFeat were integrated into several sections (primarily Section 2 and Section 4). VOC 2011 val aero bike bird boat bottle chair cow table horse mbike person plant sheep sofa train 84.0 69.0 21.7 47.7 42.2 64.7 65.8 57.4 37.4 20.5 43.7 28.4 59.8 49.7 full fc6 81.3 56.2 23.9 42.9 40.7 59.2 56.5 53.2 34.6 16.7 48.1 24.3 53.7 51.1 full fc7 81.0 52.8 25.1 43.8 40.5 55.4 57.7 51.3 32.5 11.5 48.1 21.2 57.7 56.0 fg fc6 81.4 54.1 21.1 40.6 38.7 59.9 57.2 52.5 36.5 23.6 46.4 29.0 53.0 47.5 fg fc7 80.9 50.1 20.0 40.2 34.1 59.7 59.8 52.7 32.1 14.3 48.8 24.9 52.2 48.8 fc6 83.1 60.4 23.2 48.4 47.3 61.6 60.6 59.1 45.8 20.9 57.7 28.1 60.0 48.6 fc7 82.3 56.7 20.6 49.9 44.2 59.3 61.3 57.8 38.4 15.1 53.4 24.7 60.1 55.2: segmentation accuracy (%) on the VOC 2011 validation set. v4 The softmax vs. SVM results in Appendix B contained an error, which has been fixed. We thank Sergio Guadarrama for helping to identify this issue. v5 Added results using the new network architecture from Simonyan and Zisserman to Section 3.3 and\",\n          \"is a service provided by the library of the University of Amsterdam ( (Digital Academic Repository) A power fallacy Wagenmakers, E.-J.; Verhagen, J.; Ly, A.; Bakker, M.; Lee, M.D.; Matzke, D.; Rouder, J.N.; Morey, R.D. 10. - -4 Publication date Document Version Final published version Behavior Research Methods Article 25fa Dutch Copyright Act ( Link to publication Citation for published version (APA): Wagenmakers, E.-J., Verhagen, J., Ly, A., Bakker, M., Lee, M. D., Matzke, D., Rouder, J. N., & Morey, R. D.. A power fallacy. Behavior Research Methods, 47(4),. General rights It is not permitted to download or to the text or part of it without the consent of the author(s) copyright holder(s), other than for strictly personal, individual use, unless the work is under an open content license (like Creative Commons). regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible remove it from the website. Please Ask the Library: or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible. Download date:26 Mar 2025 Behav Res 47:913-917 DOI 10. - -4 A power fallacy Wagenmakers \\u00b7 Josine Verhagen \\u00b7 Alexander Ly \\u00b7 Marjan Bakker \\u00b7 Michael D. Lee \\u00b7 Dora Matzke \\u00b7 Jeffrey N. Rouder \\u00b7 Richard D. Morey Published online: 1 October 2014 \\u00a9 Psychonomic Society, Inc. 2014 Abstract The power fallacy refers to the misconception that what holds on average -across an ensemble of hypothetical experiments- also holds for each case individually. According to the fallacy, experiments always yield more informative data than do experiments. Here we expose the fallacy with concrete examples, demonstrating that a particular outcome from a experiment can be completely uninformative, whereas a particular outcome from a experiment can be highly informative. Although power is useful in planning an experiment, it is less useful-and sometimes even misleading-for making inferences from observed data. To make inferences from data, we recommend the use of likelihood ratios or Bayes factors, which are the extension of likelihood ratios beyond point hypotheses. These methods of inference do not average over hypothetical replications of an experiment, but instead condition on the data that have actually been observed. In this way, likelihood ratios and Bayes factors rationally quantify the evidence that a particular data set provides for or against the null or any other hypothesis. E.-J. Wagenmakers (\\u0002) \\u00b7 J. Verhagen \\u00b7 A. Ly \\u00b7 M. Bakker \\u00b7 Department of Psychology, University of Amsterdam, Weesperplein 4, 1018 XA Amsterdam, The Netherlands: University of California Irvine, Irvine, CA USA J. N. Rouder University of Missouri, Columbia, MO USA R. D. Morey University of Groningen, Groningen, Netherlands Keywords Hypothesis test \\u00b7 Likelihood ratio \\u00b7 Statistical evidence \\u00b7 Bayes factor It is well known that psychology has a power problem. Based on an extensive analysis of the literature, estimates of power (i.e., the probability of rejecting the null hypothesis when it is false) range at best from 0.30 to 0.50. Low power is problematic for several reasons. First, underpowered experiments are more susceptible to the biasing impact of questionable research practices. Second, underpowered experiments are, by definition, less likely to discover systematic deviations from chance when these exist. Third, as explained below, underpowered experiments reduce the probability that a significant result truly reflects a systematic deviation from chance. These legitimate concerns suggest that power is not only useful in planning an experiment, but should also play a role in drawing inferences or making decisions based on the collected data. For instance, authors, reviewers, and journal editors may argue that: (1) for a particular set of results of a experiment, an observed significant effect is not very diagnostic; or (2) for a particular set of results from a experiment, an observed effect is very diagnostic (i.e., it supports the null hypothesis). However, power is a measure that averages over all possible outcomes of an experiment, only one of which will actually be observed. What holds on average, across a collection of Behav Res 47:913-917 hypothetically infinite unobserved data sets, need not hold for the particular data set we have collected. This difference is important, because it counters the temptation to dismiss all findings from experiments or accept all findings from experiments. Below we outline the conditions under which a experiment is uninformative: when the only summary information we have is the significance of the effect (i.e., p < \\u03b1). When we have more information about the experiment, such as test statistics or, better, the data set itself, a experiment can be very informative. A basic principle of science should be that inference is conditional on the available relevant data. If an experiment has yet to be conducted, and many data sets are possible, all possible data sets should be considered. If an experiment has been conducted, but only summary information is available, all possible data sets consistent with the summary information should be considered. But if an experiment has been conducted, and the data are available, then only the data that were actually observed should be incorporated into inference. When power is useful for inference As Ioannidis and colleagues have emphasized, low power is of concern not only when a result is obtained, but also when a significant result is obtained: \\u201cLow statistical power (because of low sample size of studies, small effects, or both) negatively affects the likelihood that a nominally statistically significant finding actually reflects a true effect.\\u201d. To explain this fact, suppose that all that is observed from the outcome of an experiment is the summary p < \\u03b1 (i.e., the effect is significant). Given that it is known that p < \\u03b1, and given that it is known that the power equals 1 -\\u03b2, what inference can be made about the odds of H1 vs H0 being true? Power is the probability of finding a significant effect when the alternative hypothesis is true, Pr (p < \\u03b1 | H1) and, similarly, the probability of finding a significant effect when the null hypothesis is true equals Pr (p < \\u03b1 | H0). For prior probabilities of true null hypotheses Pr (H0) and true alternative hypotheses Pr (H1), it then follows from probability theory that Pr (H1 | p < \\u03b1) Pr (H0 | p < \\u03b1) = Pr (p < \\u03b1 | H1) Pr (p < \\u03b1 | H0) \\u00d7 Pr (H1) Thus (1 -\\u03b2)/\\u03b1 is the extent to which the observation that p < \\u03b1 changes the prior odds that H1 rather than H0 is true. Bakker, van Dijk, and Wicherts estimated that the typical power in psychology experiments to be 0.35. This means that if one conducts a typical experiment and observes p < \\u03b1 for \\u03b1 = 0.05, this information changes the prior odds by a factor of.35/.05 = 7. This is a moderate level of evidence.1 When p < \\u03b1 for \\u03b1 = 0.01, the change in prior odds is 0..01 = 35. This is stronger evidence. Finally, when p < \\u03b1 for \\u03b1 = 0.005, the change in prior odds is 0..005 = 70. This level of evidence is an order of magnitude larger than that based on the earlier experiment using \\u03b1 = 0.05. Note that doubling the power to 0.70 also doubles the evidence. Equation 1 highlights how power affects the diagnosticity of a significant result. It is therefore tempting to believe that experiments are always more diagnostic than experiments. This belief is false; with access to the data, rather than the summary information that p < \\u03b1, we can go beyond measures of diagnosticity such as power. As Sellke, Bayarri, and Berger put it: \\u201c(...) if a study yields p =.049, this is the actual information, not the summary statement 0 < p <.05. The two statements are very different in terms of the information they convey, and replacing the former by the latter is simply an egregious Example 1: a tale of two urns Consider two urns that each contain ten balls. Urn H0 contains nine green balls and one blue ball. Urn H1 contains nine green balls and one orange ball. You are presented with one of the urns and your task is to determine the identity of the urn by drawing balls from the urn with replacement. Unbeknownst to you, the urn selected is urn H1. In the first situation, you plan an experiment that consists of drawing a single ball. The power of this experiment is defined as Pr (reject H0 | H1) = Pr (\\u201cdraw orange ball\\u201d | H1) = 0.10. This power is very low. Nevertheless, when the experiment is carried out you happen to draw the orange ball. Despite the experiment having very low power, you were lucky and obtained a decisive result. The experimental data permit the completely confident inference that you were presented with urn H1. In the second situation, you may have the time and the resources to draw more balls. If you draw N balls with 1While odds lie on a naturally meaningful scale calibrated by betting, characterizing evidence through verbal labels such as \\u201cmoderate\\u201d and \\u201cstrong\\u201d is necessarily subjective. We believe the labels are useful because they facilitate scientific communication, but they should only be considered an approximate descriptive articulation of different standards of evidence. Behav Res 47:913-917 replacement, the power of your experiment is 1 -(0.9)N. You desire 90 % power and therefore decide in advance to draw 22 balls. When the experiment is carried out you happen to draw 22 green balls. Despite the experiment having high power, you were unlucky and obtained a result that is completely uninformative. Example 2: the Consider the anomalous phenomenon of retroactive facilitation of recall. The central finding is that people recall more words when these words are to be presented again later, following the test phase. Put simply, people\\u2019s recall performance is influenced by events that have yet to happen. In the design used by Bem, each participant performed a standard recall task followed by practice on a subset of the words. For each participant, Bem computed a \\u201cprecognition score\\u201d, which is the difference in the proportion of correctly recalled items between words that did and did not receive practice. Suppose that a researcher, Dr. Z, tasks two of her students to reproduce Bem\\u2019s Experiment 9. Dr. Z gives the students the materials, but does not specify the number of participants that each should obtain. She intends to use the data to test the null hypothesis against a specific effect size consistent with Bem\\u2019s results. As usual, the null hypothesis states that effect size is zero (i.e., H0: \\u03b4 = 0), whereas the alternative hypothesis states that there is a specific effect (i.e., H1: \\u03b4 = \\u03b4a). Dr. Z chooses \\u03b4a = 0.3, a result consistent with the data reported by Bem, and plans to use a t test for her analysis. A likelihood ratio analysis The two students, A and B, set out to replicate Bem\\u2019s findings in two experiments. Student A is not particularly interested in the project and does not invest much time, obtaining only 10 participants. The power of A\\u2019s experiment is 0.22, which is quite low. Student A sends the data to Dr. Z, who is quite disappointed in the student; nevertheless, when Dr. Z carries out her analysis the t value she finds is 5.0 (p =.0004).2  a shows the distribution of p under the null and alternative hypotheses. Under the null hypothesis, the distribution of p is evenly spread across the range of possible p values (horizontal blue line). Under the alternative that 2In order to obtain a t value of 5 with a sample size of only 10 participants, the precognition score needs to have a large mean or a small Fig. 1 A dissociation between power and evidence. The panels show distributions of p values under the null hypothesis (horizontal blue line) and alternative hypothesis (red curve) in Student a\\u2019s and b\\u2019s experiments, restricted to p < 0.05 for clarity. The vertical dashed line represents the p value found in each of the two experiments. The experiment from Student a has low power (i.e., the under H1 is similar to that under H0) but nonetheless yields an informative result; the experiment from Student b has high power (i.e., the under H1 is dissimilar to that under H0) but nonetheless yields an uninformative result the effect size is \\u03b4a = 0.3, however, p values tend to gather near 0 (upper red curve). The low power of the experiment can be seen by comparing the area under the null hypothesis\\u2019 blue line to the left of p = 0.05 with that under the alternative\\u2019s red curve; the area is only 4.4 times larger. However, for the observed p value, the information is greater. We can compute the likelihood ratio of the observed data by noting the relative heights of the densities at the observed p value (vertical dashed line). The probability of the observed p value under the alternative is 9.3 times higher under the alternative hypothesis than under the null hypothesis. Although not at all decisive, this level of evidence is generally considered substantial. The key point is that it is possible to obtain informative results from a experiment, even though this was unlikely a priori. Student B is more excited about the topic than Student A, working hard to obtain 100 participants. The power of Student B\\u2019s experiment is 0.91, which is quite high. Dr. Z is considerably happier with Student B\\u2019s work, correctly reasoning that Student B\\u2019s experiment is likely to be more informative than Student A\\u2019s. When analyzing the data, however, Dr. Z observes a t value of 1.7 (p = 0.046).3  b shows the distribution of p under the null and alternative hypotheses. As before, the distribution of p is uniform under the null hypothesis and under the the alternative that the effect size is \\u03b4a = 0.3. The high power of the experiment can be seen by comparing the area under the null hypothesis\\u2019 blue line to the left of p = 0.05 3In order to obtain a t value of 1.7 with a sample size of 100, the the precognition score needs to have a small mean or a high variance. Behav Res 47:913-917 with that under the alternative\\u2019s red curve; the area is 18.2 times larger. However, for the observed p value, the information is almost completely uninformative, as the probability of the observed p value under the alternative is only 1.83 times higher under the alternative hypothesis than under the null hypothesis. The key point is that it is possible to obtain uninformative results from a experiment, even though this was unlikely a priori. A Bayes factor analysis For the above likelihood ratio analysis we compared the probability of the observed p value under the null hypothesis H0: \\u03b4 = 0 versus the alternative hypothesis H1: \\u03b4 = \\u03b4a = 0.3. The outcome of this analysis critically depends on knowledge about the true effect size \\u03b4a. Typically, however, researchers are uncertain about the possible values of \\u03b4a. This uncertainty is explicitly accounted for in Bayesian hypothesis testing by quantifying the uncertainty in \\u03b4a through the assignment of a prior distribution. The addition of the prior distribution means that instead of a single likelihood ratio, we use a weighted average likelihood ratio, known as the Bayes factor. In other words, the Bayes factor is simply a likelihood ratio that accounts for the uncertainty in \\u03b4a. More concretely, in the case of the t test example above, the likelihood ratio for observed data Y is given by LR10 = p (Y | H1) p (Y | H0) = tdf (tobs) where tdf, \\u03b4 N (tobs) is the ordinate of the tdistribution and tdf (tobs) is the ordinate of the central, both with df degrees of freedom and evaluated at the observed value tobs. To obtain a Bayes factor, we then assign \\u03b4a a prior distribution. One reasonable choice of prior distribution for \\u03b4a is the default standard Normal distribution, H1: \\u03b4a \\u223cN(0, 1), which can be motivated as a prior. This prior can be adapted for use by only using the positive half of the normal prior. For this prior, the Bayes factor is given by B10 = p (Y | H1) p (Y | H0) p (tobs | \\u03b4a)p (\\u03b4a | H1) d\\u03b4a p (t | \\u03b4 = 0) N (tobs)N+(0, 1) d\\u03b4a tdf (tobs) LR10(\\u03b4a)N+(0, 1) d\\u03b4a. where N+ denotes a normal distribution truncated below at Still intrigued by the phenomenon of precognition, and armed now with the ability to calculate Bayes factors, Dr. Z conduct two more experiments. In the first experiment, she again uses 10 participants. Assuming the same effect size \\u03b4a = 0.3, the power of this experiment is still 0.22. The experiment nevertheless yields t = 3.0 (p =.007). For these data, the Bayes factor equals B10 = 12.4, which means that the observed data are 12.4 times more likely to have occurred under H1 than under H0. This is a nonnegligible level of evidence in support of H1, even though the experiment had only low power. In the second experiment, Dr. Z again uses 100 participants. The power of this experiment is 0.91, which is exceptionally high, as before. When the experiment is carried out she finds that t = 2.0 (p = 0.024). For these observed data, the Bayes factor equals B10 = 1.38, which means that the observed data are only 1.38 times more likely to have occurred under H1 than under H0. This level of evidence is nearly uninformative, even though the experiment had exceptionally high power. In both examples described above, the p value happened to be significant at the.05 level. It may transpire, however, that a nonsignificant result is obtained and a power analysis is then used to argue that the nonsignificant outcome provides support in favor of the null hypothesis. The argument goes as follows: \\u201cWe conducted a experiment and nevertheless we did not obtain a significant result. Surely this means that our results provide strong evidence in favor of the null hypothesis.\\u201d As a counterexample, consider the following scenario. Dr. Z conducts a third experiment, once again using 100 participants for an exceptionally high power of 0.91. The data yield t = 1.2 (p =.12). The result is not significant, but neither do the data provide compelling support in favor of H0: the Bayes factor equals B01 = 2.8, which is a relatively uninformative level of evidence. Thus, even in the case that an experiment has high power, a result is not, by itself, compelling evidence for the null hypothesis. As always, the strength of the evidence will depend on both the sample size and the closeness of the observed test statistic to the null The likelihood ratio analyses and the Bayes factor analyses both require that researchers specify an alternative distribution. For a likelihood ratio analysis, for instance, the same data will carry different evidential impact when the comparison involves H1: \\u03b4a = 0.3 than when it involves H1: \\u03b4a = 0.03. We believe the specification of an acceptable H1 is a modeling task like any other, in the sense that it involves an informed choice that can be critiqued, probed, and defended through scientific argument. More importantly, the power fallacy is universal in the sense that Behav Res 47:913-917 it does not depend on a specific choice of H1; the scenarios sketched here are only meant as concrete examples to illustrate the general rule. Conclusion On average, experiments yield more informative results than experiments. Thus, when planning an experiment, it is best to think big and collect as many observations as possible. Inference is always based on the information that is available, and better experimental designs and more data usually provide more information. However, what holds on average does not hold for each individual case. With the data in hand, statistical inference should use all of the available information, by conditioning on the specific observations that have been obtained. Power is best understood as a concept that averages over all possible outcomes of an experiment, and its value is largely determined by outcomes that will not be observed when the experiment is run. Thus, when the actual data are available, a power calculation is no longer conditioned on what is known, no longer corresponds to a valid inference, and may now be misleading. Consequently, as we have demonstrated here, it is entirely possible for experiments to yield strong evidence, and for experiments to yield weak evidence. To assess the extent to which a particular data set provides evidence for or against the null hypothesis, we recommend that researchers use likelihood ratios or Bayes factors. After a likelihood ratio or Bayes factor has been presented, the demand of an editor, the suggestion of a reviewer, or the desire of an author to interpret the relevant statistical evidence with the help of power is superfluous at best and misleading at worst. Author Note This work was supported by an ERC grant from the European Research Council. Correspondence concerning this article may be addressed to Wagenmakers, University of Amsterdam, Department of Psychology, Weesperplein 4, 1018 XA Amsterdam, the Netherlands. Email address:.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_df['content'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "235Ofx81L3s3",
        "outputId": "310dcbe4-819f-4af3-ab49-67028c87c733"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Dynamical studies of transitions B y V. Dobrosavljevi'c1 and Gabriel Kotliar2 1Department of Physics and National High Magnetic Field Laboratory, Florida State University, Tallahassee, Florida 32306, USA 2Serin Physics Laboratory, Rutgers University, PO Box 849, Piscataway, NJ 08855, USA We discuss the successes of the dynamical mean field (DMF) approach to metal insulator transitions in both the clean and the disordered limit. In the latter case, standard DMF equations are generalized in order to incorporate both the physics of strong correlation and Anderson localization eﬀects. The results give new insights into the puzzling features of doped semiconductors. 1. Introduction How a substance evolves from a metallic to a non metallic state is one of the most fundamental and richest problems in condensed matter physics. In general, there are several mechanisms at play. interactions can drive a metal to insulator transition in a pure substance. This transition is named after Sir Nevill Mott who laid down the foundations for the physical understanding of this phenomenon. Another route was discovered by Anderson who realized that suﬃciently strong disorder can drive a metalinsulator transition even in systems of non interacting electrons. The theoretical description of the situation when both eﬀects are present is a central unsolved problem. For recent reviews see Lee & Ramakrishnan and Belitz & Kirkpatrick. Early treatments used direct analogies from the theory of magnetism and the scaling approach to critical phenomena. The technical apparatus of the approach was a field theoretical non linear sigma model and an expansion near two dimensions. The physical content of such theories was expressed concisely in terms of an extension of the Fermi liquid approach to disordered systems. These ideas were very successful in the description of the transport and thermodynamic properties of weakly disordered metals. The approach, however, encounter several diﬃculties in accounting for experimental observations in many systems. The origin of these diﬃculties can be traced to (1) strong interaction eﬀects, and (2) strong statistical fluctuations. One important manifestation of this eﬀect was the formation of local magnetic moments, objects which can not be understood from a weak correlation perspective. This aspect of the problem and its theoretical treatment were addressed in Paalanen et al. and Milovanovi'c et al.. Phil. Trans. R. Soc. Lond. A (Submitted) 1996 Royal Society Typescript Printed in Great Britain Dobrosavljevi'c & Kotliar In the last few years, a new dynamical mean field approach to the strong correlation problem has been developed. It has explained many puzzling features of clean three dimensional transition metal oxides. Very recently, it was extended to incorporate the interplay of Anderson localization and correlations eﬀects. The mean field method is largely complementary to the scaling approach. While the latter concentrates on the long wavelength modes which presumably govern the immediate vicinity of the transition, the former focuses on the local charge and spin dynamics of the electrons. The latter is based on an expansion around the lower critical dimension, the former being formally valid in large dimensions. It is our view that a good mean field understanding of the metal to insulator transition problem is a necessary first step towards a comprehensive theory. In this talk we will not focus on the technical aspects of the method which are thoroughly reviewed elsewhere. Instead we will attempt to give a simple description of the physical content of the approach, and of the results obtained by this method. Section 2 introduces the essential idea of the mean field method for a general periodic solid. It is our hope that its generality and local character will be of interested to the chemistry community which has stressed the importance of local bonding and correlations through the years. Section 5 discussed the extension to the disordered case. We stress the importance of statistical fluctuations and how they are captured by the mean field method. In section 3 we discuss the physics of the density driven Mott transition and explain why the mean field approach works so well for three dimensional compounds, stressing the role of orbital degeneracy. We then move to the physics of disordered interacting systems which was the focus of many years of Mott’s research. Section 4 discusses our views on the puzzle posed by the diﬀerences between uncompensated semiconductors and disordered alloys. In section 5, 6 and 7 we elaborate on the physical content of the mean field approach and argue it captures some essential elements needed to resolve this puzzle. We conclude in section 8 with a comparison to other approaches and outline some directions for further work. 2. Dynamical theory (DMF) theory - the clean limit To motivate the dynamical mean field (DMF) approach, it is useful to draw some analogies with a well established approach to the electronic structure problem, the density functional theory (DFT). The basic physical quantity in DFT is the density ρ(r), and the free energy of a system is written as a functional of the density FDF T [ρ(r)]. Its minimum gives the physical density of the system in question. In practice, the form of the functional FDF T is unknown and various approximations such as the LDA (local density approximation) are used instead of the exact density functional. The dynamical mean field approach is very similar in spirit, except that it adopts the local spectral function A(ω, r) ≡-1 πImG(r, r; ω + iδ) as the basic quantity. One then writes the free energy as a functional of this quantity, and its minimization leads to the dynamical mean field equations which give the one particle spectral function for the system of interest. One can motivate the extension from the density to the spectral function Dynamical approach as a kind of density ) in the context of strongly correlated electron systems, by thinking about their photoemission (and inverse photoemission) spectra which demonstrate the existence of bands which have the character of atomic configurations (Hubbard bands), in addition to the ordinary quasiparticle bands which are analogous to those of the non interacting system. One can incorporate quasiparticle bands and Hubbard bands naturally in a single theoretical framework by resolving in energy, the local density, i.e. the local density is the integral of the local spectral function over frequency. By resolving in energy the localized and itinerant component of the electron, DMF approach treats coherent (quasiparticle like) and incoherent (Hubbard bands like) excitations on the same footing. It is a unified framework for the description of localized and itinerant electrons. To illustrate the generality of the method let us start from a Hamiltonian containing several orbitals per unit cell. We use a compact notation where the index α = (m, σ) combines the orbital m and the spin σ. Hlattice = - iα tiα,jβ cjβ + (Eαβ -µδαβ) c+ We now focus on a single unit cell, and integrate out all degrees of freedom except for those which reside in the selected unit cell. These are described by operators cα and no longer carry a site index. The dynamics of the resulting problem is described by an impurity model which describes an impurity (cα) coupled to a bath of fermions (abµ) (Eαβ -µ δαβ) c+ bµcα + h.c. From the impurity model we can obtain all the local correlation functions, since by construction the local lattice Green’s functions are identical to the impurity Green’s function ˆG = (Gα,β). We use a matrix notation so that the local Green’s function is given by Gα,β(τ -τ ′) = -⟨Tτ cα(τ)c+ This should be viewed as a functional of the parameters ǫbµ and Vbµ,α. To determine these parameters we construct the “Weiss field” which describes the eﬀect of the rest of the electrons on the selected cell, ˆG-1 0 (iωn) = (iωn + µ) ˆI - Vbµ,α Vbµ,β, and the self energy of the impurity, ˆΣ(iωn) = ˆG-1 ˆG-1(iωn), viewed as a functional of ǫbµ and Vbµ. These parameters are determined by requiring that the bath and the local degrees of freedom describe the electrons in the original lattice problem. Namely we can construct the local Green’s function from the lattice Green’s function obtained by adding a k independent self energy to the non interacting lattice Green’s function (obtained from 2.1 by setting the interaction terms to zero) or from the impurity model. Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar (iωn + µ)ˆI -ˆt(k) -ˆΣ(iωn) -ˆE The reader will notice many common features between DMFT and the Bragg Williams theory of magnetism. However in fermionic problems a new feature emerges-the cavity field acquires nontrivial time dependence, allowing a nonperturbative treatment of local dynamics, which proves to be of crucial importance for strongly correlated electrons. In particular, the approach incorporates incoherent (inelastic) processes even on this level, as opposed to most other treatments. As a result, the formulation can be used even in the study of non- Fermi liquid metallic phases, for example in extended Hubbard models. 3. Dynamical Mean Field Theory of the Density Driven Mott Transition in Pure Systems The Mott transition in transition metal oxides has received renewed theoretical and experimental attention. On the experimental side, new compounds have been synthesized and known compounds such as V2O3 and NiSe1-xSx have been studied. On the theoretical side, new insights have been obtained from studying the one band Hubbard model in the limit of large lattice coordination. In this limit the pressure and temperature driven Mott transition as well as the density driven transition can be thoroughly analyzed. Several quantitative comparisons between the physics of three dimensional transition metal oxides and the one band Hubbard model have already been performed. For example, the doping dependence of the electronic specific heat, the resistivity and the Hall coeﬃcient in LaxSr1-xTiO3 can be explained by the one band Hubbard model without adjustable parameters after the values of U and D have been extracted from photoemission data. Similarly, the single band Hubbard model can describe the temperature dependence of both, the optical and the in V2O3. The Mott transition with integer occupation has been the subject of extensive reviews. Here we focus on some qualitative aspects of the physics of the density driven Mott transition, the emphasis will be on the question: why does the mean field theory work so well for three dimensional transition metal We start with a brief discussion of the qualitative content of the mean field theory of the doped Mott insulator. At low temperatures the mean field theory describes a Fermi liquid with a an energy scale (renormalized Fermi energy ǫF ∗) which is proportional to the doping (i.e. deviation from half filling) δ. In infinite dimensions, the renormalized Fermi energy is proportional to the quasiparticle residue Z which also vanishes linearly in doping, i.e.ǫF ∗= ZD. In this regime, the linear term of the specific heat is inversely proportional to δ and the Hall coeﬃcient is unrenormalized from its band structure value. The Fermi liquid description is valid up to a temperature scale T0 that surprisingly scales as δ and is therefore quite small very close to the Mott transition Dynamical approach The qualitative success of the mean field approach when applied to three dimensional transition metal oxides is due to the orbital degeneracy of three dimensional transition metal oxides. Orbital degeneracy causes a close competition between ferromagnetic and antiferromagnetic tendencies. The net interaction is a sum of antiferromagnetic and ferromagnetic terms which tend to largely cancel. An illuminating example that can be studied exactly is a two site system was described by Kajueter and Kotliar mimicking a Ti ion in LaTiO3. The basic scale of the problem is a bandwidth which is of order of 1 eV, but the splitting between diﬀerent spin configurations turns out to be a much smaller ( between 10-3 and 10-4 eV). If we insist on using a one band model which ignores the orbital degeneracy, to reproduce the gross features of transition metal oxides, one must introduce in the model terms that suppress the tendency towards magnetic order. In infinite dimension, this can be accomplished by choosing lattices with longer range hopping matrix elements which induce magnetic frustration or by adding and additional ferromagnetic interactions to the Hamiltonian to compensate for the antiferromagnetic exchange which is inherent to the Hubbard model on the lattice. A Hamiltonian of the form: with an additional ferromagnetic interaction J increasing as a function of x, can serve as a crude caricature of the La1-xYxTiO3 system. This compound is ferromagnetic for x near 1 but becomes antiferromangetic at small values of x.. While this approach is certainly too crude to describe details of the magnetic ordering, it expresses the fact that the localization tendencies in this system are not caused by, and are largely independent of, the magnetic ordering (which can be either ferromagnetic or antiferromagnetic). In the limit of large dimensions the J term does not aﬀect the one particle properties, so the calculations of the eﬀective mass performed in the absence of this term apply to this model as well. The Hamiltonian (3.1 ) was used by A. Georges & L. Laloux to model the Wilson ratio of liquid He3, and by Obermeier et. al. to model the magnetic response of cuprate superconductors. To summarize, the mean field theory works very well in systems where the charge and spin fluctuations are reasonably local. This is the situation in three dimensional transition metal oxides (as a result of orbital degeneracy) and in the disordered systems described in the next section. While the qualitative success of dynamical mean field theory is easy to understand, the surprising quantitative agreement of this approach as applied to orbital degenerate systems was clarified only recently. It was shown that: the quantitative value of physical quantities such as the eﬀective mass, and the optical gap, near the Mott insulating state with one electron per site which had previously been compared with experiments depend weakly on the band degeneracy (the diﬀerences are of less than 10 percent). On the other hand the high energy behavior of the spectral functions have a sizable dependence on the number of orbitals per site. Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar 4. The metal to insulator transition in disordered systems: some outstanding puzzles The presence of disorder adds a new dimension to the metal to insulator transition problem. The continuous nature of the metal to insulator transition was first demonstrated in this class of systems ). As a result of a series of intensive experimental studies a basic picture of the behavior of physical quantities near the transition has emerged. In this section we describe briefly some aspects of the experimental picture which we regard as well established, but which still remain a challenge for the microscopic theory. For more comprehensive reviews of the experimental situation in this field see Paalanen & Bhatt Sarachik and v. Lohneyesen. It is well established the T = 0 conductivity vanishes as the transition (i.e. critical dopant concentration in doped semiconductors) is approached from the metallic side. We leave the immediate vicinity of the metal to insulator transition out of this analysis because the extrapolation to zero temperature is problematic ). In the region where this extrapolation is unambiguous, the conductivity vanishes in a power law fashion. σ ≈(n -nc)µ. In uncompensated materials (one electron or hole per dopant ion), the conductivity exponent takes an anomalously small value µ ≈. Such a small value of the conductivity exponent proved to be notoriously diﬃcult to explain by any known theory. On the other hand, well compensated doped semiconductors (only partial filling of the impurity band) behave very diﬀerently; their conductivity vanishes with an exponent µ ≈1. Amorphous alloys behave the same way. In the presence of a strong magnetic field the conductivity vanishes with an exponent µ ≈1, in both compensated an uncompensated semiconductors. The presence of (SO) coupling does not seem to be a determining factor for the behavior of the conductivity. Both Si:B (where SO coupling is strong) and Si:P (where SO coupling is weak) behave qualitatively in the same way. Uncompensated doped semiconductors also display an anomalous temperature dependence. In the metallic phase for concentrations not too close to the transition the conductivity increases as the temperature is lowered. This behavior can be reversed by applying strong magnetic fields, resulting in a decreasing conductivity at low T. In our view these anomalies occur over a wide range of concentrations and are not restricted to a tiny critical region in the vicinity of the transition. For example, the anomalously small value of the conductivity exponent µ ≈ describes the data all the way to n ≈4nc! It is then natural to conclude that the observed behavior should not be identified with an asymptotic critical behavior associated with a narrow critical region. Rather, it should be described by an appropriate equation of state, that is expected to follow from a relevant description of the problem. We also emphasize that the anomalies are associated with uncompensated systems, where we expect the eﬀects of electronic correlation to be the strongest. It is thus natural to approach the problem from a DMF perspective. In this picture, many of the above features, but in particular the compensation (filling) dependence follow naturally. In contrast to the transport properties, the thermodynamic quantities χ and Phil. Trans. R. Soc. Lond. A Dynamical approach γ vary smoothly as a function of concentration across the transition, and seem to diverge at low temperatures both on the insulating and on the metallic side of the transition. The NMR experiments portray a strongly inhomogeneous picture. There is a wide distributions of Knight shifts on the Phosphorus sites. As the transition is approached a large number of sites acquire knight shifts that are larger than the measurable range indicating the formation of local moments. The knight shift on Si is a smoother function of concentration suggesting that the metal insulator transition takes place in the phosphorus impurity band. Underlying this experimental picture is a broad distribution of energy scales which makes the transition in disordered systems very diﬀerent than in the clean case. In addition, this behavior reflects unusually developed spatial fluctuations of the electronic system. While such behavior does not normally occur in ordinary metals, it is a natural consequence of the incipient localization of the electronic states - which by definition cannot happen in a uniform fashion. 5. Statistical DMF theory We have argued that in presence of disorder, the situation is qualitatively more complex than in the clean limit. Clearly, it is necessary to extend the DMF ideas in order to capture the crucial eﬀects of disorder - the spatial fluctuations of the order parameter. In the following we present a simple pedagogical derivation of the statistical DMF theory, and discuss some of its main features. We follow an approach very similar to the -Palmer formulation of the mean field theory of spin glasses. Specifically, we treat the correlation aspects of the problem in a dynamical theory fashion, but allow spatial variations of the order parameter in order to allow for Anderson localization eﬀects. The theory is then exact in the noninteracting limit, and reduces to the standard dynamical mean field theory in absence of disorder. The approach can be applied to any lattice model of interacting fermions. For simplicity, we consider a simple Hubbard model with random site energies given by the Hamiltonian (-tij + εiδij)c† i,σcj,σ + U Following the general spirit of the DMF theory, we focus on a particular site i of the lattice, and integrate all the other sites. This procedure is formally exact, but the resulting local eﬀective action takes an arbitrarily complicated form, containing vertices of all orders. However, within the DMF approach, one retains only the contributions quadratic in Fermion fields, and the local eﬀective action assumes the form i,σ(τ)(δ(τ -τ ′) (∂τ + εi -µ) +∆i,σ(τ, τ ′))ci,σ(τ ′) + U dτni,↑(τ)ni,↓(τ). Here, we have used functional integration over Grassmann fields ci,σ(τ) that rep- Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar resent electrons of spin σ on site i, and ni,σ(τ) = c† i,σ(τ)ci,σ(τ). The “hybridization function” ∆i(τ, τ ′) is given by The sums over j and k runs over the z neighbors of the site i, and jk (ωn) =< c† j(ωn)ck(ωn) >(i) are the lattice Green’s functions evaluated with the site i removed. In general, these objects can be expressed through ordinary lattice Green’s function as jk = Gjk -GjiGjk We emphasize that the above construction is carried out for a fixed realization of disorder defined by a given set of random site energies {εi}. The DMF truncation, which keeps only the quadratic contributions to the eﬀective action is exact for: (A) infinite coordination (z →∞), or (B) electrons for arbitrary coordination. Note that in the case (A) the sum in the definition of ∆i(ωn) runs over in- finitely many neighboring sites, so that the hybridization function is replaced by its average value. As a result, the all the spatial fluctuations in the “cavity” representing the environment of a given site are suppressed, prohibiting Anderson localization eﬀects. For noninteracting electrons, the resulting theory reduces to the well known “coherent potential approximation”. Since one of the main goals of the statistical DMF theory is to incorporate Anderson localization eﬀects, we concentrate on finite coordination lattices. In this case, ∆i can be considered to be a functional of the lattice Green’s functions Gjk, evaluated for fixed disorder, i. e. ∆i = ∆i [Gjk]. At strong disorder, ∆i will exhibit pronounced fluctuations from site to site, reflecting a distribution of local environments “seen” by the electrons. Finally, in order to obtain a closed set of DMF conditions, we need to specify a procedure that relates the lattice Green’s functions Gjk to the solution of the local dynamical problem, as defined by the local eﬀective action Seff(i). We first note that the local action Seff(i) is identical to that describing an Anderson impurity (AI) model embedded in a sea of conduction electrons described by a hybridization function ∆i(ωn). The solution of this AI model then uniquely defines the corresponding self energy Σi, which is given by Σi(ωn) = iωn + µ -εi -∆i(ωn) -(Gloc ii (ωn))-1, where the local Green’s function ii (ωn) =< c† i(ωn)ci(ωn) >loc, Phil. Trans. R. Soc. Lond. A Dynamical approach is evaluated with respect to the local eﬀective action Seff(i). Next, we follow an “exact eigenstate” strategy, and define the “bare” lattice Green’s functions Go ij as the exact lattice Green’s functions evaluated for the same realization of disorder {εi}, in absence of interactions. The “full”, i. e. interactionrenormalized lattice Green’s functions are within statistical DMF theory then ij[εi →εi + Σi(ωn)], closing the set of DMF conditions. We emphasize that a similar relationship relates the exact lattice Green’s functions to their noninteracting counterparts. However, in the exact formulation, the self energies describing the interaction renormalizations are in general nonlocal in space, as well as in frequency. Having this in mind, we can describe the statistical DMF theory as a requirement for these interaction dependent self energies to assume a strictly local character. For an arbitrary lattice, an iterative procedure for solving the above set of statistical DMF equations could be obtained by (i) making an initial guess for the form of ∆i(ωn), (ii) solving the corresponding AI models on every lattice site, (iii) use the resulting to calculate the full lattice Green’s functions, (iv) calculate the new values of ∆i(ωn) and go back to step (ii). This procedure can be carried out for an arbitrary lattice in any dimension, but the procedure can be fairly time consuming due to the need to compute all the components of the lattice Green’s functions Gij. However, similar computations have already been carried out to study the interplay of correlations and disorder in a (HF) approach. Interestingly, such a lattice HF treatment for a fixed disorder realization can be obtained as a further simplification of the statistical DMF theory, if the local Anderson impurity models are themselves solved at the same level. Of course, an exact solution of the statistical DMF equations goes well beyond such a HF treatment, since it can describe inelastic processes, which are expected to be of particular importance at finite temperature and in metallic non Fermi liquid In practical terms, it is important to identify specific models where the solution of the statistical DMF equations can be simplified. The situation is particularly simple in the case of a Bethe lattice (Cayley tree). Because the absence of loops on such lattices, only local Green’s functions G(i) jj (ωn) appear in the expression for ∆i(ωn). Furthermore, in this case the objects G(i) jj (ωn) can be computed from a local action of the form identical as in Eq. (5.2), except that in the expression for ∆j(ωn), the sum now runs over z -1 neighbors, excluding the site i. We thus conclude that the objects G(i) jj (ωn) are related by a stochastic recursion relation, that involves solving Anderson impurity models with random energies εi. In the limit, the recursion relations can be written in close form, and reduce to the exact solution of the disordered electrons on a Bethe lattice. Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar 6. Order parameters for the metal to insulator transition: the disordered case In attempting to describe any phase transition, a crucial step is to identify appropriate order parameters that can characterize the qualitative diﬀerences between the diﬀerent phases of the system. Since the basic focus of the dynamical mean field approach is the one particle Green’s function, it is useful to write down an explicit expression for the Greens function of a lattice system with a one body Hamiltonian Hij and a general two body interaction term in terms of a self energy Σij(iω). [G-1(ωn)]ij = [δijiωn -Hij -Σij(ωn)]. Here, we have used a matrix notation, for a fixed realization of disorder, so that the self energy Σij(ω) describes the interaction induced renormalizations of the Green’s function. A quasiparticle picture emerges under a fairly general assumptions of regularity of the interaction self energy at low frequencies. In this case Σij(ω) ≈Σij(0) + ([Z-1]ij -1)ωn + O(ω2 and the Greens function at low frequencies assumes a very transparent form with the quasiparticle Green’s function defined as [GQP ]-1 = ωnI - Z(H + Σ(0)) Introducing the low energy eigenvalues of the quasiparticle Hamiltonian En and their corresponding eigenvector |n >, we can write G(x, x′) ≈ So far we have just repeated the derivation of the disordered Fermi liquid framework of dirty metals. In that reference those concepts were used to interpret the field theoretical coupling constants of Finkelshtein’s non linear sigma model in terms of Fermi liquid parameters. Implicit in that framework was the assumption that the spatial fluctuations, or the sample to sample fluctuations were not too large. This assumption is valid for weak disorder. The main advance of the dynamical mean field approach is the ability to deal with strong spatial fluctuations. The results of the next section indicate very diﬀerent behavior for typical and average quantities, reflecting large spatial fluctuations. These diﬀerences may be the source of the finite scale divergences that were encountered in the field theoretical approach which, from the very begining, carries out an average over the disorder. We now use the previous developments to introduce the order parameters which are relevant to the DMF theory. We begin with defining the local density of states at zero frequency πImGii(0+), Phil. Trans. R. Soc. Lond. A Dynamical approach and the corresponding local quasiparticle density of states Their ratio defines the second order parameter which could be dubbed the local quasiparticle weight. In the DMF framework, all the previous equations simplify because the self energy is local (Σij(ωn) = δijΣi(ωn)) and Zi can directly be calculated from the expression ∂ωRe[Σi(ωn)]| + When these parameters are mostly uniform, i.e. site independent their averages are precisely two of the coupling constants in the field theoretical approach of Finkelshtein. In the statistical mean field approach we focus instead on whole distributions (or on typical values) of these parameters. We now turn to the physical interpretation of the order parameters, or more precisely their statistical distribution. The low energy physics is described by two parameters, ρi and Zi, which are associated with the height and the width of the resonance in the spectra of the local impurity problem. Physically, ρi can be interpreted as a the density of states for adding or removing and electron from a specific site; Zi is related to the energy scale (or timescale) over which the quasiparticle picture applies. On the metallic side of the transition, the electron behaves as a local magnetic moment up to a time scale proportional Z-1 a delocalized quasiparticle over longer timescales. The vanishing of Zi signals the conversion of quasiparticle degrees of freedom into local moments, which takes place at the Mott transition. To gain further insight into the physical content of these order parameters, we concentrate on transport properties. The fundamental diﬀerence between a metal and an insulator is defined by the ability of the electron to leave a given lattice site - to delocalize. In early work Anderson pointed out that, at least for noninteracting electrons, this property could be examined by evaluating the lifetime of an electron on a given site. Very generally, the inverse lifetime simply measures the width of the the local resonance level. We thus expect ∼Im(Gii(0+))-1. Within DMF, the local Green’s function takes the form Gii(ωn) = [iωn + µ -εi -∆i(ωn) -Σi(ωn)]-1, and since for Anderson impurity models Im Σi(0+) = 0, we conclude that the desired local lifetime can be directly related to the local hybridization function as ∼Im ∆i(0+). We thus expect Im ∆i(0) to vanish whenever the system is insulating, but to Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar remain finite in a metallic regime. We emphasizing that the same qualitative behavior should be expected for the local density of states (LDOS) ρi as can be readily seen from Eq. (5.3). We can thus use the LDOS as an order parameter that discriminates a metal from an insulator. In a random system, ρi will fluctuate from site to site, and we need a whole distribution function to fully characterize the approach to the transition. In particular, in the Mott insulator, there is a “hard” gap of order U on every lattice site, so even the average DOS discriminates the Mott insulator from the metal. The situation is more complex as the Anderson insulator is approached. Here, the local spectrum is composed of a few (discrete, bound states), separated by gaps, but the average DOS remains finite. In contrast to the Mott insulator, in the Anderson insulator the sizes and positions of the local gaps fluctuate, but in both cases a typical site has a gap at the Fermi energy. A natural order parameter is therefore the typical DOS, that is represented by the geometric average ρtyp =< ρ >geom= exp{< ln ρi >}. This quantity is found to vanish at the Anderson transition, in contrast to the average DOS, which is not critical. On the metallic side of the transition, the distribution function of a second quantity, the local quasiparticle (QP) weight, is necessary to characterized the low energy behavior near the transition. Important information is obtained from the typical value of the random variable Zi, defined as Ztyp = exp{< ln Zi >}, which emerges as a natural order parameter from previous studies of the Mott transition. Finally, we define the averaged QP DOS by This object is very important for thermodynamics, since it is directly related to quantities such as the specific heat coeﬃcient γ =, or the local spin susceptibility χloc. Note that in absence of interactions, ρQP av reduces to the usual (algebraic) average DOS, which is not critical at a U = 0 Anderson transition, but it is strongly enhanced in the vicinity of the Mott transition. It is instructive to discuss the behavior of these order parameters in the previously studied limiting cases. In the limit of large lattice coordination, the spatial fluctuations of the bath function ∆i(ωn) are unimportant, and there is no qualitative diﬀerence between typical and average quantities. In the Mott insulating phase of a periodic solid, there is a gap in the density of states, while there is a finite density of states on the metallic side of the transition. As the MIT is approached from the metallic side, ρtyp remains finite, but Ztyp is found to linearly go to zero. Another well studied limit is that of noninteracting electrons on the Bethe lattice, which is known to display an Anderson transition. The average DOS is finite both in the insulating and in the metallic phase, and is non critical at the transition. Similarly, by definition Ztyp = 1 in this noninteracting limit, so it also remains non critical. On the other hand, the typical density of states ρtyp is finite in the metal and Phil. Trans. R. Soc. Lond. A Dynamical approach zero in the Anderson insulator. This quantity is critical, and is found to vanishes exponentially with the distance to the transition. The definitions of the the order parameters that we have proposed are not restricted to the statistical DMF framework, which is simply used as a specific calculational scheme. The same definitions can in principle be used in other approaches that can calculate local unaveraged values of the local DOS ρi(ωn) or the local part of the frequency dependent self energy Σii(ωn) due to the interactions. 7. Results The dynamical mean field theory maps the insulating phase of the model onto a collection of Anderson impurity models each one of them is embedded in an insulating bath. An Anderson impurity in an insulator, away from particle hole symmetry, can either have a doublet ground state when the coupling to the enviroment is weak or a singlet ground state when the coupling to the bath exceeds a critical value. In this strong coupling limit, a bound state is pulled from the continuum formed by the bands of the insulator. From these general consideration, we obtain a two fluid picture of the insulating phase: there are sites which have a local moment down to zero temperature while other sites quenched their spin by exchanging it with a strongly coupled neighbor. The number of sites in a doublet state, in the insulating phase of the system form “Mott droplets”. Based on our experience on the Mott transition in clean systems, we expect a larger density of states when the system undergoes a metal to insulator transition into an insulating phase with a large number of doublets. This result in at least two diﬀerent regimes depending on whether the insulating phase has a high or low concentration of sites in doublet states. We have considered a z = 3. Bethe lattice, in the limit of infinite repulsion U at T = 0 and fixed average density n in the presence of a uniform distribution of random site energies εi of width W. To calculate the probability distributions of ρj and Zj we used a simulation approach, where the probability distribution for the stochastic quantity G(i) sampled from an ensemble of N sites, as originally suggested by et al.. In order to solve Anderson impurity models for given bath functions ∆j(ωn) we use the slave boson (SB) theory, which is known to be qualitatively and even semiquantitatively correct at low temperature and at low energies. In agreement with heuristic arguments, we expect the results to be a strong function of the density n. In order to illustrate this behavior, we have carried out explicit calculations for both low electron density n = 0.3, and high electron density n = 0.7 (i.e. close to half filling). We emphasize that in the clean limit (W = 0) the behavior is qualitatively identical for the two values of the density, and the system remains metallic, with only the value of the eﬀective quasiparticle mass m∗∼Z-1 being a function of n. Qualitatively diﬀerent behavior is found as the disorder is introduced. We first describe the evolution of the probability distribution of the local quasiparticle weights Zi, as the disorder is increased. The sites with Zi ≪1 represent Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar High Doping Regime   DOS statistics in the low filling (high doping) regime. The full distributions P(ln ρ) are presented for increasing amounts of disorder. We find that the maximum, i. e. < ln ρ > shifts, as the transition is approached. Note also the extremely large width of the distribution, so that ρ now spans many orders of magnitude. local magnetic moments, and as such will dominate the thermodynamic response (see the definition of ρQP). For weak disorder we expect relatively few local moments and the quasiparticle weight distribution is peaked at a finite value. As the disorder is increased, the distribution of broadens. At a critical value of the disorder W = Wnfl, the form of this distribution assumes a singular form, leading to anomalous thermodynamic response characterized by a diverging magnetic susceptibility χ and specific heat coeﬃcient γ. This behavior was found both for high and low density, in remarkable agreement with experiments carried out on for uncompensated and compensated doped semiconductors. Interestingly, a similar transition to a non Fermi liquid metallic phase, well before the MIT, has been found from the approaches in dimensions ). As the level of disorder is increased further, a metal insulator transition is reached at the second critical value of disorder W = Wc. At this point, the typical quasiparticle weight Ztyp vanishes, a behavior reminiscent of the clean Mott transition. While this behavior is found at both high and low density, the examination of the second order parameter ρi reveals striking density dependence in the vicinity of the transition. The diﬀerence are most clearly displayed by plotting the evolution of the distribution function P(ρi) as a function of doping. At low filling n = 0.3, we find that the with of this distribution becomes extremely large, spanning many decades, reflecting huge spatial fluctuations of the electronic, as shown in Fig. 1. At the same time, the most probable value ρtyp is found to dramatically decrease, vanishing linearly with the distance to the transition. In contrast to this result, a very diﬀerent behavior is found at high density n = 0.7. As we can see from Fig. 2, in this case the distribution width again broadens, albeit in a somewhat slower fashion, as the transition is Phil. Trans. R. Soc. Lond. A Dynamical approach Low Doping Regime   DOS statistics in the high filling (low doping) regime. W show the probability distribution for ln ρ, as a function of disorder. We clearly see that while the distribution broadens, < ln q >≈cons. approached. However, in this case the most probable value ρtyp is only weakly modified with increasing disorder, and is found to approach a finite value at the transition. Recalling that the statistics of ρi are intimately related to transport, our results strongly suggest that kinetic coeﬃcients such as the conductivity will strongly depend electron filling, as the MIT is approached. These expectations should be confirmed by explicitly calculating the critical behavior conductivity at diﬀerent values of the density. Finally, we mention an important feature of the insulating state in presence of both the strong electronic correlation and disorder. By explicit calculations of both the average and the typical density of states we have demonstrated that the introduction of disorder, fills the Mott gap with localized electronic states. The average DOS is therefore finite in the insulator. Nevertheless, the MIT still retains a definite Mott character, with a finite fraction of electrons turning into localized magnetic moments, and the typical quasiparticle weight Ztyp vanishing at the transition. Our results explicitly show that the metal insulator transition in presence of both interactions and disorder is a qualitatively new type of transition, having well defined signatures of both the Anderson and the Mott route to localization. 8. Conclusions The metal to insulator transition continues to be one of the central problem in condensed matter physics. Building on the fundamental concepts introduced by Mott and Anderson there have been several attempts at the construction of a microscopic theory. We have summarized in this contribution some aspects of the dynamical mean field approach to this problem. Deeply rooted in local physics, it combines the chemical and physical aspects of the problem. We regard the construction of a mean field theory as an essential first step before a more complete treatment including Gaussian and non linear Phil. Trans. R. Soc. Lond. A Dobrosavljevi'c & Kotliar fluctuations is carried out. In the clan limit the DMF description of the Mott transition has already given insights into puzzling aspects of transition metal oxide physics. Since the approach emphasizes the local environment, it has a conceptually very simple extension to the disorder case: the statistical dynamical mean field theory. The theory explicitly incorporates both the Mott and the Anderson route to localization, and thus provides a consistent description of the transition, interpolating between the respective limits of no disorder and no interaction. This key feature seems to be missing in a recent field theoretical formulation of the transition. The statistical mean field theory reproduces many remarkable features of doped semiconductors. A transition to a Griﬃths phase where local moments coexist with conduction electrons down to arbitrary low temperature precedes the true metal to insulator transition. The local properties such as the density of states depend in a distinct fashion on the level of doping (compensation). The metal to insulator transition in the low doping region (uncompensated) has a larger typical density of states and is closer to a Mott transition than the corresponding transition at larger doping levels (compensated situation). The latter contains ingredients from both the Anderson and the Mott transition. There are several issues that deserve further investigation. A detailed study of the transport properties has to be carried out. One should also elucidate the interplay of the short wavelength fluctuations with long wavelength modes. The latter are presumably described by the non linear sigma model approaches that have been investigated near two and six dimensions. A second element still missing form the mean field theory is the inclusion of short range magnetic correlations. In our view these are the most pressing problems that need to be address on the road to a comprehensive theory of the transition. Phil. Trans. R. Soc. Lond. A Dynamical approach Acknowledgements: We are grateful to E. Abrahams, R. Bhatt, N. Bonesteel, L. Gorkov, E. Miranda, D. Popovi'c, M. P. Sarachik, J. R. Schrieﬀer, and G. Thomas for useful discussions. VD was supported by the National High Magnetic Field Laboratory at Florida State University, and the Alfred P. Sloan Foundation. GK was supported by NSF DMR.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_paper_content(content):\n",
        "    \"\"\"\n",
        "    Clean the messy content from academic papers with enhanced normalization,\n",
        "    including mathematical equation removal.\n",
        "\n",
        "    Args:\n",
        "        content (str): Raw text content from the paper\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned and normalized text content\n",
        "    \"\"\"\n",
        "    # 1. Mathematical equation removal (multiple patterns)\n",
        "    # Remove inline equations between $ $ or \\( \\)\n",
        "    content = re.sub(r'\\$.*?\\$', ' ', content)  # $...$ equations\n",
        "    content = re.sub(r'\\\\\\(.*?\\\\\\)', ' ', content)  # \\(...\\) equations\n",
        "\n",
        "    # Remove display equations between $$ $$ or \\[ \\]\n",
        "    content = re.sub(r'\\$\\$.*?\\$\\$', ' ', content, flags=re.DOTALL)  # $$...$$ equations\n",
        "    content = re.sub(r'\\\\\\[.*?\\\\\\]', ' ', content, flags=re.DOTALL)  # \\[...\\] equations\n",
        "\n",
        "    # Remove equation environments \\begin{equation}...\\end{equation}\n",
        "    content = re.sub(r'\\\\begin\\{equation\\}.*?\\\\end\\{equation\\}', ' ',\n",
        "                    content, flags=re.DOTALL)\n",
        "\n",
        "    # Remove simple mathematical expressions with common operators\n",
        "    content = re.sub(r'\\b[\\w\\d]+[+\\-*/^=<>][\\w\\d]+\\b', ' ', content)\n",
        "\n",
        "    # 2. Unicode normalization\n",
        "    content = content.replace('ﬁ', 'fi').replace('ﬂ', 'fl')\n",
        "    content = content.replace('´', \"'\").replace('`', \"'\")  # Normalize apostrophes\n",
        "\n",
        "    # 3. Dash/hyphen normalization\n",
        "    content = re.sub(r'[−‐‑‒–—―]', '-', content)\n",
        "\n",
        "    # 4. Line break handling\n",
        "    content = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', content)  # Remove mid-sentence breaks\n",
        "    content = re.sub(r'\\s*-\\s*\\n\\s*', '', content)  # Handle hyphenated word breaks\n",
        "\n",
        "    # 5. Whitespace normalization\n",
        "    content = re.sub(r' +', ' ', content)\n",
        "    content = re.sub(r'\\n+', '\\n', content)\n",
        "\n",
        "    # 6. Remove citations and references\n",
        "    content = re.sub(r'\\[[0-9,]+\\]', '', content)  # [1], [1,2], etc.\n",
        "    content = re.sub(r'\\([A-Za-z]+(?: et al\\.)?,? \\d{4}[a-z]?\\)', '', content)  # (Author et al., 1999)\n",
        "\n",
        "    # 7. Remove common LaTeX/PDF artifacts\n",
        "    content = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', ' ', content)  # \\command{...}\n",
        "    content = re.sub(r'\\\\[^a-zA-Z]', ' ', content)  # Single character commands\n",
        "\n",
        "    # 8. Remove figure/table references\n",
        "    content = re.sub(r'(?:Fig|Figure|Table)\\s*[0-9IVX]+\\.?', ' ', content, flags=re.IGNORECASE)\n",
        "\n",
        "    # 9. Final cleanup\n",
        "    content = content.strip()\n",
        "    content = re.sub(r'\\s+([.,;:!?])', r'\\1', content)  # Fix space before punctuation\n",
        "\n",
        "    return content\n",
        "\n",
        "def clean_paper_dataframe(df, content_col='content'):\n",
        "    \"\"\"\n",
        "    Apply enhanced cleaning function to a DataFrame column with progress tracking.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing paper content\n",
        "        content_col (str): Name of the column containing content to clean\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with cleaned content\n",
        "    \"\"\"\n",
        "    tqdm.pandas(desc=\"Cleaning paper content\")\n",
        "    df[content_col] = df[content_col].progress_apply(clean_paper_content)\n",
        "    return df"
      ],
      "metadata": {
        "id": "r9hAoSNNNowt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_df = clean_paper_dataframe(paper_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8b5605b8d6974722b0b662823e472f6e",
            "164980e770df4155ac6a079835567e23",
            "b31222af777d4423a84b2dd2a9a1f45e",
            "bd552b16f8a444aea4d5e441f2e171e0",
            "1a787ecac1604b54ba680c09dfe8aa95",
            "889bcceaabe94b249647f18d6641cdfa",
            "96179168aac74229964ad8c96b694f59",
            "ca735f5743d447c681611f72973d7e0f",
            "8b935033e8d74583991f826af5835f62",
            "86c5d2866c804bc48a5b0bb0d27fdafa",
            "1edcd1d94aa14d1eaace0fed7304349e"
          ]
        },
        "id": "8CgjlfVGNw08",
        "outputId": "9cfbdab8-4e2b-489c-87ca-bee1c237b5e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cleaning paper content:   0%|          | 0/4354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b5605b8d6974722b0b662823e472f6e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paper_df.to_parquet(f'{base_path}/paper_df.parquet')\n",
        "# paper_df = pd.read_parquet(f'{base_path}/paper_df.parquet').set_index('paper_id')"
      ],
      "metadata": {
        "id": "W8vdxpxTORhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling missing values"
      ],
      "metadata": {
        "id": "Ri3Ssu3YWmm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged.info()"
      ],
      "metadata": {
        "id": "_vDjDGy8Woix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede45f7c-97ad-4dac-a7e7-e3601a83e2c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 410691 entries, 0 to 410690\n",
            "Data columns (total 19 columns):\n",
            " #   Column               Non-Null Count   Dtype \n",
            "---  ------               --------------   ----- \n",
            " 0   paper                410691 non-null  object\n",
            " 1   referenced_paper     410691 non-null  object\n",
            " 2   is_referenced        410691 non-null  int64 \n",
            " 3   doi_p1               408010 non-null  object\n",
            " 4   title_p1             410691 non-null  object\n",
            " 5   publication_year_p1  410691 non-null  int64 \n",
            " 6   publication_date_p1  410691 non-null  object\n",
            " 7   cited_by_count_p1    410691 non-null  int64 \n",
            " 8   type_p1              410691 non-null  object\n",
            " 9   authors_p1           410105 non-null  object\n",
            " 10  concepts_p1          410691 non-null  object\n",
            " 11  doi_p2               407460 non-null  object\n",
            " 12  title_p2             405659 non-null  object\n",
            " 13  publication_year_p2  410691 non-null  int64 \n",
            " 14  publication_date_p2  410691 non-null  object\n",
            " 15  cited_by_count_p2    410691 non-null  int64 \n",
            " 16  type_p2              410691 non-null  object\n",
            " 17  authors_p2           408214 non-null  object\n",
            " 18  concepts_p2          410691 non-null  object\n",
            "dtypes: int64(5), object(14)\n",
            "memory usage: 59.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- missing doi => impute with empty string\n",
        "- missing title => impute with empty string\n",
        "- missing authors => impute with empty string"
      ],
      "metadata": {
        "id": "T2D5Dw8TWx2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk kolom teks yang akan dipakai: 'doi, title, authors'\n",
        "for col in ['title_p1', 'title_p2', 'doi_p1', 'doi_p2', 'authors_p1', 'authors_p2']:\n",
        "    train_merged[col] = train_merged[col].fillna(\"\")\n",
        "    test_merged[col] = test_merged[col].fillna(\"\")\n",
        "    print(f\"Filled NaNs in {col} with empty string\")"
      ],
      "metadata": {
        "id": "0ZSw_1v1WyQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463c18fe-1eca-4612-d296-dae3604bf154"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filled NaNs in title_p1 with empty string\n",
            "Filled NaNs in title_p2 with empty string\n",
            "Filled NaNs in doi_p1 with empty string\n",
            "Filled NaNs in doi_p2 with empty string\n",
            "Filled NaNs in authors_p1 with empty string\n",
            "Filled NaNs in authors_p2 with empty string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing"
      ],
      "metadata": {
        "id": "hMYIDXSvic4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_emoticon(df):\n",
        "    # Iterate over the rows in the dataframe\n",
        "    emoticon_pattern = r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F900-\\U0001F9FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F300-\\U0001F5FF\\U0001F900-\\U0001F9FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F900-\\U0001F9FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F300-\\U0001F5FF\\U0001F900-\\U0001F9FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F300-\\U0001F5FF\\U0001F900-\\U0001F9FF]+'\n",
        "\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        output = re.sub(emoticon_pattern, '', value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_miscellanous_unnecessary_char(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        # Define the mention pattern\n",
        "\n",
        "        output = re.sub(r\"(RT |https://t\\.co/[A-Za-z0-9]+|\\[.*?\\])\", \"\", value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_mention(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        # Define the mention pattern\n",
        "        mention_pattern = r'@[A-Za-z0-9+/=]+'\n",
        "\n",
        "        output = re.sub(mention_pattern, '', value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_link(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        # Define the mention pattern\n",
        "        link_pattern = r'\\s*https?://\\S+$'\n",
        "\n",
        "        output = re.sub(link_pattern, '', value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def remove_escape_sequences(text):\n",
        "    # Remove escape sequences\n",
        "    text = re.sub(r'\\\\\\\\', '', text)     # \\\\\n",
        "    text = re.sub(r\"\\\\'\", \"'\", text)     # \\'\n",
        "    text = re.sub(r'\\\\\"', '\"', text)     # \\\"\n",
        "    text = re.sub(r'\\\\n', '', text)      # \\n\n",
        "    text = re.sub(r'\\n\\n', '', text)\n",
        "    text = re.sub(r'\\\\t', '', text)      # \\t\n",
        "    text = re.sub(r'\\\\b', '', text)      # \\b\n",
        "    text = re.sub(r'\\\\r', '', text)      # \\r\n",
        "    text = re.sub(r'\\\\f', '', text)      # \\f\n",
        "    text = re.sub(r'\\\\012', '', text)    # \\012\n",
        "    text = re.sub(r'\\\\x0A', '', text)    # \\x0A\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_html_entities(text):\n",
        "    # Remove HTML entities\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    text = re.sub(r'&lt;', '', text)\n",
        "    text = re.sub(r'&gt;', '', text)\n",
        "    text = re.sub(r'&quot;', '', text)\n",
        "    text = re.sub(r'&apos;', '', text)\n",
        "    text = re.sub(r'&nbsp;', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def drop_escape_html_character(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        value = remove_escape_sequences(value)\n",
        "        value = remove_html_entities(value)\n",
        "\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= value\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_hashtags(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        # Define the mention pattern\n",
        "\n",
        "        output = re.sub(r'#\\w+', '', value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def lowercasing(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        output =  value.lower()\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def discard_non_ascii(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        # Define the mention pattern\n",
        "        ascii_pattern = r'[^\\x00-\\x7F]+'\n",
        "\n",
        "        output = re.sub(ascii_pattern, '', value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "    # Create a translation table mapping punctuation characters to None\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "    # Remove punctuation using the translation table\n",
        "    sentence_without_punctuation = sentence.translate(translator)\n",
        "\n",
        "    return sentence_without_punctuation\n",
        "\n",
        "\n",
        "def drop_puctuation(df):\n",
        "    for i in range(0,len(df['text'])):\n",
        "        # Perform the desired manipulation\n",
        "\n",
        "        value = str(df['text'][i])\n",
        "\n",
        "        output =  remove_punctuation(value)\n",
        "\n",
        "        #print(output)\n",
        "        # Change the value in place\n",
        "        df.loc[i,\"text\"]= output\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_stopwords_english(df, text_col='text', keep_negation=True):\n",
        "    \"\"\"\n",
        "    Remove English stopwords from the specified text column of a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df           : pandas.DataFrame with a column of strings.\n",
        "        text_col     : name of the column containing text.\n",
        "        keep_negation: whether to keep the word 'not' (so negations aren't lost).\n",
        "\n",
        "    Returns:\n",
        "        df with the same text column stripped of stopwords.\n",
        "    \"\"\"\n",
        "    # Load English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Optionally keep negation\n",
        "    if keep_negation and 'not' in stop_words:\n",
        "        stop_words.remove('not')\n",
        "\n",
        "    # Define a helper to clean a single sentence\n",
        "    def _filter_sentence(sent: str) -> str:\n",
        "        words = sent.split()\n",
        "        filtered = [w for w in words if w.lower() not in stop_words]\n",
        "        return ' '.join(filtered)\n",
        "\n",
        "    # Apply to the entire column\n",
        "    df[text_col] = df[text_col].astype(str).apply(_filter_sentence)\n",
        "    return df\n",
        "\n",
        "def stemming_english(df, text_col='text'):\n",
        "    \"\"\"\n",
        "    Stem English text in a DataFrame column.\n",
        "\n",
        "    Args:\n",
        "        df       : pandas.DataFrame containing the text.\n",
        "        text_col : name of the column with the raw text.\n",
        "\n",
        "    Returns:\n",
        "        The same DataFrame with the specified column stemmed.\n",
        "    \"\"\"\n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    def _stem_sentence(sent: str) -> str:\n",
        "        tokens = word_tokenize(sent)\n",
        "        stems = [stemmer.stem(tok) for tok in tokens]\n",
        "        return ' '.join(stems)\n",
        "\n",
        "    # Apply stemming\n",
        "    df[text_col] = df[text_col].astype(str).apply(_stem_sentence)\n",
        "    return df\n",
        "\n",
        "def truncate_text(df, text_col='text', n=10000):\n",
        "    \"\"\"\n",
        "    Truncate text in the specified column to only the first n words.\n",
        "\n",
        "    Args:\n",
        "        df       : pandas.DataFrame containing the text.\n",
        "        text_col : name of the column to process.\n",
        "        n        : maximum number of words to keep.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the text truncated to the first n words.\n",
        "    \"\"\"\n",
        "    def take_first_n(sentence):\n",
        "        words = sentence.split()\n",
        "        return \" \".join(words[:n])\n",
        "\n",
        "    df[text_col] = df[text_col].astype(str).apply(take_first_n)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ——— PIPELINE CONFIGURATION ———\n",
        "# Comment out any step you don't want.\n",
        "PIPELINE = [\n",
        "    truncate_text,\n",
        "    drop_emoticon,\n",
        "    drop_miscellanous_unnecessary_char,\n",
        "    drop_mention,\n",
        "    drop_link,\n",
        "    drop_escape_html_character,\n",
        "    drop_hashtags,\n",
        "    lowercasing,\n",
        "    discard_non_ascii,\n",
        "    drop_puctuation,\n",
        "    drop_stopwords_english,\n",
        "    stemming_english\n",
        "]\n",
        "\n",
        "\n",
        "def preprocess_file(input_path: str, output_path: str, pipeline):\n",
        "    \"\"\"\n",
        "    Reads a single .txt file as one document, applies pipeline steps,\n",
        "    and writes cleaned text to output_path.\n",
        "    \"\"\"\n",
        "    # Read entire paper as one string\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    # Wrap into DataFrame\n",
        "    df = pd.DataFrame({'text': [content]})\n",
        "    # Apply each preprocessing function, with progress\n",
        "    for func in pipeline:\n",
        "        df = func(df)\n",
        "    # Extract cleaned text\n",
        "    cleaned = df.at[0, 'text']\n",
        "    # Write back out\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(cleaned)\n",
        "\n",
        "\n",
        "def run_text_preprocessing_pipeline(input_dir: str, output_dir: str, pipeline):\n",
        "    \"\"\"\n",
        "    Applies the pipeline to every .txt in input_dir,\n",
        "    saving results in output_dir.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # Gather .txt files\n",
        "    txt_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]\n",
        "    # Iterate with progress bar\n",
        "    for fname in tqdm(txt_files, desc='Processing files'):\n",
        "        in_path  = os.path.join(input_dir, fname)\n",
        "        out_path = os.path.join(output_dir, fname)\n",
        "        preprocess_file(in_path, out_path, pipeline)\n",
        "    print(f\"→ Completed {len(txt_files)} files.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qiznMzPYiy8G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_path = '/paper_database'\n",
        "interim_text_path = '/paper_database_processed_cap_10000'\n",
        "\n",
        "# Karena keterbatasan resource, kode dijalankan secara terpisah\n",
        "# run_text_preprocessing_pipeline(raw_text_path, interim_text_path, PIPELINE)"
      ],
      "metadata": {
        "id": "5rOm_sTaiii6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "F_7y63q-6zk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catatan: Karena keterbatasan resource, maka kode pada bagian ini dijalankan pada runtime yang terpisah, dan hasilnya akan diload pada notebook ini."
      ],
      "metadata": {
        "id": "o2r2QC2Y1eFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: days_diff + months_diff + year_diff"
      ],
      "metadata": {
        "id": "SJmjEHPzvl7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fitur selisih hari, bulan, dan tahun antara paper dengan referenced_paper"
      ],
      "metadata": {
        "id": "ZyStKmmGZNck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'publication_date' columns to datetime objects\n",
        "train_merged['publication_date_p1'] = pd.to_datetime(train_merged['publication_date_p1'], errors='coerce')\n",
        "train_merged['publication_date_p2'] = pd.to_datetime(train_merged['publication_date_p2'], errors='coerce')\n",
        "test_merged['publication_date_p1'] = pd.to_datetime(test_merged['publication_date_p1'], errors='coerce')\n",
        "test_merged['publication_date_p2'] = pd.to_datetime(test_merged['publication_date_p2'], errors='coerce')\n",
        "\n",
        "# Calculate the difference in days and months\n",
        "train_merged['days_diff'] = (train_merged['publication_date_p1'] - train_merged['publication_date_p2']).dt.days\n",
        "train_merged['months_diff'] = (train_merged['publication_date_p1'] - train_merged['publication_date_p2']).dt.days // 30\n",
        "test_merged['days_diff'] = (test_merged['publication_date_p1'] - test_merged['publication_date_p2']).dt.days\n",
        "test_merged['months_diff'] = (test_merged['publication_date_p1'] - test_merged['publication_date_p2']).dt.days // 30\n",
        "\n",
        "# Fill NaN values with 0 (you might consider other imputation strategies)\n",
        "train_merged['days_diff'] = train_merged['days_diff'].fillna(0).astype(int)\n",
        "train_merged['months_diff'] = train_merged['months_diff'].fillna(0).astype(int)\n",
        "test_merged['days_diff'] = test_merged['days_diff'].fillna(0).astype(int)\n",
        "test_merged['months_diff'] = test_merged['months_diff'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "cLRQgVBpvoJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged['year_diff'] = train_merged['publication_year_p1'] - train_merged['publication_year_p2']\n",
        "test_merged['year_diff'] = test_merged['publication_year_p1'] - test_merged['publication_year_p2']\n",
        "print(\"Created 'year_diff' feature.\")\n",
        "train_merged.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "0DB8GntS6xfM",
        "outputId": "2f6b2c37-8b54-4804-dc45-f8934928a5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'year_diff' feature.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   paper referenced_paper  is_referenced  \\\n",
              "0  p2128            p3728              0   \n",
              "1  p0389            p3811              0   \n",
              "2  p1298            p3760              0   \n",
              "3  p0211            p1808              0   \n",
              "4  p0843            p2964              0   \n",
              "\n",
              "                                              doi_p1  \\\n",
              "0   https://doi.org/10.18653/v1/2021.findings-acl.84   \n",
              "1  https://doi.org/10.1016/b978-1-55860-377-6.500...   \n",
              "2         https://doi.org/10.1109/tpami.2016.2644615   \n",
              "3         https://doi.org/10.1109/tpami.2017.2699184   \n",
              "4         https://doi.org/10.1007/s11831-021-09694-4   \n",
              "\n",
              "                                            title_p1  publication_year_p1  \\\n",
              "0   A Survey of Data Augmentation Approaches for NLP                 2021   \n",
              "1  Residual Algorithms: Reinforcement Learning wi...                 1995   \n",
              "2  SegNet: A Deep Convolutional Encoder-Decoder A...                 2017   \n",
              "3  DeepLab: Semantic Image Segmentation with Deep...                 2017   \n",
              "4  Particle Swarm Optimization Algorithm and Its ...                 2022   \n",
              "\n",
              "  publication_date_p1  cited_by_count_p1       type_p1  \\\n",
              "0            1/1/2021                357       article   \n",
              "1            1/1/1995                981  book-chapter   \n",
              "2            1/2/2017              16255       article   \n",
              "3           4/27/2017              18641       article   \n",
              "4           4/19/2022                799        review   \n",
              "\n",
              "                                          authors_p1  \\\n",
              "0  Steven Y. Feng; Varun Gangal; Jason Wei; Sarat...   \n",
              "1                                    Leemon C. Baird   \n",
              "2  Vijay Badrinarayanan; A. C. Kendall; Roberto C...   \n",
              "3  Liang-Chieh Chen; George Papandreou; Iasonas K...   \n",
              "4                                       Ahmed G. Gad   \n",
              "\n",
              "                                         concepts_p1  \\\n",
              "0  Computer science; Popularity; Artificial intel...   \n",
              "1  Residual; Algorithm; Reinforcement learning; C...   \n",
              "2  Computer science; Artificial intelligence; Ups...   \n",
              "3  Conditional random field; Artificial intellige...   \n",
              "4  Particle swarm optimization; Swarm intelligenc...   \n",
              "\n",
              "                                     doi_p2  \\\n",
              "0        https://doi.org/10.1137/16m1080173   \n",
              "1   https://doi.org/10.1109/cvpr.2019.00447   \n",
              "2    https://doi.org/10.1002/pmic.201500396   \n",
              "3  https://doi.org/10.1609/aimag.v31i3.2303   \n",
              "4        https://doi.org/10.1007/bf00114723   \n",
              "\n",
              "                                            title_p2  publication_year_p2  \\\n",
              "0  Optimization Methods for Large-Scale Machine L...                 2018   \n",
              "1  Filter Pruning via Geometric Median for Deep C...                 2019   \n",
              "2  Integrative methods for analyzing big data in ...                 2015   \n",
              "3  Building Watson: An Overview of the DeepQA Pro...                 2010   \n",
              "4  Linear Least-Squares algorithms for temporal d...                 1996   \n",
              "\n",
              "  publication_date_p2  cited_by_count_p2  type_p2  \\\n",
              "0            1/1/2018               2492  article   \n",
              "1            6/1/2019               1078  article   \n",
              "2          12/17/2015                182   review   \n",
              "3            9/1/2010               1479  article   \n",
              "4            1/1/1996                645  article   \n",
              "\n",
              "                                          authors_p2  \\\n",
              "0        Léon Bottou; Frank E. Curtis; Jorge Nocedal   \n",
              "1  Yang He; Ping Liu; Ziwei Wang; Zhilan Hu; Yi Yang   \n",
              "2  Vladimir Gligorijević; Noël Malod‐Dognin; Nata...   \n",
              "3  David Ferrucci; Eric W. Brown; Jennifer Chu‐Ca...   \n",
              "4                 Steven J. Bradtke; Andrew G. Barto   \n",
              "\n",
              "                                         concepts_p2  year_diff  \n",
              "0  Computer science; Machine learning; Artificial...          3  \n",
              "1  FLOPS; Computer science; Convolutional neural ...        -24  \n",
              "2  Big data; Data science; Precision medicine; Re...          2  \n",
              "3  Watson; Champion; IBM; Computer science; Archi...          7  \n",
              "4  Recursive least squares filter; Algorithm; Tem...         26  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4d7631e-53c0-4021-a3ad-87a0dae8828b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>referenced_paper</th>\n",
              "      <th>is_referenced</th>\n",
              "      <th>doi_p1</th>\n",
              "      <th>title_p1</th>\n",
              "      <th>publication_year_p1</th>\n",
              "      <th>publication_date_p1</th>\n",
              "      <th>cited_by_count_p1</th>\n",
              "      <th>type_p1</th>\n",
              "      <th>authors_p1</th>\n",
              "      <th>concepts_p1</th>\n",
              "      <th>doi_p2</th>\n",
              "      <th>title_p2</th>\n",
              "      <th>publication_year_p2</th>\n",
              "      <th>publication_date_p2</th>\n",
              "      <th>cited_by_count_p2</th>\n",
              "      <th>type_p2</th>\n",
              "      <th>authors_p2</th>\n",
              "      <th>concepts_p2</th>\n",
              "      <th>year_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p2128</td>\n",
              "      <td>p3728</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.18653/v1/2021.findings-acl.84</td>\n",
              "      <td>A Survey of Data Augmentation Approaches for NLP</td>\n",
              "      <td>2021</td>\n",
              "      <td>1/1/2021</td>\n",
              "      <td>357</td>\n",
              "      <td>article</td>\n",
              "      <td>Steven Y. Feng; Varun Gangal; Jason Wei; Sarat...</td>\n",
              "      <td>Computer science; Popularity; Artificial intel...</td>\n",
              "      <td>https://doi.org/10.1137/16m1080173</td>\n",
              "      <td>Optimization Methods for Large-Scale Machine L...</td>\n",
              "      <td>2018</td>\n",
              "      <td>1/1/2018</td>\n",
              "      <td>2492</td>\n",
              "      <td>article</td>\n",
              "      <td>Léon Bottou; Frank E. Curtis; Jorge Nocedal</td>\n",
              "      <td>Computer science; Machine learning; Artificial...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p0389</td>\n",
              "      <td>p3811</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1016/b978-1-55860-377-6.500...</td>\n",
              "      <td>Residual Algorithms: Reinforcement Learning wi...</td>\n",
              "      <td>1995</td>\n",
              "      <td>1/1/1995</td>\n",
              "      <td>981</td>\n",
              "      <td>book-chapter</td>\n",
              "      <td>Leemon C. Baird</td>\n",
              "      <td>Residual; Algorithm; Reinforcement learning; C...</td>\n",
              "      <td>https://doi.org/10.1109/cvpr.2019.00447</td>\n",
              "      <td>Filter Pruning via Geometric Median for Deep C...</td>\n",
              "      <td>2019</td>\n",
              "      <td>6/1/2019</td>\n",
              "      <td>1078</td>\n",
              "      <td>article</td>\n",
              "      <td>Yang He; Ping Liu; Ziwei Wang; Zhilan Hu; Yi Yang</td>\n",
              "      <td>FLOPS; Computer science; Convolutional neural ...</td>\n",
              "      <td>-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>p1298</td>\n",
              "      <td>p3760</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1109/tpami.2016.2644615</td>\n",
              "      <td>SegNet: A Deep Convolutional Encoder-Decoder A...</td>\n",
              "      <td>2017</td>\n",
              "      <td>1/2/2017</td>\n",
              "      <td>16255</td>\n",
              "      <td>article</td>\n",
              "      <td>Vijay Badrinarayanan; A. C. Kendall; Roberto C...</td>\n",
              "      <td>Computer science; Artificial intelligence; Ups...</td>\n",
              "      <td>https://doi.org/10.1002/pmic.201500396</td>\n",
              "      <td>Integrative methods for analyzing big data in ...</td>\n",
              "      <td>2015</td>\n",
              "      <td>12/17/2015</td>\n",
              "      <td>182</td>\n",
              "      <td>review</td>\n",
              "      <td>Vladimir Gligorijević; Noël Malod‐Dognin; Nata...</td>\n",
              "      <td>Big data; Data science; Precision medicine; Re...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p0211</td>\n",
              "      <td>p1808</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1109/tpami.2017.2699184</td>\n",
              "      <td>DeepLab: Semantic Image Segmentation with Deep...</td>\n",
              "      <td>2017</td>\n",
              "      <td>4/27/2017</td>\n",
              "      <td>18641</td>\n",
              "      <td>article</td>\n",
              "      <td>Liang-Chieh Chen; George Papandreou; Iasonas K...</td>\n",
              "      <td>Conditional random field; Artificial intellige...</td>\n",
              "      <td>https://doi.org/10.1609/aimag.v31i3.2303</td>\n",
              "      <td>Building Watson: An Overview of the DeepQA Pro...</td>\n",
              "      <td>2010</td>\n",
              "      <td>9/1/2010</td>\n",
              "      <td>1479</td>\n",
              "      <td>article</td>\n",
              "      <td>David Ferrucci; Eric W. Brown; Jennifer Chu‐Ca...</td>\n",
              "      <td>Watson; Champion; IBM; Computer science; Archi...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p0843</td>\n",
              "      <td>p2964</td>\n",
              "      <td>0</td>\n",
              "      <td>https://doi.org/10.1007/s11831-021-09694-4</td>\n",
              "      <td>Particle Swarm Optimization Algorithm and Its ...</td>\n",
              "      <td>2022</td>\n",
              "      <td>4/19/2022</td>\n",
              "      <td>799</td>\n",
              "      <td>review</td>\n",
              "      <td>Ahmed G. Gad</td>\n",
              "      <td>Particle swarm optimization; Swarm intelligenc...</td>\n",
              "      <td>https://doi.org/10.1007/bf00114723</td>\n",
              "      <td>Linear Least-Squares algorithms for temporal d...</td>\n",
              "      <td>1996</td>\n",
              "      <td>1/1/1996</td>\n",
              "      <td>645</td>\n",
              "      <td>article</td>\n",
              "      <td>Steven J. Bradtke; Andrew G. Barto</td>\n",
              "      <td>Recursive least squares filter; Algorithm; Tem...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4d7631e-53c0-4021-a3ad-87a0dae8828b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4d7631e-53c0-4021-a3ad-87a0dae8828b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4d7631e-53c0-4021-a3ad-87a0dae8828b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e025046-e5df-4c1c-8db0-7a5522a2e3b4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e025046-e5df-4c1c-8db0-7a5522a2e3b4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e025046-e5df-4c1c-8db0-7a5522a2e3b4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_merged"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: TF-IDF Cosine Similarity pada Judul"
      ],
      "metadata": {
        "id": "8iGsxYFq8iec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung kemiripan judul paper dengan referenced_paper menggunakan TF-IDF Cosine Similarity"
      ],
      "metadata": {
        "id": "5eXRbSy8ZfYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan semua judul unik dari train dan test untuk vocabulary TF-IDF\n",
        "all_titles = pd.concat([train_merged['title_p1'], train_merged['title_p2'],\n",
        "                        test_merged['title_p1'], test_merged['title_p2']]).astype(str).unique()\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000) # Sesuaikan max_features\n",
        "\n",
        "# Fit vectorizer pada semua judul unik\n",
        "vectorizer.fit(all_titles)\n",
        "print(f\"TF-IDF Vectorizer fitted with vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# Transform judul p1 dan p2 untuk train dan test\n",
        "tfidf_p1_train = vectorizer.transform(train_merged['title_p1'])\n",
        "tfidf_p2_train = vectorizer.transform(train_merged['title_p2'])\n",
        "tfidf_p1_test = vectorizer.transform(test_merged['title_p1'])\n",
        "tfidf_p2_test = vectorizer.transform(test_merged['title_p2'])\n",
        "\n",
        "# menghitung cosine similarity baris per baris\n",
        "def calculate_cosine_similarity_rowwise(matrix1, matrix2):\n",
        "    similarities = np.zeros(matrix1.shape[0])\n",
        "    for i in range(matrix1.shape[0]):\n",
        "        vec1 = matrix1[i]\n",
        "        vec2 = matrix2[i]\n",
        "        sim = cosine_similarity(vec1, vec2)[0, 0]\n",
        "        similarities[i] = sim\n",
        "        if (i + 1) % 50000 == 0:\n",
        "            print(f\"Processed {i+1} rows for cosine similarity...\")\n",
        "    return similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRnHSqg68Vn",
        "outputId": "9909c716-90df-4b13-b477-74eaf28fb8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorizer fitted with vocabulary size: 5459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung similarity untuk train dan test\n",
        "print(\"Calculating title cosine similarity for train set...\")\n",
        "train_merged['title_similarity'] = calculate_cosine_similarity_rowwise(tfidf_p1_train, tfidf_p2_train)\n",
        "print(\"Calculating title cosine similarity for test set...\")\n",
        "test_merged['title_similarity'] = calculate_cosine_similarity_rowwise(tfidf_p1_test, tfidf_p2_test)\n",
        "\n",
        "print(\"Created 'title_similarity' feature.\")\n",
        "print(\"\\n--- Train Merged Head with New Features ---\")\n",
        "print(train_merged[['year_diff', 'cited_by_count_p2', 'title_similarity', 'is_referenced']].head())\n",
        "\n",
        "# Hapus variabel TF-IDF besar untuk memori\n",
        "del tfidf_p1_train, tfidf_p2_train, tfidf_p1_test, tfidf_p2_test, all_titles\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldA7OXr0GQVZ",
        "outputId": "9a5e899c-88c6-4624-f009-446e38db0441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating title cosine similarity for train set...\n",
            "Processed 50000 rows for cosine similarity...\n",
            "Processed 100000 rows for cosine similarity...\n",
            "Processed 150000 rows for cosine similarity...\n",
            "Processed 200000 rows for cosine similarity...\n",
            "Processed 250000 rows for cosine similarity...\n",
            "Processed 300000 rows for cosine similarity...\n",
            "Processed 350000 rows for cosine similarity...\n",
            "Processed 400000 rows for cosine similarity...\n",
            "Calculating title cosine similarity for test set...\n",
            "Processed 50000 rows for cosine similarity...\n",
            "Processed 100000 rows for cosine similarity...\n",
            "Processed 150000 rows for cosine similarity...\n",
            "Processed 200000 rows for cosine similarity...\n",
            "Processed 250000 rows for cosine similarity...\n",
            "Processed 300000 rows for cosine similarity...\n",
            "Created 'title_similarity' feature.\n",
            "\n",
            "--- Train Merged Head with New Features ---\n",
            "   year_diff  cited_by_count_p2  title_similarity  is_referenced\n",
            "0          3               2492               0.0              0\n",
            "1        -24               1078               0.0              0\n",
            "2          2                182               0.0              0\n",
            "3          7               1479               0.0              0\n",
            "4         26                645               0.0              0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80011"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: concepts_overlap_count & concepts_jaccard"
      ],
      "metadata": {
        "id": "fp2js6J9rxCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung berapa jumlah concepts yang overlap antara kedua paper, serta menghitung similaritas menggunakan jaccard similarity"
      ],
      "metadata": {
        "id": "psxUsOHnZpZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating Concept Similarity Features...\")\n",
        "\n",
        "def calculate_concept_features(concepts1_str, concepts2_str):\n",
        "    try:\n",
        "        # Proses string konsep 1\n",
        "        if pd.isna(concepts1_str) or concepts1_str == \"\":\n",
        "            set1 = set()\n",
        "        else:\n",
        "            set1 = set(c.strip().lower() for c in concepts1_str.split(';') if c.strip())\n",
        "\n",
        "        # Proses string konsep 2\n",
        "        if pd.isna(concepts2_str) or concepts2_str == \"\":\n",
        "            set2 = set()\n",
        "        else:\n",
        "            set2 = set(c.strip().lower() for c in concepts2_str.split(';') if c.strip())\n",
        "\n",
        "        # Hitung Intersection dan Union\n",
        "        intersection_count = len(set1.intersection(set2))\n",
        "        union_count = len(set1.union(set2))\n",
        "\n",
        "        # Hitung Jaccard Similarity\n",
        "        if union_count == 0:\n",
        "            # Jika kedua set kosong, Jaccard bisa dianggap 0 atau 1. 0 lebih aman.\n",
        "            jaccard_similarity = 0.0\n",
        "        else:\n",
        "            jaccard_similarity = intersection_count / union_count\n",
        "\n",
        "        # Kembalikan hasil sebagai Pandas Series agar mudah diassign ke kolom baru\n",
        "        return pd.Series([intersection_count, jaccard_similarity])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing concepts: '{concepts1_str}' vs '{concepts2_str}'. Error: {e}\")\n",
        "        return pd.Series([0, 0.0])\n",
        "\n",
        "\n",
        "# Terapkan fungsi ke DataFrame train dan test\n",
        "print(\"Applying concept feature calculation to train_merged...\")\n",
        "train_merged[['concepts_overlap_count', 'concepts_jaccard']] = train_merged.apply(\n",
        "    lambda row: calculate_concept_features(row.get('concepts_p1', ''), row.get('concepts_p2', '')),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Applying concept feature calculation to test_merged...\")\n",
        "test_merged[['concepts_overlap_count', 'concepts_jaccard']] = test_merged.apply(\n",
        "    lambda row: calculate_concept_features(row.get('concepts_p1', ''), row.get('concepts_p2', '')),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Created 'concepts_overlap_count' and 'concepts_jaccard' features.\")\n",
        "\n",
        "print(\"\\n--- Train Merged Head with New Concept Features ---\")\n",
        "print(train_merged[['year_diff', 'cited_by_count_p2', 'title_similarity',\n",
        "                 'concepts_overlap_count', 'concepts_jaccard', 'is_referenced']].head())\n",
        "\n",
        "print(\"\\n--- Describe New Concept Features (Train) ---\")\n",
        "print(train_merged[['concepts_overlap_count', 'concepts_jaccard']].describe())\n",
        "\n",
        "# Cek missing values pada fitur baru (seharusnya tidak ada)\n",
        "print(\"\\nMissing values in new concept features (Train):\")\n",
        "print(train_merged[['concepts_overlap_count', 'concepts_jaccard']].isnull().sum())\n",
        "print(\"\\nMissing values in new concept features (Test):\")\n",
        "print(test_merged[['concepts_overlap_count', 'concepts_jaccard']].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSmJlQL1sQES",
        "outputId": "50d9a090-0bc3-4eb3-e13f-4a672ca69abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Concept Similarity Features...\n",
            "Applying concept feature calculation to train_merged...\n",
            "Applying concept feature calculation to test_merged...\n",
            "Created 'concepts_overlap_count' and 'concepts_jaccard' features.\n",
            "\n",
            "--- Train Merged Head with New Concept Features ---\n",
            "   year_diff  cited_by_count_p2  title_similarity  concepts_overlap_count  \\\n",
            "0          3               2492               0.0                     2.0   \n",
            "1        -24               1078               0.0                     0.0   \n",
            "2          2                182               0.0                     1.0   \n",
            "3          7               1479               0.0                     1.0   \n",
            "4         26                645               0.0                     0.0   \n",
            "\n",
            "   concepts_jaccard  is_referenced  \n",
            "0          0.250000              0  \n",
            "1          0.000000              0  \n",
            "2          0.111111              0  \n",
            "3          0.111111              0  \n",
            "4          0.000000              0  \n",
            "\n",
            "--- Describe New Concept Features (Train) ---\n",
            "       concepts_overlap_count  concepts_jaccard\n",
            "count           410691.000000     410691.000000\n",
            "mean                 0.860727          0.104050\n",
            "std                  0.771024          0.101012\n",
            "min                  0.000000          0.000000\n",
            "25%                  0.000000          0.000000\n",
            "50%                  1.000000          0.111111\n",
            "75%                  1.000000          0.111111\n",
            "max                  5.000000          1.000000\n",
            "\n",
            "Missing values in new concept features (Train):\n",
            "concepts_overlap_count    0\n",
            "concepts_jaccard          0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in new concept features (Test):\n",
            "concepts_overlap_count    0\n",
            "concepts_jaccard          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: authors_overlap_count & authors_jaccard"
      ],
      "metadata": {
        "id": "fClQQ17X37aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung berapa jumlah authors yang overlap antara kedua paper, serta menghitung similaritas menggunakan jaccard similarity"
      ],
      "metadata": {
        "id": "GlQJ8FjPZ7Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating Author Similarity Features...\")\n",
        "\n",
        "def calculate_author_features(authors1_str, authors2_str):\n",
        "    try:\n",
        "        # Proses string penulis 1\n",
        "        if pd.isna(authors1_str) or authors1_str == \"\":\n",
        "            set1 = set()\n",
        "        else:\n",
        "            set1 = set(a.strip().lower() for a in authors1_str.split(';') if a.strip())\n",
        "\n",
        "        # Proses string penulis 2\n",
        "        if pd.isna(authors2_str) or authors2_str == \"\":\n",
        "            set2 = set()\n",
        "        else:\n",
        "            set2 = set(a.strip().lower() for a in authors2_str.split(';') if a.strip())\n",
        "\n",
        "        # Hitung Intersection dan Union\n",
        "        intersection_count = len(set1.intersection(set2))\n",
        "        union_count = len(set1.union(set2))\n",
        "\n",
        "        # Hitung Jaccard Similarity\n",
        "        if union_count == 0:\n",
        "            jaccard_similarity = 0.0\n",
        "        else:\n",
        "            jaccard_similarity = intersection_count / union_count\n",
        "\n",
        "        return pd.Series([intersection_count, jaccard_similarity])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing authors: '{authors1_str}' vs '{authors2_str}'. Error: {e}\")\n",
        "        return pd.Series([0, 0.0]) # Default\n",
        "\n",
        "\n",
        "# Terapkan fungsi ke DataFrame train dan test\n",
        "print(\"Applying author feature calculation to train_merged...\")\n",
        "train_merged[['authors_overlap_count', 'authors_jaccard']] = train_merged.apply(\n",
        "    lambda row: calculate_author_features(row.get('authors_p1', ''), row.get('authors_p2', '')),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Applying author feature calculation to test_merged...\")\n",
        "test_merged[['authors_overlap_count', 'authors_jaccard']] = test_merged.apply(\n",
        "    lambda row: calculate_author_features(row.get('authors_p1', ''), row.get('authors_p2', '')),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Created 'authors_overlap_count' and 'authors_jaccard' features.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Describe New Author Features (Train) ---\")\n",
        "if 'authors_overlap_count' in train_merged.columns:\n",
        "    print(train_merged[['authors_overlap_count', 'authors_jaccard']].describe())\n",
        "\n",
        "# Cek missing values pada fitur baru (seharusnya tidak ada)\n",
        "print(\"\\nMissing values in new author features (Train):\")\n",
        "if 'authors_overlap_count' in train_merged.columns:\n",
        "    print(train_merged[['authors_overlap_count', 'authors_jaccard']].isnull().sum())\n",
        "print(\"\\nMissing values in new author features (Test):\")\n",
        "if 'authors_overlap_count' in test_merged.columns:\n",
        "    print(test_merged[['authors_overlap_count', 'authors_jaccard']].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKXGJgSw388s",
        "outputId": "dec5d59c-7280-4fee-d972-47f779cf2059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Author Similarity Features...\n",
            "Applying author feature calculation to train_merged...\n",
            "Applying author feature calculation to test_merged...\n",
            "Created 'authors_overlap_count' and 'authors_jaccard' features.\n",
            "\n",
            "--- Train Merged Head with New Author Features ---\n",
            "   year_diff  cited_by_count_p2  title_similarity  concepts_overlap_count  \\\n",
            "0          3               2492               0.0                     2.0   \n",
            "1        -24               1078               0.0                     0.0   \n",
            "2          2                182               0.0                     1.0   \n",
            "3          7               1479               0.0                     1.0   \n",
            "4         26                645               0.0                     0.0   \n",
            "\n",
            "   concepts_jaccard  authors_overlap_count  authors_jaccard  is_referenced  \n",
            "0          0.250000                    0.0              0.0              0  \n",
            "1          0.000000                    0.0              0.0              0  \n",
            "2          0.111111                    0.0              0.0              0  \n",
            "3          0.111111                    0.0              0.0              0  \n",
            "4          0.000000                    0.0              0.0              0  \n",
            "\n",
            "--- Describe New Author Features (Train) ---\n",
            "       authors_overlap_count  authors_jaccard\n",
            "count          410691.000000    410691.000000\n",
            "mean                0.002408         0.000460\n",
            "std                 0.067200         0.014507\n",
            "min                 0.000000         0.000000\n",
            "25%                 0.000000         0.000000\n",
            "50%                 0.000000         0.000000\n",
            "75%                 0.000000         0.000000\n",
            "max                 5.000000         1.000000\n",
            "\n",
            "Missing values in new author features (Train):\n",
            "authors_overlap_count    0\n",
            "authors_jaccard          0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in new author features (Test):\n",
            "authors_overlap_count    0\n",
            "authors_jaccard          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur-fitur: Authors Citation & Paper Score"
      ],
      "metadata": {
        "id": "Muq1Bd_gB1sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "membuat fitur-fitur yang berusaha membuat sinyal dari jumlah sitasi author-author paper dan referenced_paper, menggunakan statistics seperti mean, max, diff, dan ratio"
      ],
      "metadata": {
        "id": "C5p-vJ8rB9Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Author Citation Statistics Preprocessing...\")\n",
        "\n",
        "metadata_authors_df = metadata_df[['paper_id', 'authors', 'cited_by_count']].copy()\n",
        "metadata_authors_df['authors'] = metadata_authors_df['authors'].fillna(\"\")\n",
        "median_cited_count = metadata_authors_df['cited_by_count'].median()\n",
        "metadata_authors_df['cited_by_count'] = metadata_authors_df['cited_by_count'].fillna(0)\n",
        "print(f\"Filled NaN cited_by_count with 0.\")\n",
        "\n",
        "\n",
        "# Fungsi untuk split, clean, dan lowercase\n",
        "def clean_split_authors(authors_str):\n",
        "    if pd.isna(authors_str) or authors_str == \"\":\n",
        "        return []\n",
        "    return [a.strip().lower() for a in authors_str.split(';') if a.strip()]\n",
        "\n",
        "metadata_authors_df['author_list'] = metadata_authors_df['authors'].apply(clean_split_authors)\n",
        "\n",
        "metadata_exploded = metadata_authors_df.explode('author_list')\n",
        "metadata_exploded = metadata_exploded[metadata_exploded['author_list'] != '']\n",
        "metadata_exploded = metadata_exploded.rename(columns={'author_list': 'author_name'})\n",
        "print(f\"Exploded metadata shape: {metadata_exploded.shape}\")\n",
        "\n",
        "\n",
        "# Hitung Statistik Agregat per Penulis Unik\n",
        "print(\"Calculating aggregate statistics per author...\")\n",
        "author_stats = metadata_exploded.groupby('author_name')['cited_by_count'].agg(\n",
        "    total_citations = 'sum',\n",
        "    mean_citations = 'mean',\n",
        "    max_citations = 'max',\n",
        "    paper_count = 'count'\n",
        ").reset_index()\n",
        "\n",
        "print(f\"Calculated stats for {author_stats.shape[0]} unique authors.\")\n",
        "print(\"Author stats sample:\")\n",
        "print(author_stats.sort_values(by='total_citations', ascending=False).head())\n",
        "\n",
        "\n",
        "author_stats_dict = author_stats.set_index('author_name').to_dict('index')\n",
        "print(\"Created author_stats_dict for quick lookup.\")\n",
        "\n",
        "del metadata_authors_df, metadata_exploded, author_stats\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avUoOLxOB7JI",
        "outputId": "33684016-5af7-496b-9bdc-1865039294fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Author Citation Statistics Preprocessing...\n",
            "Filled NaN cited_by_count with 0.\n",
            "Exploded metadata shape: (14718, 4)\n",
            "Calculating aggregate statistics per author...\n",
            "Calculated stats for 10429 unique authors.\n",
            "Author stats sample:\n",
            "          author_name  total_citations  mean_citations  max_citations  \\\n",
            "7707    ross girshick           205965     7921.730769          44209   \n",
            "4714       kaiming he           185842     6637.214286          44209   \n",
            "9210   virginia braun           138239   138239.000000         138239   \n",
            "9160  victoria clarke           138239   138239.000000         138239   \n",
            "5175      leo breiman           117960    58980.000000         101861   \n",
            "\n",
            "      paper_count  \n",
            "7707           26  \n",
            "4714           28  \n",
            "9210            1  \n",
            "9160            1  \n",
            "5175            2  \n",
            "Created author_stats_dict for quick lookup.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating Author Citation Score Features...\")\n",
        "\n",
        "# Pastikan dictionary statistik penulis sudah dibuat di cell sebelumnya\n",
        "if 'author_stats_dict' not in locals() and 'author_stats_dict' not in globals():\n",
        "     raise NameError(\"Variable 'author_stats_dict' not defined. Please run the author stats preprocessing cell first.\")\n",
        "\n",
        "# Fungsi helper untuk mendapatkan statistik agregat penulis untuk sebuah paper\n",
        "def get_aggregated_author_stats(authors_str, author_stats_lookup, agg_methods=['mean', 'max', 'sum']):\n",
        "    try:\n",
        "        if pd.isna(authors_str) or authors_str == \"\":\n",
        "            author_list = []\n",
        "        else:\n",
        "            author_list = [a.strip().lower() for a in authors_str.split(';') if a.strip()]\n",
        "\n",
        "        # Kumpulkan statistik untuk setiap penulis di paper ini\n",
        "        author_metrics = {'total_citations': [], 'mean_citations': [], 'max_citations': [], 'paper_count': []}\n",
        "        found_authors = 0\n",
        "        for author in author_list:\n",
        "            stats = author_stats_lookup.get(author) # Lookup ke dict\n",
        "            if stats:\n",
        "                 found_authors += 1\n",
        "                 author_metrics['total_citations'].append(stats.get('total_citations', 0))\n",
        "                 author_metrics['mean_citations'].append(stats.get('mean_citations', 0))\n",
        "                 author_metrics['max_citations'].append(stats.get('max_citations', 0))\n",
        "                 author_metrics['paper_count'].append(stats.get('paper_count', 0))\n",
        "\n",
        "        # Jika tidak ada penulis yang ditemukan atau tidak ada penulis sama sekali\n",
        "        if not author_list or found_authors == 0:\n",
        "            return pd.Series([0.0] * (len(author_metrics) * len(agg_methods)))\n",
        "\n",
        "\n",
        "        # Hitung agregat (mean, max, sum) dari statistik penulis\n",
        "        results = []\n",
        "        for stat_name, values in author_metrics.items():\n",
        "             if not values:\n",
        "                 for method in agg_methods: results.append(0.0)\n",
        "                 continue\n",
        "\n",
        "             if 'mean' in agg_methods:\n",
        "                 results.append(np.mean(values) if values else 0.0)\n",
        "             if 'max' in agg_methods:\n",
        "                 results.append(np.max(values) if values else 0.0)\n",
        "             if 'sum' in agg_methods:\n",
        "                 results.append(np.sum(values) if values else 0.0)\n",
        "\n",
        "        return pd.Series(results)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing author stats for: '{authors_str}'. Error: {e}\")\n",
        "        return pd.Series([0.0] * (4 * len(agg_methods)))\n",
        "\n",
        "\n",
        "# Tentukan metode agregasi yang diinginkan\n",
        "aggregation_methods = ['mean', 'max', 'sum']\n",
        "new_author_stat_cols = []\n",
        "for stat in ['total_citations', 'mean_citations', 'max_citations', 'paper_count']:\n",
        "     for method in aggregation_methods:\n",
        "         new_author_stat_cols.append(f\"{method}_{stat}\")\n",
        "\n",
        "# Terapkan fungsi untuk p1 dan p2\n",
        "print(\"Applying author citation stats calculation to train_merged...\")\n",
        "train_merged[[f\"{col}_p1\" for col in new_author_stat_cols]] = train_merged.apply(\n",
        "    lambda row: get_aggregated_author_stats(row.get('authors_p1', ''), author_stats_dict, aggregation_methods),\n",
        "    axis=1\n",
        ")\n",
        "train_merged[[f\"{col}_p2\" for col in new_author_stat_cols]] = train_merged.apply(\n",
        "    lambda row: get_aggregated_author_stats(row.get('authors_p2', ''), author_stats_dict, aggregation_methods),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Applying author citation stats calculation to test_merged...\")\n",
        "test_merged[[f\"{col}_p1\" for col in new_author_stat_cols]] = test_merged.apply(\n",
        "    lambda row: get_aggregated_author_stats(row.get('authors_p1', ''), author_stats_dict, aggregation_methods),\n",
        "    axis=1\n",
        ")\n",
        "test_merged[[f\"{col}_p2\" for col in new_author_stat_cols]] = test_merged.apply(\n",
        "    lambda row: get_aggregated_author_stats(row.get('authors_p2', ''), author_stats_dict, aggregation_methods),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"Created {len(new_author_stat_cols)*2} new author citation score features.\")\n",
        "\n",
        "\n",
        "#  Buat Fitur Perbandingan (Difference/Ratio)\n",
        "print(\"Creating comparison features for author citation scores...\")\n",
        "comparison_features_created = []\n",
        "for col_base in new_author_stat_cols:\n",
        "     col_p1 = f\"{col_base}_p1\"\n",
        "     col_p2 = f\"{col_base}_p2\"\n",
        "     diff_col = f\"diff_{col_base}\"\n",
        "     ratio_col = f\"ratio_{col_base}\"\n",
        "\n",
        "     # Hitung Difference\n",
        "     train_merged[diff_col] = train_merged[col_p1] - train_merged[col_p2]\n",
        "     test_merged[diff_col] = test_merged[col_p1] - test_merged[col_p2]\n",
        "     comparison_features_created.append(diff_col)\n",
        "\n",
        "     # Hitung Ratio (handle division by zero)\n",
        "     train_merged[ratio_col] = train_merged[col_p1] / (train_merged[col_p2] + 1e-6)\n",
        "     test_merged[ratio_col] = test_merged[col_p1] / (test_merged[col_p2] + 1e-6)\n",
        "\n",
        "     train_merged[ratio_col] = train_merged[ratio_col].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "     test_merged[ratio_col] = test_merged[ratio_col].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "     comparison_features_created.append(ratio_col)\n",
        "\n",
        "print(f\"Created {len(comparison_features_created)} comparison features for author scores.\")\n",
        "\n",
        "\n",
        "# Tampilkan Hasil\n",
        "print(\"\\n--- Train Merged Head showing some new Author Citation Score Features ---\")\n",
        "# Tampilkan beberapa contoh fitur baru\n",
        "cols_to_show = ['mean_total_citations_p1', 'mean_total_citations_p2',\n",
        "                'max_total_citations_p1', 'max_total_citations_p2',\n",
        "                'diff_mean_total_citations', 'ratio_mean_total_citations',\n",
        "                'is_referenced']\n",
        "existing_cols_to_show = [f for f in cols_to_show if f in train_merged.columns]\n",
        "print(train_merged[existing_cols_to_show].head())\n",
        "\n",
        "print(\"\\n--- Describe some new Author Citation Score Features (Train) ---\")\n",
        "cols_to_describe = ['mean_total_citations_p1', 'max_total_citations_p1', 'diff_mean_total_citations', 'ratio_mean_total_citations']\n",
        "existing_cols_to_describe = [f for f in cols_to_describe if f in train_merged.columns]\n",
        "if existing_cols_to_describe:\n",
        "    print(train_merged[existing_cols_to_describe].describe())\n",
        "\n",
        "print(\"\\nChecking NaNs in a sample of new author citation score features...\")\n",
        "cols_to_check_nan = [f\"{col}_p1\" for col in new_author_stat_cols[:2]] + \\\n",
        "                    [f\"{col}_p2\" for col in new_author_stat_cols[:2]] + \\\n",
        "                    comparison_features_created[:2]\n",
        "existing_cols_to_check_nan = [f for f in cols_to_check_nan if f in train_merged.columns]\n",
        "if existing_cols_to_check_nan:\n",
        "     print(\"Train:\\n\", train_merged[existing_cols_to_check_nan].isnull().sum())\n",
        "     print(\"\\nTest:\\n\", test_merged[existing_cols_to_check_nan].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ0XnN63CBRm",
        "outputId": "e01dcff9-2407-4344-d7dd-1c4cdaef4074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Author Citation Score Features...\n",
            "Applying author citation stats calculation to train_merged...\n",
            "Applying author citation stats calculation to test_merged...\n",
            "Created 24 new author citation score features.\n",
            "Creating comparison features for author citation scores...\n",
            "Created 24 comparison features for author scores.\n",
            "\n",
            "--- Train Merged Head showing some new Author Citation Score Features ---\n",
            "   mean_total_citations_p1  mean_total_citations_p2  max_total_citations_p1  \\\n",
            "0               676.600000                  27744.0                  1955.0   \n",
            "1               981.000000                   3476.6                   981.0   \n",
            "2             16877.666667                    182.0                 17325.0   \n",
            "3             34921.600000                   1479.0                 58681.0   \n",
            "4               799.000000                    843.0                   799.0   \n",
            "\n",
            "   max_total_citations_p2  diff_mean_total_citations  \\\n",
            "0                 65209.0              -27067.400000   \n",
            "1                 11753.0               -2495.600000   \n",
            "2                   182.0               16695.666667   \n",
            "3                  1479.0               33442.600000   \n",
            "4                  1041.0                 -44.000000   \n",
            "\n",
            "   ratio_mean_total_citations  is_referenced  \n",
            "0                    0.024387              0  \n",
            "1                    0.282172              0  \n",
            "2                   92.734432              0  \n",
            "3                   23.611629              0  \n",
            "4                    0.947805              0  \n",
            "\n",
            "--- Describe some new Author Citation Score Features (Train) ---\n",
            "       mean_total_citations_p1  max_total_citations_p1  \\\n",
            "count            410691.000000           410691.000000   \n",
            "mean               4973.458566             9894.535648   \n",
            "std               12673.970303            25635.816086   \n",
            "min                   0.000000                0.000000   \n",
            "25%                 467.000000              575.000000   \n",
            "50%                1208.000000             1919.000000   \n",
            "75%                4047.800000             7275.000000   \n",
            "max              145252.500000           205965.000000   \n",
            "\n",
            "       diff_mean_total_citations  ratio_mean_total_citations  \n",
            "count              410691.000000                4.106910e+05  \n",
            "mean                 -109.768943                3.220430e+07  \n",
            "std                 18195.669904                1.189834e+09  \n",
            "min               -154792.333333                0.000000e+00  \n",
            "25%                 -2021.000000                2.296315e-01  \n",
            "50%                   122.000000                1.227051e+00  \n",
            "75%                  2334.500000                6.761478e+00  \n",
            "max                145252.500000                1.452525e+11  \n",
            "\n",
            "Checking NaNs in a sample of new author citation score features...\n",
            "Train:\n",
            " mean_total_citations_p1       0\n",
            "max_total_citations_p1        0\n",
            "mean_total_citations_p2       0\n",
            "max_total_citations_p2        0\n",
            "diff_mean_total_citations     0\n",
            "ratio_mean_total_citations    0\n",
            "dtype: int64\n",
            "\n",
            "Test:\n",
            " mean_total_citations_p1       0\n",
            "max_total_citations_p1        0\n",
            "mean_total_citations_p2       0\n",
            "max_total_citations_p2        0\n",
            "diff_mean_total_citations     0\n",
            "ratio_mean_total_citations    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aggregation_methods_used = ['mean', 'max', 'sum']\n",
        "base_stats_used = ['total_citations', 'mean_citations', 'max_citations', 'paper_count']\n",
        "author_stat_cols_bases = []\n",
        "for stat in base_stats_used:\n",
        "     for method in aggregation_methods_used:\n",
        "         author_stat_cols_bases.append(f\"{method}_{stat}\")\n",
        "\n",
        "author_stat_features_p1 = [f\"{col}_p1\" for col in author_stat_cols_bases]\n",
        "author_stat_features_p2 = [f\"{col}_p2\" for col in author_stat_cols_bases]\n",
        "author_stat_features_comp = []\n",
        "for col_base in author_stat_cols_bases:\n",
        "     author_stat_features_comp.append(f\"diff_{col_base}\")\n",
        "     author_stat_features_comp.append(f\"ratio_{col_base}\")"
      ],
      "metadata": {
        "id": "ZwF1GjDWXHbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: matched_words_count"
      ],
      "metadata": {
        "id": "Gy5GFnK_O79I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung berapa banyak jumlah kata yang sama untuk pasangan paper dan referenced_paper"
      ],
      "metadata": {
        "id": "La3WsQu6bwbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def unique_words_excluding_stopwords(text):\n",
        "  if pd.isna(text) or text == '':\n",
        "    return []\n",
        "  words = text.lower().split()\n",
        "  unique_words = []\n",
        "  for word in words:\n",
        "    if word not in stop_words and word not in unique_words:\n",
        "      unique_words.append(word)\n",
        "  return unique_words\n",
        "\n",
        "tqdm.pandas(desc=\"Extracting unique words\")\n",
        "paper_df['unique_words'] = paper_df['content'].progress_apply(unique_words_excluding_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA8yVtWZO_Kx",
        "outputId": "52e22c19-71b0-4ad4-af79-e7220b9e50f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Extracting unique words: 100%|██████████| 4354/4354 [05:21<00:00, 13.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_matched_words(row, paper_df):\n",
        "    paper_words = set(paper_df.loc[paper_df['paper_id'] == row['paper'], 'unique_words'].iloc[0])\n",
        "    ref_paper_words = set(paper_df.loc[paper_df['paper_id'] == row['referenced_paper'], 'unique_words'].iloc[0])\n",
        "    return len(paper_words.intersection(ref_paper_words))\n",
        "\n",
        "tqdm.pandas(desc=\"Counting matched words in train\")\n",
        "train_merged['matched_words_count'] = train_merged.progress_apply(lambda row: count_matched_words(row, paper_df), axis=1)\n",
        "\n",
        "tqdm.pandas(desc=\"Counting matched words in test\")\n",
        "test_merged['matched_words_count'] = test_merged.progress_apply(lambda row: count_matched_words(row, paper_df), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpCz7c24QPgA",
        "outputId": "419ae162-6ae0-4f1e-a393-cbdea74f13a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Counting matched words in train: 100%|██████████| 410691/410691 [15:38<00:00, 437.75it/s]\n",
            "Counting matched words in test: 100%|██████████| 336021/336021 [12:27<00:00, 449.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: TITLE-TinyBERT-L6-score"
      ],
      "metadata": {
        "id": "zrTC-MCsfA8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "menggunakan cross-encoder TinyBERT-L6 untuk mendapatkan skor \"retreival\" dari pasangan judul paper dan referenced_paper"
      ],
      "metadata": {
        "id": "2CWKOwr6b3DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multiprocessing.set_start_method('spawn', force=True)\n",
        "\n",
        "# Initialize the Cross-Encoder\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L6', device='cuda')\n",
        "\n",
        "def get_title_cross_encoder_score(row):\n",
        "    try:\n",
        "        query = row['title_p1'] if row['title_p1'] is not None else \"\"\n",
        "        passage = row['title_p2'] if row['title_p2'] is not None else \"\"\n",
        "        score = cross_encoder.predict([[query, passage]])[0]\n",
        "        return score\n",
        "    except IndexError:\n",
        "        return 0\n",
        "\n",
        "\n",
        "tqdm.pandas(desc=\"Calculating TITLE-TinyBERT-L6 Score for Train\")\n",
        "train_merged['TITLE-TinyBERT-L6-score'] = train_merged.progress_apply(\n",
        "    lambda row: get_title_cross_encoder_score(row), axis=1\n",
        ")\n",
        "\n",
        "tqdm.pandas(desc=\"Calculating TinyBERT-L6 Score for Test\")\n",
        "test_merged['TITLE-TinyBERT-L6-score'] = test_merged.progress_apply(\n",
        "    lambda row: get_title_cross_encoder_score(row), axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "-N0EoGR8fB8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: specterv2_similarity (chunking)"
      ],
      "metadata": {
        "id": "BUqoPR76ooMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proses ini melibatkan pemecahan teks setiap paper menjadi chunk-chunk kecil yang saling tumpang tindih untuk diubah menjadi vektor embedding menggunakan model specterv2. Kemudian, vektor-vektor embedding dari setiap chunk diagregasikan melalui mean pooling untuk menghasilkan satu vektor representasi tunggal bagi keseluruhan teks paper. Vektor tunggal inilah yang selanjutnya digunakan untuk menghitung tingkat kemiripan antara suatu paper dengan referenced_paper."
      ],
      "metadata": {
        "id": "5UqerpoDfH2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = 'allenai/specter2_base'\n",
        "MAX_SEQ_LENGTH = 512  # Maximum token length for SPECTER2\n",
        "CHUNK_OVERLAP = 50    # Overlap between text chunks\n",
        "BATCH_SIZE = 16       # Batch size for embedding chunks\n",
        "\n",
        "# Setup Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Checkpointing Setup\n",
        "embeddings_dir = os.path.join(base_path, 'paper_embeddings')\n",
        "os.makedirs(embeddings_dir, exist_ok=True)\n",
        "print(f\"Checkpoints will be saved in: {embeddings_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQLzMerpysp",
        "outputId": "dd58edd8-54e6-4cc5-8fa2-8fa79036beb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model: allenai/specter2_base\n",
            "Checkpoints will be saved in: /content/drive/MyDrive/gammafest/gammafest25/paper_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, tokenizer, max_length, overlap):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping tokenized chunks.\n",
        "    Returns a list of token ID lists.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Calculate effective chunk size (excluding special tokens like [CLS], [SEP])\n",
        "    effective_chunk_size = max_length - tokenizer.num_special_tokens_to_add()\n",
        "\n",
        "    if num_tokens <= effective_chunk_size:\n",
        "        return [tokenizer.encode(text, max_length=max_length, truncation=True)]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < num_tokens:\n",
        "        end = min(start + effective_chunk_size, num_tokens)\n",
        "        chunk_tokens = tokens[start:end]\n",
        "\n",
        "        chunk_with_specials = tokenizer.build_inputs_with_special_tokens(chunk_tokens)\n",
        "        if len(chunk_with_specials) > max_length:\n",
        "\n",
        "             chunk_with_specials = tokenizer.encode(\n",
        "                 tokenizer.decode(chunk_tokens),\n",
        "                 max_length=max_length,\n",
        "                 truncation=True\n",
        "             )\n",
        "\n",
        "\n",
        "        chunks.append(chunk_with_specials)\n",
        "\n",
        "        start += effective_chunk_size - overlap\n",
        "\n",
        "        if effective_chunk_size <= overlap:\n",
        "             start += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def get_pooled_embedding(token_ids, model, device):\n",
        "    \"\"\"\n",
        "    Gets the mean pooled embedding for a single sequence of token IDs.\n",
        "    Handles moving data to device and running the model.\n",
        "    \"\"\"\n",
        "    input_ids = torch.tensor([token_ids]).to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Perform mean pooling: average token embeddings, excluding padding\n",
        "        sum_embeddings = torch.sum(last_hidden_states * attention_mask.unsqueeze(-1), 1)\n",
        "        sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9) # Avoid division by zero\n",
        "        mean_pooled_embedding = sum_embeddings / sum_mask\n",
        "\n",
        "    return mean_pooled_embedding.squeeze(0) # Remove batch dimension\n",
        "\n",
        "def get_paper_embedding_batched(paper_id, paper_df, model, tokenizer, device, max_length, overlap, batch_size):\n",
        "    \"\"\"\n",
        "    Calculates the aggregated embedding for a paper using batched processing of chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        content = paper_df.loc[paper_id, 'content']\n",
        "        if pd.isna(content) or len(content.strip()) == 0:\n",
        "            print(f\"Warning: Paper ID {paper_id} has empty or missing content. Returning zeros.\")\n",
        "            return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "    except KeyError:\n",
        "        print(f\"Error: Paper ID {paper_id} not found in paper_df. Returning zeros.\")\n",
        "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "    # Get tokenized chunks\n",
        "    chunks_token_ids = chunk_text(content, tokenizer, max_length, overlap)\n",
        "\n",
        "    if not chunks_token_ids:\n",
        "         print(f\"Warning: Chunking returned no chunks for paper ID {paper_id}. Returning zeros.\")\n",
        "         return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "    all_chunk_embeddings = []\n",
        "\n",
        "     # Process chunks in batches\n",
        "    for i in range(0, len(chunks_token_ids), batch_size):\n",
        "        batch_chunks = chunks_token_ids[i:i + batch_size]\n",
        "\n",
        "        # Pad batch for consistent tensor shapes\n",
        "        inputs = tokenizer.pad(\n",
        "            {'input_ids': batch_chunks},\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_states = outputs.last_hidden_state # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "            # Perform mean pooling for the batch considering attention mask\n",
        "            attention_mask = inputs['attention_mask'].to(device) # (batch_size, seq_len)\n",
        "\n",
        "            # Sum token embeddings (zero out padded ones using mask)\n",
        "            sum_embeddings = torch.sum(last_hidden_states * attention_mask.unsqueeze(-1), 1) # (batch_size, hidden_size)\n",
        "\n",
        "            # Sum the attention mask to get the actual number of tokens per sequence (non-padding)\n",
        "            sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9) # (batch_size,) - Avoid division by zero\n",
        "\n",
        "            # Perform division. Unsqueeze sum_mask to enable correct broadcasting.\n",
        "            batch_pooled_embeddings = sum_embeddings / sum_mask.unsqueeze(-1) # (batch_size, hidden_size) <-- Corrected line!\n",
        "\n",
        "            all_chunk_embeddings.append(batch_pooled_embeddings.cpu()) # Move to CPU immediately\n",
        "\n",
        "\n",
        "    # Aggregate the chunk embeddings (mean pooling of the pooled chunks)\n",
        "    if all_chunk_embeddings:\n",
        "        all_chunk_embeddings_tensor = torch.cat(all_chunk_embeddings, dim=0) # (num_chunks, hidden_size)\n",
        "        aggregated_embedding = torch.mean(all_chunk_embeddings_tensor, dim=0) # (hidden_size,)\n",
        "        return aggregated_embedding.numpy().astype(np.float32) # Return as numpy array\n",
        "    else:\n",
        "        print(f\"Warning: No chunk embeddings generated for paper ID {paper_id}. Returning zeros.\")\n",
        "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "# Get all unique paper IDs\n",
        "all_paper_ids = pd.concat([\n",
        "    train_merged['paper'],\n",
        "    train_merged['referenced_paper'],\n",
        "    test_merged['paper'],\n",
        "    test_merged['referenced_paper']\n",
        "]).unique().tolist()\n",
        "\n",
        "print(f\"Found {len(all_paper_ids)} unique paper IDs to process.\")\n",
        "\n",
        "# Load existing embeddings and calculate missing ones (with checkpointing)\n",
        "paper_embeddings_map = {}\n",
        "processed_ids = set()\n",
        "print(\"Loading existing embeddings from checkpoints...\")\n",
        "\n",
        "# First pass: Load existing embeddings\n",
        "for paper_id in tqdm(all_paper_ids, desc=\"Loading Checkpoints\"):\n",
        "    checkpoint_path = os.path.join(embeddings_dir, f'{paper_id}.pt')\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            # Load to CPU first, then potentially move if needed later (though numpy is used for similarity)\n",
        "            embedding = torch.load(checkpoint_path, map_location='cpu')\n",
        "            # Ensure it's a numpy array float32\n",
        "            if isinstance(embedding, torch.Tensor):\n",
        "                embedding = embedding.numpy().astype(np.float32)\n",
        "            paper_embeddings_map[paper_id] = embedding\n",
        "            processed_ids.add(paper_id)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint for paper ID {paper_id}: {e}. Recalculating.\")\n",
        "\n",
        "missing_ids = [pid for pid in all_paper_ids if pid not in processed_ids]\n",
        "print(f\"{len(processed_ids)} embeddings loaded from checkpoints.\")\n",
        "print(f\"{len(missing_ids)} embeddings need to be calculated.\")\n",
        "\n",
        "# Second pass: Calculate and save missing embeddings\n",
        "if missing_ids:\n",
        "    print(\"Calculating and saving missing embeddings...\")\n",
        "    for paper_id in tqdm(missing_ids, desc=\"Calculating Embeddings\"):\n",
        "        checkpoint_path = os.path.join(embeddings_dir, f'{paper_id}.pt')\n",
        "        try:\n",
        "            # Calculate embedding (function handles batching of chunks internally)\n",
        "            embedding_np = get_paper_embedding_batched(\n",
        "                paper_id, paper_df, model, tokenizer, device,\n",
        "                MAX_SEQ_LENGTH, CHUNK_OVERLAP, BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            torch.save(torch.from_numpy(embedding_np).cpu(), checkpoint_path)\n",
        "\n",
        "            # Add to map\n",
        "            paper_embeddings_map[paper_id] = embedding_np\n",
        "            processed_ids.add(paper_id) # Mark as processed\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing paper ID {paper_id}: {e}\")\n",
        "\n",
        "# Verify all embeddings are loaded/calculated\n",
        "if len(paper_embeddings_map) != len(all_paper_ids):\n",
        "    print(\"Warning: Not all unique paper IDs have embeddings in the map after processing.\")\n",
        "    print(f\"Expected {len(all_paper_ids)}, got {len(paper_embeddings_map)}\")\n",
        "\n",
        "\n",
        "def calculate_similarity(df, embeddings_map, col1, col2):\n",
        "    \"\"\"\n",
        "    Calculates cosine similarity between embeddings for pairs of papers in a dataframe.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    print(f\"Calculating similarities for {col1} and {col2} pairs...\")\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Similarity\"):\n",
        "        id1 = row[col1]\n",
        "        id2 = row[col2]\n",
        "\n",
        "        emb1 = embeddings_map.get(id1)\n",
        "        emb2 = embeddings_map.get(id2)\n",
        "\n",
        "        if emb1 is None or emb2 is None:\n",
        "            print(f\"Warning: Embedding not found for pair ({id1}, {id2}). Setting similarity to NaN.\")\n",
        "            similarities.append(np.nan)\n",
        "        else:\n",
        "            try:\n",
        "                 sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
        "                 similarities.append(sim)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error calculating similarity for pair ({id1}, {id2}): {e}. Setting to NaN.\")\n",
        "                 similarities.append(np.nan)\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Calculate and add similarity to train_merged\n",
        "train_merged['specterv2_similarity'] = calculate_similarity(\n",
        "    train_merged, paper_embeddings_map, 'paper', 'referenced_paper'\n",
        ")\n",
        "\n",
        "# Calculate and add similarity to test_merged\n",
        "test_merged['specterv2_similarity'] = calculate_similarity(\n",
        "    test_merged, paper_embeddings_map, 'paper', 'referenced_paper'\n",
        ")\n",
        "\n",
        "print(\"Process completed. Dataframes saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "7321e12292c54600b43d7448f8350149",
            "eacf49ded09344a8b07dd99fa7bd2546",
            "6036c7f968aa4ca78a8796237394024b",
            "2a47134d17eb483298643dd486bf6af5",
            "861c6e27ba34454896951b414aedf013",
            "212c439007ef43369bd589722502b03c",
            "8109c3f0e6394e6db6885049f6ee2e5e",
            "e00656e191c14deebc84dae1aa1b8d79",
            "a2669458128a4985ad1dc44ee72742e5",
            "11a29a1cfcef4f0ba09b39801b415ed8",
            "d8d127b4cef84e57af690dfed7951c2e",
            "81b869caa9e24d74a81728fbcde45fa0",
            "e556058fcbcb4a4294e59df5886e64fe",
            "724eddd00dcd4f27a50bd91ec3c5e2ad",
            "115b2cdcf094423689ee540a4236ce4b",
            "e24a8af0801142ac8e4669bd7253f032",
            "94ac3407855e4fe18c191d0e6ef7817d",
            "6b5c068d1c7049e68be250da04672946",
            "6879585291a44506bc6973e0759a8e08",
            "a6dd455975654929b902f11b0bf488d2",
            "d43faa93273e4e0e8f3562e7fb1d0b88",
            "23089bc31b504118b7f421238f62a301",
            "776b938694e34fe1943468fbf91a8d56",
            "9ab9e036a383408ab49700f8d974f927",
            "bfd50cacbbbd4dffaa2f2abe96343be9",
            "1e6a3aa77fe44182a43f679576eac340",
            "1915bc31b7244f32bf5bfba8850bba98",
            "81e7ea6a3acf49f98db6c7a004def16f",
            "a870cce3712444b4807f66de9e0e2e56",
            "b63047605e914969aa13a75a0aaa3108",
            "afcb4fb49cc8409fab577e286fb55c61",
            "f2bc821acf9847ecb90d5c8924661925",
            "ca34e723e83b45fe9bffcfdefe43eba8",
            "aa9bb0472b5a47dfa59f0a563cf2c696",
            "134276b04eed45bf9d4fab6b1c28bfb8",
            "e6ec0a52db2946dfb728768332e3ba64",
            "bb428071b94e4f75a353b4e31c923a85",
            "528974af71ab4cc6a9d54ccba66557fb",
            "fc38cafe8d314083a5dc3e5b524113ff",
            "9d9592c02e1f4977b234dcb0a0325fab",
            "96940619509a4146a70fa6fe10666f00",
            "526ebefcddec45b499bd16c7924cf1f8",
            "585ab793cb9c4825be357e350ffdd3f9",
            "dd75de9c8f3a49a18a2eeec9e60f65a4"
          ]
        },
        "id": "s3JiI6NCormo",
        "outputId": "d39b2d6f-8379-4cc9-e18c-6c356c5384a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4354 unique paper IDs to process.\n",
            "Loading existing embeddings from checkpoints...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading Checkpoints:   0%|          | 0/4354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7321e12292c54600b43d7448f8350149"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 embeddings loaded from checkpoints.\n",
            "4354 embeddings need to be calculated.\n",
            "Calculating and saving missing embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating Embeddings:   0%|          | 0/4354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b869caa9e24d74a81728fbcde45fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating similarities for paper and referenced_paper pairs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating Similarity:   0%|          | 0/410691 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "776b938694e34fe1943468fbf91a8d56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating similarities for paper and referenced_paper pairs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating Similarity:   0%|          | 0/336021 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa9bb0472b5a47dfa59f0a563cf2c696"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modified dataframes...\n",
            "Process completed. Dataframes saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitur: scibert (chunking)"
      ],
      "metadata": {
        "id": "O8AV9SAI9-xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration (SciBERT)\n",
        "SCIBERT_MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
        "SCIBERT_MAX_SEQ_LENGTH = 512  # Maximum token length for SciBERT (BERT-based)\n",
        "SCIBERT_CHUNK_OVERLAP = 50    # Overlap between text chunks\n",
        "SCIBERT_BATCH_SIZE = 16       # Batch size for embedding chunks\n",
        "base_path_scibert = f'{base_path}/scibert_checkpoints'\n",
        "\n",
        "# Setup Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load SciBERT Model and Tokenizer\n",
        "print(f\"Loading SciBERT model: {SCIBERT_MODEL_NAME}\")\n",
        "scibert_tokenizer = AutoTokenizer.from_pretrained(SCIBERT_MODEL_NAME)\n",
        "scibert_model = AutoModel.from_pretrained(SCIBERT_MODEL_NAME).to(device)\n",
        "scibert_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# SciBERT Checkpointing Setup\n",
        "scibert_embeddings_dir = os.path.join(base_path_scibert, 'scibert_paper_embeddings')\n",
        "os.makedirs(scibert_embeddings_dir, exist_ok=True)\n",
        "print(f\"SciBERT checkpoints will be saved in: {scibert_embeddings_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "5c9b32e8fe7a4b54bd3e0aab54f42d46",
            "a115d28e445644b68a9922e293fe9338",
            "94fe5cb76756460f8aa13a04aa64735b",
            "d8db7b6758f44a3b9ce66795c3159095",
            "00404e5f36f5439e80aa7aef19f0a6c0",
            "610d46faeeaf43c09ecaded942d1df26",
            "e96f48865c01439fa2dc3505cb5a6f93",
            "8b77e8179c254ff5827a37eddd7f98a5",
            "ed06a49d83d5490ebed92b1468280191",
            "ef8c8f48ef834978b4b27074ef89b632",
            "68fc96c230234143a0a07ae762e5daf3",
            "f7e42a88752949ab9b879f870674cb65",
            "e02afe75b64f4e359864c65c82b891ca",
            "f9b057b39ecc40158b6c2b2c6befffd5",
            "af5c5a33eb6048918d7abccfad415dfd",
            "ba1abad662b74ae5b096171ad939c0f9",
            "d0a0952637b947aa9e907b1ae6e892b3",
            "a831f3f64ce0484e924dd618581ecf48",
            "8aa7828a5d0546598f799f3b7dea2b8e",
            "af232549f3a54425a7a21a7ac9e29313",
            "66a9945503b94117ab4ed426813a08d9",
            "f0e197d7863c438896a130560de3e176",
            "09357792143c40bfacf2309e2348966a",
            "e35a5cb6506247e291f71c0a24ff5a26",
            "586463985b364cb6a780f1004c8723a3",
            "1bdc26e3de304cdf80f75ebac00e28c3",
            "ffeda570f3c745f48d0a6af33a5b03cf",
            "fec50b02c01d4b49a4984203689b7250",
            "eaac03911e3644078686986f1772d4fe",
            "7b6df432fc6e4c94ba7a61826799f11e",
            "c2f06f31cf464edcabf6a2f6900e057d",
            "cdf2c82cb2b341279dc96a69331c73ba",
            "fb8ecd1b94d04c3a968d7328294625c7",
            "aabe8b4ecafe4852a9f8099f154ec11a",
            "624f7256b53948ee8848e77fbf04a6c3",
            "4b372fda4ddd490fb3b529449035467f",
            "c4a5d33d9d78411eb4cb4c61bec09a7a",
            "ff0cc3e043ab40cf844b4cbcff06ed42",
            "475df0b23b43408cbdc582e62d4e4bf4",
            "a7c07b11ed1b4c4795608115a52e3220",
            "64853a7c4d21491ebf15c9178e1e1613",
            "874fe5be17ed4f2c91fc98847f79544b",
            "0e0f07841adc4be9ba040af2ec0fcb4f",
            "a431d2e0e3fb4e64953b5c7501166445"
          ]
        },
        "id": "2lHHTahr-i35",
        "outputId": "5aa2ad33-61f7-4229-978d-8e1ab2081d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading SciBERT model: allenai/scibert_scivocab_uncased\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c9b32e8fe7a4b54bd3e0aab54f42d46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7e42a88752949ab9b879f870674cb65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09357792143c40bfacf2309e2348966a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aabe8b4ecafe4852a9f8099f154ec11a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SciBERT checkpoints will be saved in: /content/drive/MyDrive/gammafest/gammafest25//scibert_checkpoints/scibert_paper_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions (Reused/Adapted)\n",
        "\n",
        "def chunk_text(text, tokenizer, max_length, overlap):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping tokenized chunks.\n",
        "    Returns a list of token ID lists.\n",
        "    \"\"\"\n",
        "    # Tokenize the text without adding special tokens initially\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Calculate effective chunk size (excluding special tokens like [CLS], [SEP])\n",
        "    effective_chunk_size = max_length - tokenizer.num_special_tokens_to_add(pair=False) # Use pair=False for single sequence\n",
        "\n",
        "    if num_tokens <= effective_chunk_size:\n",
        "        encoded_text = tokenizer.encode(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            truncation=\"only_first\", # Truncate the first sequence if needed\n",
        "            padding=\"max_length\", # Pad to max_length if shorter\n",
        "            return_attention_mask=False,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        return [encoded_text]\n",
        "\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < num_tokens:\n",
        "        end = min(start + effective_chunk_size, num_tokens)\n",
        "        chunk_tokens = tokens[start:end]\n",
        "\n",
        "        # Encode the chunk segment, adding special tokens and potentially truncating if needed\n",
        "        encoded_chunk = tokenizer.encode(\n",
        "            tokenizer.decode(chunk_tokens), # Decode chunk tokens back to text segment\n",
        "            max_length=max_length,\n",
        "            truncation=\"only_first\",\n",
        "            padding=\"max_length\", # Pad to max_length\n",
        "            return_attention_mask=False,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        chunks.append(encoded_chunk)\n",
        "\n",
        "        # Calculate the start of the next chunk\n",
        "        start += effective_chunk_size - overlap\n",
        "\n",
        "        # Ensure we don't move backwards or stay in the same place if overlap >= effective_chunk_size\n",
        "        if effective_chunk_size <= overlap and start <= (start - (effective_chunk_size - overlap)):\n",
        "             start = (start - (effective_chunk_size - overlap)) + 1 # Move at least one token forward from the true start of the previous chunk segment\n",
        "\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_paper_embedding_batched(paper_id, paper_df, model, tokenizer, device, max_length, overlap, batch_size):\n",
        "    \"\"\"\n",
        "    Calculates the aggregated embedding for a paper using batched processing of chunks.\n",
        "    Uses the provided model and tokenizer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use .loc for index-based lookup\n",
        "        content = paper_df.loc[paper_id, 'content']\n",
        "        if pd.isna(content) or len(content.strip()) == 0:\n",
        "            print(f\"Warning: Paper ID {paper_id} has empty or missing content. Returning zeros.\")\n",
        "            return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "    except KeyError:\n",
        "        print(f\"Error: Paper ID {paper_id} not found in paper_df index. Returning zeros.\")\n",
        "        # Return zero vector with the correct hidden size for the model\n",
        "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "    # Get tokenized chunks\n",
        "    # Pass SciBERT-specific max_length and overlap\n",
        "    chunks_token_ids = chunk_text(content, tokenizer, max_length, overlap)\n",
        "\n",
        "    if not chunks_token_ids:\n",
        "         print(f\"Warning: Chunking returned no chunks for paper ID {paper_id}. Returning zeros.\")\n",
        "         # Return zero vector with the correct hidden size for the model\n",
        "         return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "    all_chunk_embeddings = []\n",
        "\n",
        "    # Process chunks in batches\n",
        "    for i in range(0, len(chunks_token_ids), batch_size):\n",
        "        batch_chunks = chunks_token_ids[i:i + batch_size]\n",
        "\n",
        "        # Pad batch for consistent tensor shapes\n",
        "        # Use the provided tokenizer\n",
        "        inputs = tokenizer.pad(\n",
        "            {'input_ids': batch_chunks},\n",
        "            return_tensors='pt',\n",
        "            padding=True, # or 'longest'\n",
        "            # truncation is handled in chunk_text\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Use the provided model\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_states = outputs.last_hidden_state # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "            # Perform mean pooling for the batch considering attention mask\n",
        "            attention_mask = inputs['attention_mask'].to(device) # (batch_size, seq_len)\n",
        "\n",
        "            # Sum token embeddings (zero out padded ones using mask)\n",
        "            sum_embeddings = torch.sum(last_hidden_states * attention_mask.unsqueeze(-1), 1) # (batch_size, hidden_size)\n",
        "\n",
        "            # Sum the attention mask to get the actual number of tokens per sequence (non-padding)\n",
        "            sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9) # (batch_size,) - Avoid division by zero\n",
        "\n",
        "            # Perform division. Unsqueeze sum_mask to enable correct broadcasting.\n",
        "            batch_pooled_embeddings = sum_embeddings / sum_mask.unsqueeze(-1) # (batch_size, hidden_size)\n",
        "\n",
        "            all_chunk_embeddings.append(batch_pooled_embeddings.cpu()) # Move to CPU immediately to save GPU memory\n",
        "\n",
        "\n",
        "    if all_chunk_embeddings:\n",
        "        all_chunk_embeddings_tensor = torch.cat(all_chunk_embeddings, dim=0) # (num_chunks, hidden_size)\n",
        "        aggregated_embedding = torch.mean(all_chunk_embeddings_tensor, dim=0) # (hidden_size,)\n",
        "        return aggregated_embedding.numpy().astype(np.float32) # Return as numpy array\n",
        "    else:\n",
        "        print(f\"Warning: No chunk embeddings generated for paper ID {paper_id}. Returning zeros.\")\n",
        "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
        "\n",
        "\n",
        "# Get all unique paper IDs\n",
        "all_paper_ids = pd.concat([\n",
        "    train_merged['paper'],\n",
        "    train_merged['referenced_paper'],\n",
        "    test_merged['paper'],\n",
        "    test_merged['referenced_paper']\n",
        "]).unique().tolist()\n",
        "\n",
        "print(f\"Found {len(all_paper_ids)} unique paper IDs to process for SciBERT.\")\n",
        "\n",
        "# Load existing SciBERT embeddings and calculate missing ones (with checkpointing)\n",
        "scibert_paper_embeddings_map = {}\n",
        "scibert_processed_ids = set()\n",
        "print(\"Loading existing SciBERT embeddings from checkpoints...\")\n",
        "\n",
        "# First pass: Load existing embeddings\n",
        "for paper_id in tqdm(all_paper_ids, desc=\"Loading SciBERT Checkpoints\"):\n",
        "    checkpoint_path = os.path.join(scibert_embeddings_dir, f'{paper_id}.pt')\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            embedding = torch.load(checkpoint_path, map_location='cpu')\n",
        "            if isinstance(embedding, torch.Tensor):\n",
        "                embedding = embedding.numpy().astype(np.float32)\n",
        "            if embedding.shape[0] == scibert_model.config.hidden_size:\n",
        "                 scibert_paper_embeddings_map[paper_id] = embedding\n",
        "                 scibert_processed_ids.add(paper_id)\n",
        "            else:\n",
        "                 print(f\"Warning: Checkpoint for {paper_id} has wrong size ({embedding.shape[0]} vs {scibert_model.config.hidden_size}). Recalculating.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint for paper ID {paper_id}: {e}. Recalculating.\")\n",
        "\n",
        "scibert_missing_ids = [pid for pid in all_paper_ids if pid not in scibert_processed_ids]\n",
        "print(f\"{len(scibert_processed_ids)} SciBERT embeddings loaded from checkpoints.\")\n",
        "print(f\"{len(scibert_missing_ids)} SciBERT embeddings need to be calculated.\")\n",
        "\n",
        "# Second pass: Calculate and save missing embeddings\n",
        "if scibert_missing_ids:\n",
        "    print(\"Calculating and saving missing SciBERT embeddings...\")\n",
        "    for paper_id in tqdm(scibert_missing_ids, desc=\"Calculating SciBERT Embeddings\"):\n",
        "        checkpoint_path = os.path.join(scibert_embeddings_dir, f'{paper_id}.pt')\n",
        "        try:\n",
        "            # Calculate embedding using the SciBERT model, tokenizer, and config\n",
        "            embedding_np = get_paper_embedding_batched(\n",
        "                paper_id, paper_df, scibert_model, scibert_tokenizer, device,\n",
        "                SCIBERT_MAX_SEQ_LENGTH, SCIBERT_CHUNK_OVERLAP, SCIBERT_BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            # Save the calculated embedding (save numpy array directly or convert to tensor)\n",
        "            torch.save(torch.from_numpy(embedding_np).cpu(), checkpoint_path)\n",
        "\n",
        "            # Add to map\n",
        "            scibert_paper_embeddings_map[paper_id] = embedding_np\n",
        "            scibert_processed_ids.add(paper_id) # Mark as processed\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing paper ID {paper_id}: {e}\")\n",
        "\n",
        "# Verify all SciBERT embeddings are loaded/calculated\n",
        "if len(scibert_paper_embeddings_map) != len(all_paper_ids):\n",
        "    print(\"Warning: Not all unique paper IDs have SciBERT embeddings in the map after processing.\")\n",
        "    print(f\"Expected {len(all_paper_ids)}, got {len(scibert_paper_embeddings_map)}\")\n",
        "\n",
        "\n",
        "def calculate_similarity(df, embeddings_map, col1, col2):\n",
        "    \"\"\"\n",
        "    Calculates cosine similarity between embeddings for pairs of papers in a dataframe.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    print(f\"Calculating similarities for {col1} and {col2} pairs...\")\n",
        "    # Use iterrows cautiously on large dataframes, consider apply or vectorization if possible\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Similarity\"):\n",
        "        id1 = row[col1]\n",
        "        id2 = row[col2]\n",
        "\n",
        "        emb1 = embeddings_map.get(id1)\n",
        "        emb2 = embeddings_map.get(id2)\n",
        "\n",
        "        if emb1 is None or emb2 is None:\n",
        "            # Handle cases where an embedding was not found\n",
        "            print(f\"Warning: SciBERT embedding not found for pair ({id1}, {id2}). Setting similarity to NaN.\")\n",
        "            similarities.append(np.nan)\n",
        "        else:\n",
        "            # Ensure embeddings are numpy arrays and calculate cosine similarity\n",
        "            try:\n",
        "                 # Ensure embeddings are 2D arrays for cosine_similarity(X, Y) where X and Y are samples\n",
        "                 sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
        "                 similarities.append(sim)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error calculating similarity for pair ({id1}, {id2}): {e}. Setting to NaN.\")\n",
        "                 similarities.append(np.nan)\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Calculate and add similarity to train_merged\n",
        "train_merged['scibert_similarity'] = calculate_similarity(\n",
        "    train_merged, scibert_paper_embeddings_map, 'paper', 'referenced_paper'\n",
        ")\n",
        "\n",
        "# Calculate and add similarity to test_merged\n",
        "test_merged['scibert_similarity'] = calculate_similarity(\n",
        "    test_merged, scibert_paper_embeddings_map, 'paper', 'referenced_paper'\n",
        ")\n",
        "\n",
        "print(\"SciBERT similarity calculation process completed. Dataframes saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "79762e6e48fd4b5f871a5317d90f7f73",
            "3989a1a29f3b48299f33f6bc57c3bde0",
            "df15ab1a65a546dc870e7c37f89e2637",
            "c6e38ce681ff49dd9a39c815eaaedb86",
            "03ad2b2a2e5e4c019b194ac91ea847aa",
            "f473c095635e47648e15c369c0f9a146",
            "a2e8362380e84fbf922108bc3b703f60",
            "41f4696080ca42c98905a05b9a3dfd90",
            "eb20adbf2b3945cfab9beee3cf6b7bb6",
            "b517e704754b405c9be23eb0c48d60f9",
            "e490c8e19e3c45f5b5d5ddcd4cfb0be1",
            "ad272040b83545959a980c4e87130ede",
            "6a34fb6016514e21b99e2deee33116a9",
            "adcb57d49af54a899bab46564740f2b4",
            "abc6b8e6cd814545b2509e96ce87ffb2",
            "d2d19fe4af164c48be6c25230786920b",
            "54b2340468124960a6814fa0d5763c80",
            "70471ccd64114dfcb7edc1836f6d2f67",
            "1409fd1d1ab4440fbb9ec2597a345c78",
            "365f1e2ed47f40baa5401e8f3ca08870",
            "1850aecc37bf4383951e1900e0c75ff5",
            "64358a8811094c699c8c64d1261eeff7",
            "8609da6e0e554904b89eccb0228691fc",
            "3d0ed2129ce9408c9c413b873ee48192",
            "d27204351de24d6d8ebfb92efbf19d75",
            "405a4d2c3fdb46a3918b36d63e7035d7",
            "d8de2e3603c34d83b00b379ba91f05f3",
            "89f241962bb04fb7b8ac2e78d2541889",
            "186cddc2b5594928843f83c59b10a62c",
            "d02d39b9f9fa4c4da8c4bf49d4f74a19",
            "cfc0d27d67314138a1d7a970ead95e58",
            "cd9716269c3343fda48d630cf0b0d6aa",
            "6a6f76535b16465cb75eaadf9996400b",
            "32ce1b82b03542c48e7dd65ece2cd978",
            "ba107ad84ca14c7eb0f520334bf1dc9c",
            "3bb471cff5f141458e115ce864c032df",
            "15b40ee28004423f9898d0ff1b46baa4",
            "28021b14b12743a3a2c8d9726fb1c8b0",
            "7187e902e2ed4a60b93c2be267dcb1bf",
            "bd6a40134863496a9b95d00db25acb42",
            "0c5fcfc3ec80464c95f173527b98087f",
            "a6391222275d4a31adc50a56cab6877c",
            "fe367f4c3ee540aeae8430034976b3e3",
            "ce315401820943aa9f8df1537ea4baf1"
          ]
        },
        "id": "HYdHRAQG-AoL",
        "outputId": "fa17e105-f8c5-423c-febc-3bccd4ae6549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4354 unique paper IDs to process for SciBERT.\n",
            "Loading existing SciBERT embeddings from checkpoints...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading SciBERT Checkpoints:   0%|          | 0/4354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79762e6e48fd4b5f871a5317d90f7f73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 SciBERT embeddings loaded from checkpoints.\n",
            "4354 SciBERT embeddings need to be calculated.\n",
            "Calculating and saving missing SciBERT embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating SciBERT Embeddings:   0%|          | 0/4354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad272040b83545959a980c4e87130ede"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating similarities for paper and referenced_paper pairs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating Similarity:   0%|          | 0/410691 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8609da6e0e554904b89eccb0228691fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating similarities for paper and referenced_paper pairs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating Similarity:   0%|          | 0/336021 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32ce1b82b03542c48e7dd65ece2cd978"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modified dataframes with SciBERT similarity...\n",
            "SciBERT similarity calculation process completed. Dataframes saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: specterv2_similarity (first 400 words)"
      ],
      "metadata": {
        "id": "d5zslI3WYAzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "METADATA_FILE = metadata_file\n",
        "MODEL_NAME = 'allenai/specter2_base'\n",
        "EMBEDDING_FILE = f'paper_embeddings_{MODEL_NAME.split(\"/\")[-1]}_from_paper_df.pkl'\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 512\n",
        "WORDS_TO_USE = 400\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "# Basic cleaning - apply to title and content before combining\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    # Basic cleaning: remove newline, lower, strip, multiple spaces\n",
        "    text = text.replace('\\n', \" \").strip().lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load Metadata (for Titles)\n",
        "print(f\"Loading metadata (for titles) from {METADATA_FILE}...\")\n",
        "try:\n",
        "    metadata_df = pd.read_csv(METADATA_FILE, usecols=['paper_id', 'title']) # Only load needed columns\n",
        "    if 'paper_id' not in metadata_df.columns or 'title' not in metadata_df.columns:\n",
        "         raise ValueError(\"Metadata CSV must contain 'paper_id' and 'title' columns.\")\n",
        "    print(f\"Loaded titles for {len(metadata_df)} papers.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Metadata file not found at {METADATA_FILE}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading metadata: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Handle missing titles\n",
        "metadata_df['title'] = metadata_df['title'].fillna('')\n",
        "metadata_df['title_processed'] = metadata_df['title'].apply(preprocess_text)\n",
        "\n",
        "# 2. Prepare Text Data from paper_df\n",
        "# Ensure paper_df exists and has the correct columns\n",
        "if 'paper_df' not in locals():\n",
        "    print(\"Error: paper_df DataFrame not found. Please load it first.\")\n",
        "    # Example: paper_df = pd.read_parquet('path/to/your/paper_df.parquet')\n",
        "    exit()\n",
        "if not all(col in paper_df.columns for col in ['paper_id', 'content']):\n",
        "     raise ValueError(\"paper_df must contain 'paper_id' and 'content' columns.\")\n",
        "\n",
        "print(f\"Processing content from loaded paper_df ({len(paper_df)} rows)...\")\n",
        "# Handle missing content\n",
        "paper_df['content'] = paper_df['content'].fillna('')\n",
        "\n",
        "# Take the start of the full text content\n",
        "def get_content_subset(text, words_to_use):\n",
        "    words = str(text).split()\n",
        "    return \" \".join(words[:words_to_use])\n",
        "\n",
        "paper_df['content_subset'] = paper_df['content'].apply(lambda x: get_content_subset(x, WORDS_TO_USE))\n",
        "paper_df['content_subset_processed'] = paper_df['content_subset'].apply(preprocess_text)\n",
        "\n",
        "# 3. Merge Title and Content Subset\n",
        "print(\"Merging titles with processed content...\")\n",
        "\n",
        "merged_df = pd.merge(\n",
        "    metadata_df[['paper_id', 'title_processed']],\n",
        "    paper_df[['paper_id', 'content_subset_processed']],\n",
        "    on='paper_id',\n",
        "    how='inner'\n",
        ")\n",
        "print(f\"Merged data contains {len(merged_df)} papers.\")\n",
        "\n",
        "# Combine title and the start of the full text\n",
        "merged_df['text_to_embed'] = merged_df['title_processed'] + ' ' + merged_df['content_subset_processed']\n",
        "# If content subset ended up empty, just use title\n",
        "merged_df['text_to_embed'] = merged_df.apply(\n",
        "    lambda row: row['title_processed'] if not row['content_subset_processed'] else row['text_to_embed'],\n",
        "    axis=1\n",
        ")\n",
        "merged_df['text_to_embed'] = merged_df['text_to_embed'].fillna('')\n",
        "\n",
        "# Get unique papers to embed (using the merged data)\n",
        "papers_to_embed_df = merged_df[['paper_id', 'text_to_embed']].drop_duplicates('paper_id')\n",
        "papers_to_embed_df = papers_to_embed_df[papers_to_embed_df['text_to_embed'].str.len() > 0]\n",
        "\n",
        "papers_to_embed = papers_to_embed_df.set_index('paper_id')\n",
        "paper_ids = papers_to_embed.index.tolist()\n",
        "texts = papers_to_embed['text_to_embed'].tolist()\n",
        "print(f\"Prepared text for {len(paper_ids)} unique papers with non-empty content.\")\n",
        "\n",
        "# 4. Load Pre-trained Model\n",
        "print(f\"Loading sentence-transformer model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.max_seq_length = MAX_LENGTH\n",
        "print(\"Model loaded.\")\n",
        "print(f\"Model will run on device: {model.device}\")\n",
        "\n",
        "\n",
        "# 5. Generate Embeddings (Batch Processing)\n",
        "print(f\"Starting embedding generation with batch size {BATCH_SIZE}...\")\n",
        "paper_embeddings = {}\n",
        "if texts:\n",
        "    embeddings_np = model.encode(\n",
        "        texts,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    for paper_id, embedding in zip(paper_ids, embeddings_np):\n",
        "         paper_embeddings[paper_id] = embedding.astype(np.float16) # float16 saves space\n",
        "    print(f\"Finished embedding generation.\")\n",
        "    if len(paper_embeddings) > 0:\n",
        "         print(f\"Sample embedding shape: {list(paper_embeddings.values())[0].shape}, dtype: {list(paper_embeddings.values())[0].dtype}\")\n",
        "else:\n",
        "     print(\"Warning: No non-empty texts found to generate embeddings.\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Cleaned up model from memory.\")\n",
        "\n",
        "\n",
        "# 6. Save Embeddings\n",
        "print(f\"Saving embeddings to {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'wb') as f_out:\n",
        "        pickle.dump(paper_embeddings, f_out)\n",
        "    print(\"Embeddings saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n"
      ],
      "metadata": {
        "id": "OM0vuhGBrmts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/kaggle/input/gammafest-pmo/paper_embeddings_specter2_base_from_paper_df (1).pkl'\n",
        "\n",
        "print(f\"Loading embeddings from {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'rb') as f_in:\n",
        "        paper_embeddings = pickle.load(f_in)\n",
        "    print(f\"Loaded {len(paper_embeddings)} embeddings.\")\n",
        "    if not paper_embeddings:\n",
        "        raise ValueError(\"Embeddings dictionary is empty!\")\n",
        "    embedding_dim = list(paper_embeddings.values())[0].shape[0]\n",
        "    default_embedding = np.zeros(embedding_dim, dtype=np.float16) # Match dtype\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embedding file not found at {EMBEDDING_FILE}\")\n",
        "    print(\"Please run the embedding generation script first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calculate Pairwise Embedding Similarity ---\n",
        "print(\"Calculating pairwise embedding similarity...\")\n",
        "\n",
        "def get_embedding_similarity(row, embeddings_dict, default_emb):\n",
        "    \"\"\"Calculates cosine similarity between embeddings of paper pairs.\"\"\"\n",
        "    emb1 = embeddings_dict.get(row['paper'], default_emb).reshape(1, -1).astype(np.float32) # Use float32 for calculation\n",
        "    emb2 = embeddings_dict.get(row['referenced_paper'], default_emb).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    sim = cosine_similarity(emb1, emb2)[0, 0]\n",
        "    # Clip similarity to [0, 1] range just in case of floating point issues\n",
        "    return np.clip(sim, 0.0, 1.0)\n",
        "\n",
        "# Apply to train\n",
        "tqdm.pandas(desc=\"Calculating Train Similarity\")\n",
        "train_merged['specterv2_similarity'] = train_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "# Apply to test\n",
        "tqdm.pandas(desc=\"Calculating Test Similarity\")\n",
        "test_merged['specterv2_similarity'] = test_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "\n",
        "# Fill any NaNs (e.g., if similarity couldn't be calculated)\n",
        "train_merged['specterv2_similarity'] = train_merged['specterv2_similarity'].fillna(0.0)\n",
        "test_merged['specterv2_similarity'] = test_merged['specterv2_similarity'].fillna(0.0)\n",
        "\n",
        "print(\"Finished calculating embedding similarity.\")\n",
        "print(train_merged[['specterv2_similarity']].describe())"
      ],
      "metadata": {
        "id": "7rwjCd-OuIdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: scibert_similarity (first 400 words)"
      ],
      "metadata": {
        "id": "m-UqjuslYF32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
        "EMBEDDING_FILE = f'paper_embeddings_{MODEL_NAME.split(\"/\")[-1]}_from_paper_df.pkl'\n",
        "# Load Pre-trained Model\n",
        "print(f\"Loading sentence-transformer model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.max_seq_length = MAX_LENGTH\n",
        "print(\"Model loaded.\")\n",
        "print(f\"Model will run on device: {model.device}\")\n",
        "\n",
        "\n",
        "# 5. Generate Embeddings (Batch Processing)\n",
        "print(f\"Starting embedding generation with batch size {BATCH_SIZE}...\")\n",
        "paper_embeddings = {}\n",
        "if texts:\n",
        "    embeddings_np = model.encode(\n",
        "        texts,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    for paper_id, embedding in zip(paper_ids, embeddings_np):\n",
        "         paper_embeddings[paper_id] = embedding.astype(np.float16) # float16 saves space\n",
        "    print(f\"Finished embedding generation.\")\n",
        "    if len(paper_embeddings) > 0:\n",
        "         print(f\"Sample embedding shape: {list(paper_embeddings.values())[0].shape}, dtype: {list(paper_embeddings.values())[0].dtype}\")\n",
        "else:\n",
        "     print(\"Warning: No non-empty texts found to generate embeddings.\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Cleaned up model from memory.\")\n",
        "\n",
        "\n",
        "# 6. Save Embeddings\n",
        "print(f\"Saving embeddings to {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'wb') as f_out:\n",
        "        pickle.dump(paper_embeddings, f_out)\n",
        "    print(\"Embeddings saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n"
      ],
      "metadata": {
        "id": "D97Y-1ZFrnAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/kaggle/input/gammafest/paper_embeddings_scibert_scivocab_uncased_from_paper_df.pkl'\n",
        "\n",
        "print(f\"Loading embeddings from {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'rb') as f_in:\n",
        "        paper_embeddings = pickle.load(f_in)\n",
        "    print(f\"Loaded {len(paper_embeddings)} embeddings.\")\n",
        "    if not paper_embeddings:\n",
        "        raise ValueError(\"Embeddings dictionary is empty!\")\n",
        "    embedding_dim = list(paper_embeddings.values())[0].shape[0]\n",
        "    default_embedding = np.zeros(embedding_dim, dtype=np.float16) # Match dtype\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embedding file not found at {EMBEDDING_FILE}\")\n",
        "    print(\"Please run the embedding generation script first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calculate Pairwise Embedding Similarity ---\n",
        "print(\"Calculating pairwise embedding similarity...\")\n",
        "\n",
        "def get_embedding_similarity(row, embeddings_dict, default_emb):\n",
        "    \"\"\"Calculates cosine similarity between embeddings of paper pairs.\"\"\"\n",
        "    emb1 = embeddings_dict.get(row['paper'], default_emb).reshape(1, -1).astype(np.float32) # Use float32 for calculation\n",
        "    emb2 = embeddings_dict.get(row['referenced_paper'], default_emb).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    sim = cosine_similarity(emb1, emb2)[0, 0]\n",
        "    # Clip similarity to [0, 1] range just in case of floating point issues\n",
        "    return np.clip(sim, 0.0, 1.0)\n",
        "\n",
        "# Apply to train\n",
        "tqdm.pandas(desc=\"Calculating Train Similarity\")\n",
        "train_merged['scibert_similarity'] = train_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "# Apply to test\n",
        "tqdm.pandas(desc=\"Calculating Test Similarity\")\n",
        "test_merged['scibert_similarity'] = test_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "\n",
        "# Fill any NaNs (e.g., if similarity couldn't be calculated)\n",
        "train_merged['scibert_similarity'] = train_merged['scibert_similarity'].fillna(0.0)\n",
        "test_merged['scibert_similarity'] = test_merged['scibert_similarity'].fillna(0.0)\n",
        "\n",
        "print(\"Finished calculating embedding similarity.\")\n",
        "print(train_merged[['scibert_similarity']].describe())"
      ],
      "metadata": {
        "id": "UqOq6QHIvHKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: multi-qa-mpnet Similiarity(first 400 words)"
      ],
      "metadata": {
        "id": "DklGilM2v52S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
        "EMBEDDING_FILE = f'paper_embeddings_{MODEL_NAME.split(\"/\")[-1]}_from_paper_df.pkl'\n",
        "# Load Pre-trained Model\n",
        "print(f\"Loading sentence-transformer model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.max_seq_length = MAX_LENGTH\n",
        "print(\"Model loaded.\")\n",
        "print(f\"Model will run on device: {model.device}\")\n",
        "\n",
        "\n",
        "# 5. Generate Embeddings (Batch Processing)\n",
        "print(f\"Starting embedding generation with batch size {BATCH_SIZE}...\")\n",
        "paper_embeddings = {}\n",
        "if texts:\n",
        "    embeddings_np = model.encode(\n",
        "        texts,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    for paper_id, embedding in zip(paper_ids, embeddings_np):\n",
        "         paper_embeddings[paper_id] = embedding.astype(np.float16) # float16 saves space\n",
        "    print(f\"Finished embedding generation.\")\n",
        "    if len(paper_embeddings) > 0:\n",
        "         print(f\"Sample embedding shape: {list(paper_embeddings.values())[0].shape}, dtype: {list(paper_embeddings.values())[0].dtype}\")\n",
        "else:\n",
        "     print(\"Warning: No non-empty texts found to generate embeddings.\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Cleaned up model from memory.\")\n",
        "\n",
        "\n",
        "# 6. Save Embeddings\n",
        "print(f\"Saving embeddings to {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'wb') as f_out:\n",
        "        pickle.dump(paper_embeddings, f_out)\n",
        "    print(\"Embeddings saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n"
      ],
      "metadata": {
        "id": "4UYrAVWjwaYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/kaggle/input/gammafest/paper_embeddings_scibert_scivocab_uncased_from_paper_df.pkl'\n",
        "\n",
        "print(f\"Loading embeddings from {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'rb') as f_in:\n",
        "        paper_embeddings = pickle.load(f_in)\n",
        "    print(f\"Loaded {len(paper_embeddings)} embeddings.\")\n",
        "    if not paper_embeddings:\n",
        "        raise ValueError(\"Embeddings dictionary is empty!\")\n",
        "    embedding_dim = list(paper_embeddings.values())[0].shape[0]\n",
        "    default_embedding = np.zeros(embedding_dim, dtype=np.float16) # Match dtype\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embedding file not found at {EMBEDDING_FILE}\")\n",
        "    print(\"Please run the embedding generation script first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calculate Pairwise Embedding Similarity ---\n",
        "print(\"Calculating pairwise embedding similarity...\")\n",
        "\n",
        "def get_embedding_similarity(row, embeddings_dict, default_emb):\n",
        "    \"\"\"Calculates cosine similarity between embeddings of paper pairs.\"\"\"\n",
        "    emb1 = embeddings_dict.get(row['paper'], default_emb).reshape(1, -1).astype(np.float32) # Use float32 for calculation\n",
        "    emb2 = embeddings_dict.get(row['referenced_paper'], default_emb).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    sim = cosine_similarity(emb1, emb2)[0, 0]\n",
        "    # Clip similarity to [0, 1] range just in case of floating point issues\n",
        "    return np.clip(sim, 0.0, 1.0)\n",
        "\n",
        "# Apply to train\n",
        "tqdm.pandas(desc=\"Calculating Train Similarity\")\n",
        "train_merged['qa-mpnet_similarity'] = train_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "# Apply to test\n",
        "tqdm.pandas(desc=\"Calculating Test Similarity\")\n",
        "test_merged['qa-mpnet_similarity'] = test_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "\n",
        "# Fill any NaNs (e.g., if similarity couldn't be calculated)\n",
        "train_merged['qa-mpnet_similarity'] = train_merged['qa-mpnet_similarity'].fillna(0.0)\n",
        "test_merged['qa-mpnet_similarity'] = test_merged['qa-mpnet_similarity'].fillna(0.0)\n",
        "\n",
        "print(\"Finished calculating embedding similarity.\")\n",
        "print(train_merged[['qa-mpnet_similarity']].describe())"
      ],
      "metadata": {
        "id": "UxFPqZF1wcYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: all-mpnet Similiarity(first 400 words)"
      ],
      "metadata": {
        "id": "lG2syZjrxHPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
        "EMBEDDING_FILE = f'paper_embeddings_{MODEL_NAME.split(\"/\")[-1]}_from_paper_df.pkl'\n",
        "# Load Pre-trained Model\n",
        "print(f\"Loading sentence-transformer model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.max_seq_length = MAX_LENGTH\n",
        "print(\"Model loaded.\")\n",
        "print(f\"Model will run on device: {model.device}\")\n",
        "\n",
        "\n",
        "# 5. Generate Embeddings (Batch Processing)\n",
        "print(f\"Starting embedding generation with batch size {BATCH_SIZE}...\")\n",
        "paper_embeddings = {}\n",
        "if texts:\n",
        "    embeddings_np = model.encode(\n",
        "        texts,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    for paper_id, embedding in zip(paper_ids, embeddings_np):\n",
        "         paper_embeddings[paper_id] = embedding.astype(np.float16) # float16 saves space\n",
        "    print(f\"Finished embedding generation.\")\n",
        "    if len(paper_embeddings) > 0:\n",
        "         print(f\"Sample embedding shape: {list(paper_embeddings.values())[0].shape}, dtype: {list(paper_embeddings.values())[0].dtype}\")\n",
        "else:\n",
        "     print(\"Warning: No non-empty texts found to generate embeddings.\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Cleaned up model from memory.\")\n",
        "\n",
        "\n",
        "# 6. Save Embeddings\n",
        "print(f\"Saving embeddings to {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'wb') as f_out:\n",
        "        pickle.dump(paper_embeddings, f_out)\n",
        "    print(\"Embeddings saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n"
      ],
      "metadata": {
        "id": "arVmAGZWxGBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/kaggle/input/gammafest-pmo/paper_embeddings_all-mpnet-base-v2_from_paper_df.pkl'\n",
        "print(f\"Loading embeddings from {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'rb') as f_in:\n",
        "        paper_embeddings = pickle.load(f_in)\n",
        "    print(f\"Loaded {len(paper_embeddings)} embeddings.\")\n",
        "    if not paper_embeddings:\n",
        "        raise ValueError(\"Embeddings dictionary is empty!\")\n",
        "    embedding_dim = list(paper_embeddings.values())[0].shape[0]\n",
        "    default_embedding = np.zeros(embedding_dim, dtype=np.float16) # Match dtype\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embedding file not found at {EMBEDDING_FILE}\")\n",
        "    print(\"Please run the embedding generation script first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calculate Pairwise Embedding Similarity ---\n",
        "print(\"Calculating pairwise embedding similarity...\")\n",
        "\n",
        "def get_embedding_similarity(row, embeddings_dict, default_emb):\n",
        "    \"\"\"Calculates cosine similarity between embeddings of paper pairs.\"\"\"\n",
        "    emb1 = embeddings_dict.get(row['paper'], default_emb).reshape(1, -1).astype(np.float32) # Use float32 for calculation\n",
        "    emb2 = embeddings_dict.get(row['referenced_paper'], default_emb).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    sim = cosine_similarity(emb1, emb2)[0, 0]\n",
        "    # Clip similarity to [0, 1] range just in case of floating point issues\n",
        "    return np.clip(sim, 0.0, 1.0)\n",
        "\n",
        "# Apply to train\n",
        "tqdm.pandas(desc=\"Calculating Train Similarity\")\n",
        "train_merged['all-mpnet_similarity'] = train_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "# Apply to test\n",
        "tqdm.pandas(desc=\"Calculating Test Similarity\")\n",
        "test_merged['all-mpnet_similarity'] = test_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "\n",
        "# Fill any NaNs (e.g., if similarity couldn't be calculated)\n",
        "train_merged['all-mpnet_similarity'] = train_merged['all-mpnet_similarity'].fillna(0.0)\n",
        "test_merged['all-mpnet_similarity'] = test_merged['all-mpnet_similarity'].fillna(0.0)\n",
        "\n",
        "print(\"Finished calculating embedding similarity.\")\n",
        "print(train_merged[['all-mpnet_similarity']].describe())"
      ],
      "metadata": {
        "id": "p2RxqyrwxZuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: SciNCL Similiarity(first 400 words)"
      ],
      "metadata": {
        "id": "tyHtDcb4x00s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'malteos/scincl'\n",
        "EMBEDDING_FILE = f'paper_embeddings_{MODEL_NAME.split(\"/\")[-1]}_from_paper_df.pkl'\n",
        "# Load Pre-trained Model\n",
        "print(f\"Loading sentence-transformer model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.max_seq_length = MAX_LENGTH\n",
        "print(\"Model loaded.\")\n",
        "print(f\"Model will run on device: {model.device}\")\n",
        "\n",
        "\n",
        "# 5. Generate Embeddings (Batch Processing)\n",
        "print(f\"Starting embedding generation with batch size {BATCH_SIZE}...\")\n",
        "paper_embeddings = {}\n",
        "if texts:\n",
        "    embeddings_np = model.encode(\n",
        "        texts,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    for paper_id, embedding in zip(paper_ids, embeddings_np):\n",
        "         paper_embeddings[paper_id] = embedding.astype(np.float16) # float16 saves space\n",
        "    print(f\"Finished embedding generation.\")\n",
        "    if len(paper_embeddings) > 0:\n",
        "         print(f\"Sample embedding shape: {list(paper_embeddings.values())[0].shape}, dtype: {list(paper_embeddings.values())[0].dtype}\")\n",
        "else:\n",
        "     print(\"Warning: No non-empty texts found to generate embeddings.\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Cleaned up model from memory.\")\n",
        "\n",
        "\n",
        "# 6. Save Embeddings\n",
        "print(f\"Saving embeddings to {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'wb') as f_out:\n",
        "        pickle.dump(paper_embeddings, f_out)\n",
        "    print(\"Embeddings saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n"
      ],
      "metadata": {
        "id": "snaMehiZx4Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/kaggle/input/gammafest-pmo/paper_embeddings_scincl_from_paper_df.pkl'\n",
        "\n",
        "print(f\"Loading embeddings from {EMBEDDING_FILE}...\")\n",
        "try:\n",
        "    with open(EMBEDDING_FILE, 'rb') as f_in:\n",
        "        paper_embeddings = pickle.load(f_in)\n",
        "    print(f\"Loaded {len(paper_embeddings)} embeddings.\")\n",
        "    if not paper_embeddings:\n",
        "        raise ValueError(\"Embeddings dictionary is empty!\")\n",
        "    embedding_dim = list(paper_embeddings.values())[0].shape[0]\n",
        "    default_embedding = np.zeros(embedding_dim, dtype=np.float16) # Match dtype\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embedding file not found at {EMBEDDING_FILE}\")\n",
        "    print(\"Please run the embedding generation script first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embeddings: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Calculate Pairwise Embedding Similarity ---\n",
        "print(\"Calculating pairwise embedding similarity...\")\n",
        "\n",
        "def get_embedding_similarity(row, embeddings_dict, default_emb):\n",
        "    \"\"\"Calculates cosine similarity between embeddings of paper pairs.\"\"\"\n",
        "    emb1 = embeddings_dict.get(row['paper'], default_emb).reshape(1, -1).astype(np.float32) # Use float32 for calculation\n",
        "    emb2 = embeddings_dict.get(row['referenced_paper'], default_emb).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    sim = cosine_similarity(emb1, emb2)[0, 0]\n",
        "    # Clip similarity to [0, 1] range just in case of floating point issues\n",
        "    return np.clip(sim, 0.0, 1.0)\n",
        "\n",
        "# Apply to train\n",
        "tqdm.pandas(desc=\"Calculating Train Similarity\")\n",
        "train_merged['scincl_similarity'] = train_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "# Apply to test\n",
        "tqdm.pandas(desc=\"Calculating Test Similarity\")\n",
        "test_merged['scincl_similarity'] = test_merged.progress_apply(\n",
        "    lambda row: get_embedding_similarity(row, paper_embeddings, default_embedding), axis=1\n",
        ")\n",
        "\n",
        "# Fill any NaNs (e.g., if similarity couldn't be calculated)\n",
        "train_merged['scincl_similarity'] = train_merged['scincl_similarity'].fillna(0.0)\n",
        "test_merged['scincl_similarity'] = test_merged['scincl_similarity'].fillna(0.0)\n",
        "\n",
        "print(\"Finished calculating embedding similarity.\")\n",
        "print(train_merged[['scincl_similarity']].describe())"
      ],
      "metadata": {
        "id": "aeVvveb0x5BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: TF-IDF Similiarity(first 400 words)"
      ],
      "metadata": {
        "id": "GVbSfuawYHhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_TEXT_DIR = f'{base_path}/paper_database_processed_cap_400'\n",
        "\n",
        "# Read all paper into one memory\n",
        "all_ids = pd.concat([\n",
        "    train_merged[['paper', 'referenced_paper']]\n",
        "    # df_test [['paper', 'referenced_paper']]\n",
        "]).stack().unique()\n",
        "\n",
        "texts = {}\n",
        "for pid in all_ids:\n",
        "    fp = os.path.join(tf_idf_TEXT_DIR, f\"{pid}.txt\")\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        texts[pid] = f.read()\n"
      ],
      "metadata": {
        "id": "dbscthsIEtKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit TF_IDF\n",
        "corpus     = [texts[pid] for pid in all_ids]\n",
        "vectorizer = TfidfVectorizer(max_features=50_000, ngram_range=(1,2))\n",
        "tfidf_mat  = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Map paper_id → row index in tfidf_mat\n",
        "id2idx = { pid: idx for idx, pid in enumerate(all_ids) }\n",
        "\n"
      ],
      "metadata": {
        "id": "-n953yEqDZ1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION TO COMPUTE TF–IDF COSINE SIM FOR A PAIRWISE DF"
      ],
      "metadata": {
        "id": "ZLAdzq69Exj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tfidf_sim(df):\n",
        "    sims = []\n",
        "    for a, b in tqdm(zip(df['paper'], df['referenced_paper']), total=len(df)):\n",
        "        ia = id2idx[a]\n",
        "        ib = id2idx[b]\n",
        "        sim = cosine_similarity(tfidf_mat[ia], tfidf_mat[ib])[0, 0]\n",
        "        sims.append(sim)\n",
        "    df = df.copy()\n",
        "    df['tfidf_sim'] = sims\n",
        "    return df"
      ],
      "metadata": {
        "id": "wbgkefi2Eu5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged = add_tfidf_sim(train_merged)\n",
        "test_merged  = add_tfidf_sim(test_merged)"
      ],
      "metadata": {
        "id": "APuCzFyGE0yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: bm25 score"
      ],
      "metadata": {
        "id": "8zuMBZdjYJ4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Tokenizer\n",
        "\n",
        "def tokenize(text):\n",
        "    # simple whitespace+alphanumeric tokenizer\n",
        "    return re.findall(r'\\w+', text.lower())\n"
      ],
      "metadata": {
        "id": "IJHcJ8-hFOMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = pd.concat([\n",
        "    train_df[['paper', 'referenced_paper']]\n",
        "    # df_test [['paper', 'referenced_paper']]\n",
        "]).stack().unique()\n",
        "\n",
        "texts = {\n",
        "    pid: open(os.path.join(tf_idf_TEXT_DIR, f\"{pid}.txt\"), encoding=\"utf-8\").read()\n",
        "    for pid in all_ids\n",
        "}"
      ],
      "metadata": {
        "id": "V3yxGERUFRW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 3) BUILD CORPUS STATS ────────────────────────────────────────────────────\n",
        "corpus_ids    = list(texts.keys())\n",
        "corpus_tokens = [tokenize(texts[pid]) for pid in corpus_ids]\n",
        "N_docs        = len(corpus_ids)\n",
        "avgdl         = sum(len(tokens) for tokens in corpus_tokens) / N_docs\n",
        "\n",
        "# document frequencies (for IDF)\n",
        "df_counts = Counter()\n",
        "for tokens in corpus_tokens:\n",
        "    df_counts.update(set(tokens))\n",
        "\n",
        "# inverse document frequencies\n",
        "idf = {\n",
        "    term: math.log((N_docs - df + 0.5) / (df + 0.5) + 1.0)\n",
        "    for term, df in df_counts.items()\n",
        "}\n",
        "\n",
        "# per‐doc term‐frequencies & doc lengths\n",
        "tf_map = {}\n",
        "dl_map = {}\n",
        "for pid, tokens in zip(corpus_ids, corpus_tokens):\n",
        "    tf_map[pid] = Counter(tokens)\n",
        "    dl_map[pid] = len(tokens)\n",
        "\n",
        "# BM25 hyper‑params\n",
        "k1, b = 1.5, 0.75\n"
      ],
      "metadata": {
        "id": "5ZGU7uUxFSVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_score(query_tokens, doc_id):\n",
        "    \"\"\"Compute BM25 of query_tokens against single document doc_id.\"\"\"\n",
        "    tf_B = tf_map[doc_id]\n",
        "    dl_B = dl_map[doc_id]\n",
        "    score = 0.0\n",
        "    for t in query_tokens:\n",
        "        if t not in idf:\n",
        "            continue\n",
        "        tf = tf_B.get(t, 0)\n",
        "        num = tf * (k1 + 1)\n",
        "        den = tf + k1 * (1 - b + b * dl_B / avgdl)\n",
        "        score += idf[t] * (num / den)\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "-6xM6TKQFVgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_bm25_score(df):\n",
        "    sims = []\n",
        "    for a_pid, b_pid in tqdm(zip(df['paper'], df['referenced_paper']), total=len(df)):\n",
        "        q_tokens = tokenize(texts[a_pid])\n",
        "        sims.append(bm25_score(q_tokens, b_pid))\n",
        "    df = df.copy()\n",
        "    df['bm25_score'] = sims\n",
        "    return df"
      ],
      "metadata": {
        "id": "I2B_dQKyFWY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged = add_bm25_score(train_merged)\n",
        "test_merged = add_bm25_score(test_merged)"
      ],
      "metadata": {
        "id": "ZX0DbrZtFXUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !!! Fitur: Author mentioned"
      ],
      "metadata": {
        "id": "q4MaMG-LYUks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_metadata(df: pd.DataFrame, metadata: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # Merge primary paper metadata (p1)\n",
        "    merged = pd.merge(\n",
        "        df,\n",
        "        metadata,\n",
        "        left_on='paper',\n",
        "        right_on='paper_id',\n",
        "        how='left',\n",
        "        suffixes=('', '_p1_meta')\n",
        "    )\n",
        "\n",
        "    # Merge referenced paper metadata (p2)\n",
        "    merged = pd.merge(\n",
        "        merged,\n",
        "        metadata,\n",
        "        left_on='referenced_paper',\n",
        "        right_on='paper_id',\n",
        "        how='left',\n",
        "        suffixes=('_p1', '_p2')\n",
        "    )\n",
        "\n",
        "    # Drop any duplicate 'paper_id' columns from merges\n",
        "    for duplicate_col in ['paper_id_p1', 'paper_id_p2']:\n",
        "        if duplicate_col in merged.columns:\n",
        "            merged.drop(columns=[duplicate_col], inplace=True)\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "# Perform merges on train and test\n",
        "print(\"Merging metadata onto train_df...\")\n",
        "df_train_author = merge_metadata(train_merged, metadata_df)\n",
        "\n",
        "print(\"Merging metadata onto test_df...\")\n",
        "df_test_author = merge_metadata(test_merged, metadata_df)\n"
      ],
      "metadata": {
        "id": "DW4xKOcAFhk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_author = df_train_author[['paper', 'referenced_paper', 'authors_p2']]\n",
        "df_test_author = df_test_author[['paper', 'referenced_paper', 'authors_p2']]"
      ],
      "metadata": {
        "id": "sSpRGa9oIvSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_author_lastname(authors: str) -> str:\n",
        "    if not isinstance(authors, str) or not authors.strip():\n",
        "        return ''\n",
        "\n",
        "    # Split by semicolon, take first author, then split into name parts\n",
        "    first_author = authors.split(';')[0].strip()\n",
        "    name_parts = first_author.split()\n",
        "\n",
        "    # Return the last token (surname) if available\n",
        "    return name_parts[-1] if name_parts else ''\n",
        "\n",
        "# Example usage on train_merged and test_merged DataFrames\n",
        "\n",
        "df_train_author['authors_p2'] = df_train_author['authors_p2'].apply(get_first_author_lastname)\n",
        "# df_train = df_train.drop(columns=['authors_p2'])\n",
        "print(\"Extracted first author last names for train_merged.\")\n",
        "\n",
        "df_test_author['authors_p2'] = df_test_author['authors_p2'].apply(get_first_author_lastname)\n",
        "# df_test = df_test.drop(columns=['authors_p2'])\n",
        "print(\"Extracted first author last names for test_merged.\")\n"
      ],
      "metadata": {
        "id": "g2yAKkp-Izab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flag_author_mentions(train_df: pd.DataFrame,\n",
        "                         test_df: pd.DataFrame,\n",
        "                         text_folder: str,\n",
        "                         file_ext: str = \".txt\",\n",
        "                         use_apply_progress: bool = False):\n",
        "    \"\"\"\n",
        "    For each (paper, referenced_paper, authors_p2) row, check whether the\n",
        "    last name in authors_p2 appears as a whole word in the text of 'paper'.\n",
        "\n",
        "    Args:\n",
        "      train_df          : DataFrame with ['paper', 'referenced_paper', 'authors_p2']\n",
        "      test_df           : same structure as train_df\n",
        "      text_folder       : path to folder containing <paper_id>.txt files\n",
        "      file_ext          : extension of those files (default \".txt\")\n",
        "      use_apply_progress: if True, show a tqdm bar on the pandas apply()\n",
        "\n",
        "    Returns:\n",
        "      (train_out, test_out) : two DataFrames mirroring train_df/test_df\n",
        "                              with an extra column 'author_mentioned'.\n",
        "    \"\"\"\n",
        "    # Optional: show progress on the final apply\n",
        "    if use_apply_progress:\n",
        "        from tqdm import tqdm as _tqdm\n",
        "        tqdm.pandas()  # enable .progress_apply()\n",
        "\n",
        "    # 1) Tag origin\n",
        "    train = train_df.copy(); train[\"_split\"] = \"train\"\n",
        "    test  = test_df.copy();  test[\"_split\"]  = \"test\"\n",
        "    full = pd.concat([train, test], ignore_index=True)\n",
        "\n",
        "    # 2) Gather per-paper list of last names to check\n",
        "    to_check = (\n",
        "        full\n",
        "        .dropna(subset=[\"authors_p2\"])\n",
        "        .groupby(\"paper\")[\"authors_p2\"]\n",
        "        .unique()\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    # 3) Read each paper once, with tqdm\n",
        "    mentions = {}\n",
        "    for pid, last_names in tqdm(to_check.items(),\n",
        "                                 total=len(to_check),\n",
        "                                 desc=\"Scanning papers\"):\n",
        "        path = os.path.join(text_folder, f\"{pid}{file_ext}\")\n",
        "        try:\n",
        "            text = open(path, encoding=\"utf8\").read().lower()\n",
        "        except FileNotFoundError:\n",
        "            mentions[pid] = set()\n",
        "            continue\n",
        "\n",
        "        found = {\n",
        "            name for name in last_names\n",
        "            if re.search(rf\"\\b{re.escape(name.lower())}\\b\", text)\n",
        "        }\n",
        "        mentions[pid] = found\n",
        "\n",
        "    # 4) Flag each row (with or without a progress bar)\n",
        "    apply_fn = full.progress_apply if use_apply_progress else full.apply\n",
        "    full[\"author_mentioned\"] = apply_fn(\n",
        "        lambda row: row[\"authors_p2\"] in mentions.get(row[\"paper\"], set()),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 5) Split back out and drop helper column\n",
        "    train_out = full[full[\"_split\"] == \"train\"].drop(columns=[\"_split\"])\n",
        "    test_out  = full[full[\"_split\"] == \"test\"].drop(columns=[\"_split\"])\n",
        "\n",
        "    return train_out, test_out"
      ],
      "metadata": {
        "id": "bnAs8DXOJEWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flagged, test_flagged = flag_author_mentions(\n",
        "    train_df=df_train_author,\n",
        "    test_df=df_test_author,\n",
        "    text_folder=base_path + \"/data/2_interim/paper_database_processed_cap_10000\",\n",
        "    use_apply_progress=True  # if you want a tqdm bar on the apply\n",
        ")"
      ],
      "metadata": {
        "id": "8UpKcgE5JFd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged = train_merged.merge(train_flagged[['paper', 'referenced_paper', 'author_mentioned']], on=['paper', 'referenced_paper'], how='left')\n",
        "test_merged = test_merged.merge(test_flagged[['paper', 'referenced_paper', 'author_mentioned']], on=['paper', 'referenced_paper'], how='left')"
      ],
      "metadata": {
        "id": "kMGXkx1cJJ3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "zGf9D7PI_ngs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save Checkpoint\n",
        "# train_merged.to_parquet(f'{base_path}/train_merged_final.parquet')\n",
        "# test_merged.to_parquet(f'{base_path}/test_merged_final.parquet')"
      ],
      "metadata": {
        "id": "fn83KgDCXVzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil Feature Engineering akan diload menggunakan kode dibawah, sekali lagi, karena keterbatasan resource"
      ],
      "metadata": {
        "id": "XBuE75MT2CDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load\n",
        "train_merged = pd.read_parquet(f'{base_path}/train_merged_final.parquet')\n",
        "test_merged = pd.read_parquet(f'{base_path}/test_merged_final.parquet')\n",
        "paper_df = pd.read_parquet(f'{base_path}/paper_df.parquet').set_index('paper_id')"
      ],
      "metadata": {
        "id": "tHivXuh96NJD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN IF YOU WANT TO USE THESE AUTHORS STATS FEATURES\n",
        "\n",
        "aggregation_methods_used = ['mean', 'max', 'sum']\n",
        "base_stats_used = ['total_citations', 'mean_citations', 'max_citations', 'paper_count']\n",
        "author_stat_cols_bases = []\n",
        "for stat in base_stats_used:\n",
        "     for method in aggregation_methods_used:\n",
        "         author_stat_cols_bases.append(f\"{method}_{stat}\")\n",
        "\n",
        "author_stat_features_p1 = [f\"{col}_p1\" for col in author_stat_cols_bases]\n",
        "author_stat_features_p2 = [f\"{col}_p2\" for col in author_stat_cols_bases]\n",
        "author_stat_features_comp = []\n",
        "for col_base in author_stat_cols_bases:\n",
        "     author_stat_features_comp.append(f\"diff_{col_base}\")\n",
        "     author_stat_features_comp.append(f\"ratio_{col_base}\")"
      ],
      "metadata": {
        "id": "bHThLGXO3fv1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "included_cols = [\n",
        "    # embedding\n",
        "    'embedding_similarity', #first 400 words\n",
        "    'embedding_similarity2', #first 400 words\n",
        "    'specterv2_similarity', #chunking method\n",
        "    'scibert_similarity', #chunking method\n",
        "    'TITLE-TinyBERT-L6-score',\n",
        "    'tfidf_sim',\n",
        "    'bm25_score',\n",
        "       'embedding_similarity_scincl', 'embedding_similarity_all-mpnet',\n",
        "       'embedding_similarity_qa-mpnet',\n",
        "    # re\n",
        "    'author_mentioned',\n",
        "\n",
        "    # traditional semantic similarity\n",
        "    'title_similarity',\n",
        "    'concepts_jaccard',\n",
        "    'authors_jaccard',\n",
        "    'matched_words_count',\n",
        "\n",
        "    # publication diff\n",
        "    'year_diff',\n",
        "    'days_diff',\n",
        "    'months_diff',\n",
        "\n",
        "    # fitur statistika authors & papers\n",
        "    'cited_by_count_p2',\n",
        "    *author_stat_features_p1,\n",
        "    *author_stat_features_p2,\n",
        "    *author_stat_features_comp\n",
        "]\n",
        "\n",
        "# Convert boolean features to integers in train_merged and test_merged\n",
        "for col in train_merged.columns:\n",
        "    if train_merged[col].dtype == 'bool':\n",
        "        train_merged[col] = train_merged[col].astype(int)\n",
        "for col in test_merged.columns:\n",
        "    if test_merged[col].dtype == 'bool':\n",
        "        test_merged[col] = test_merged[col].astype(int)\n",
        "\n",
        "features = [col for col in included_cols] # USING INCLUDED COLS\n",
        "target = 'is_referenced'\n",
        "\n",
        "# Pastikan tidak ada duplikasi fitur\n",
        "features = sorted(list(set(features)))\n",
        "\n",
        "print(f\"Total features selected: {len(features)}\")\n",
        "print(f\"Selected features: {features}\")\n",
        "\n",
        "# Pisahkan X dan y untuk training\n",
        "# Pastikan hanya memilih kolom fitur yang benar-benar ada di DataFrame\n",
        "existing_features = [f for f in features if f in train_merged.columns]\n",
        "missing_features = [f for f in features if f not in train_merged.columns]\n",
        "if missing_features:\n",
        "     print(\"\\nWarning: The following features were selected but not found in train_merged:\")\n",
        "     print(missing_features)\n",
        "     print(\"Please check feature creation steps in Cell 6.\")\n",
        "\n",
        "X = train_merged[existing_features].copy()\n",
        "y = train_merged[target].copy()\n",
        "\n",
        "# Siapkan X_test\n",
        "existing_features_test = [f for f in features if f in test_merged.columns] # Cek juga di test\n",
        "X_test = test_merged[existing_features_test].copy()\n",
        "\n",
        "# Cek kesamaan kolom antara X dan X_test\n",
        "if set(X.columns) != set(X_test.columns):\n",
        "    print(\"\\nWarning: Feature columns mismatch between processed train and test sets!\")\n",
        "    print(\"Train columns:\", sorted(list(X.columns)))\n",
        "    print(\"Test columns:\", sorted(list(X_test.columns)))\n",
        "\n",
        "\n",
        "print(\"\\nChecking final NaNs in selected features:\")\n",
        "print(\"Train X NaNs:\\n\", X.isnull().sum().sum()) # Tampilkan total NaN saja\n",
        "print(\"\\nTest X_test NaNs:\\n\", X_test.isnull().sum().sum())\n",
        "\n",
        "\n",
        "print(\"\\nFinal shapes:\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nURmd3wQJMOe",
        "outputId": "4016640b-f267-4949-c32c-517261da727a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features selected: 67\n",
            "Selected features: ['TITLE-TinyBERT-L6-score', 'author_mentioned', 'authors_jaccard', 'bm25_score', 'cited_by_count_p2', 'concepts_jaccard', 'days_diff', 'diff_max_max_citations', 'diff_max_mean_citations', 'diff_max_paper_count', 'diff_max_total_citations', 'diff_mean_max_citations', 'diff_mean_mean_citations', 'diff_mean_paper_count', 'diff_mean_total_citations', 'diff_sum_max_citations', 'diff_sum_mean_citations', 'diff_sum_paper_count', 'diff_sum_total_citations', 'embedding_similarity', 'embedding_similarity2', 'embedding_similarity_all-mpnet', 'embedding_similarity_qa-mpnet', 'embedding_similarity_scincl', 'matched_words_count', 'max_max_citations_p1', 'max_max_citations_p2', 'max_mean_citations_p1', 'max_mean_citations_p2', 'max_paper_count_p1', 'max_paper_count_p2', 'max_total_citations_p1', 'max_total_citations_p2', 'mean_max_citations_p1', 'mean_max_citations_p2', 'mean_mean_citations_p1', 'mean_mean_citations_p2', 'mean_paper_count_p1', 'mean_paper_count_p2', 'mean_total_citations_p1', 'mean_total_citations_p2', 'months_diff', 'ratio_max_max_citations', 'ratio_max_mean_citations', 'ratio_max_paper_count', 'ratio_max_total_citations', 'ratio_mean_max_citations', 'ratio_mean_mean_citations', 'ratio_mean_paper_count', 'ratio_mean_total_citations', 'ratio_sum_max_citations', 'ratio_sum_mean_citations', 'ratio_sum_paper_count', 'ratio_sum_total_citations', 'scibert_similarity', 'specterv2_similarity', 'sum_max_citations_p1', 'sum_max_citations_p2', 'sum_mean_citations_p1', 'sum_mean_citations_p2', 'sum_paper_count_p1', 'sum_paper_count_p2', 'sum_total_citations_p1', 'sum_total_citations_p2', 'tfidf_sim', 'title_similarity', 'year_diff']\n",
            "\n",
            "Checking final NaNs in selected features:\n",
            "Train X NaNs:\n",
            " 0\n",
            "\n",
            "Test X_test NaNs:\n",
            " 0\n",
            "\n",
            "Final shapes:\n",
            "X shape: (410691, 67)\n",
            "y shape: (410691,)\n",
            "X_test shape: (336021, 67)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQjyfdSNmqFU",
        "outputId": "88d11e74-338d-456f-8d62-1e8c57e71843"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 410691 entries, 0 to 410690\n",
            "Data columns (total 67 columns):\n",
            " #   Column                          Non-Null Count   Dtype  \n",
            "---  ------                          --------------   -----  \n",
            " 0   TITLE-TinyBERT-L6-score         410691 non-null  float64\n",
            " 1   author_mentioned                410691 non-null  int64  \n",
            " 2   authors_jaccard                 410691 non-null  float64\n",
            " 3   bm25_score                      410691 non-null  float64\n",
            " 4   cited_by_count_p2               410691 non-null  int64  \n",
            " 5   concepts_jaccard                410691 non-null  float64\n",
            " 6   days_diff                       410691 non-null  int64  \n",
            " 7   diff_max_max_citations          410691 non-null  float64\n",
            " 8   diff_max_mean_citations         410691 non-null  float64\n",
            " 9   diff_max_paper_count            410691 non-null  float64\n",
            " 10  diff_max_total_citations        410691 non-null  float64\n",
            " 11  diff_mean_max_citations         410691 non-null  float64\n",
            " 12  diff_mean_mean_citations        410691 non-null  float64\n",
            " 13  diff_mean_paper_count           410691 non-null  float64\n",
            " 14  diff_mean_total_citations       410691 non-null  float64\n",
            " 15  diff_sum_max_citations          410691 non-null  float64\n",
            " 16  diff_sum_mean_citations         410691 non-null  float64\n",
            " 17  diff_sum_paper_count            410691 non-null  float64\n",
            " 18  diff_sum_total_citations        410691 non-null  float64\n",
            " 19  embedding_similarity            410691 non-null  float64\n",
            " 20  embedding_similarity2           410691 non-null  float64\n",
            " 21  embedding_similarity_all-mpnet  410691 non-null  float64\n",
            " 22  embedding_similarity_qa-mpnet   410691 non-null  float64\n",
            " 23  embedding_similarity_scincl     410691 non-null  float64\n",
            " 24  matched_words_count             410691 non-null  int64  \n",
            " 25  max_max_citations_p1            410691 non-null  float64\n",
            " 26  max_max_citations_p2            410691 non-null  float64\n",
            " 27  max_mean_citations_p1           410691 non-null  float64\n",
            " 28  max_mean_citations_p2           410691 non-null  float64\n",
            " 29  max_paper_count_p1              410691 non-null  float64\n",
            " 30  max_paper_count_p2              410691 non-null  float64\n",
            " 31  max_total_citations_p1          410691 non-null  float64\n",
            " 32  max_total_citations_p2          410691 non-null  float64\n",
            " 33  mean_max_citations_p1           410691 non-null  float64\n",
            " 34  mean_max_citations_p2           410691 non-null  float64\n",
            " 35  mean_mean_citations_p1          410691 non-null  float64\n",
            " 36  mean_mean_citations_p2          410691 non-null  float64\n",
            " 37  mean_paper_count_p1             410691 non-null  float64\n",
            " 38  mean_paper_count_p2             410691 non-null  float64\n",
            " 39  mean_total_citations_p1         410691 non-null  float64\n",
            " 40  mean_total_citations_p2         410691 non-null  float64\n",
            " 41  months_diff                     410691 non-null  int64  \n",
            " 42  ratio_max_max_citations         410691 non-null  float64\n",
            " 43  ratio_max_mean_citations        410691 non-null  float64\n",
            " 44  ratio_max_paper_count           410691 non-null  float64\n",
            " 45  ratio_max_total_citations       410691 non-null  float64\n",
            " 46  ratio_mean_max_citations        410691 non-null  float64\n",
            " 47  ratio_mean_mean_citations       410691 non-null  float64\n",
            " 48  ratio_mean_paper_count          410691 non-null  float64\n",
            " 49  ratio_mean_total_citations      410691 non-null  float64\n",
            " 50  ratio_sum_max_citations         410691 non-null  float64\n",
            " 51  ratio_sum_mean_citations        410691 non-null  float64\n",
            " 52  ratio_sum_paper_count           410691 non-null  float64\n",
            " 53  ratio_sum_total_citations       410691 non-null  float64\n",
            " 54  scibert_similarity              410691 non-null  float32\n",
            " 55  specterv2_similarity            410691 non-null  float32\n",
            " 56  sum_max_citations_p1            410691 non-null  float64\n",
            " 57  sum_max_citations_p2            410691 non-null  float64\n",
            " 58  sum_mean_citations_p1           410691 non-null  float64\n",
            " 59  sum_mean_citations_p2           410691 non-null  float64\n",
            " 60  sum_paper_count_p1              410691 non-null  float64\n",
            " 61  sum_paper_count_p2              410691 non-null  float64\n",
            " 62  sum_total_citations_p1          410691 non-null  float64\n",
            " 63  sum_total_citations_p2          410691 non-null  float64\n",
            " 64  tfidf_sim                       410691 non-null  float64\n",
            " 65  title_similarity                410691 non-null  float64\n",
            " 66  year_diff                       410691 non-null  int64  \n",
            "dtypes: float32(2), float64(59), int64(6)\n",
            "memory usage: 206.8 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Minification"
      ],
      "metadata": {
        "id": "-Pdi9XHzC2Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "# for df in [X_copy, X_test_copy]:\n",
        "for df in [X, X_test]:\n",
        "    original = df.copy()\n",
        "    df = reduce_mem_usage(df)\n",
        "\n",
        "    for col in list(df):\n",
        "        if df[col].dtype!='O':\n",
        "            if (df[col]-original[col]).sum()!=0:\n",
        "                df[col] = original[col]\n",
        "                print('Bad transformation', col)\n",
        "X.info()"
      ],
      "metadata": {
        "id": "9s4TJNGJC5_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46906676-fc1f-46b4-c7fc-dbe9c29307b4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 81.07 Mb (60.8% reduction)\n",
            "Bad transformation TITLE-TinyBERT-L6-score\n",
            "Bad transformation authors_jaccard\n",
            "Bad transformation bm25_score\n",
            "Bad transformation concepts_jaccard\n",
            "Bad transformation diff_max_mean_citations\n",
            "Bad transformation diff_mean_max_citations\n",
            "Bad transformation diff_mean_mean_citations\n",
            "Bad transformation diff_mean_paper_count\n",
            "Bad transformation diff_mean_total_citations\n",
            "Bad transformation diff_sum_mean_citations\n",
            "Bad transformation embedding_similarity\n",
            "Bad transformation embedding_similarity2\n",
            "Bad transformation embedding_similarity_all-mpnet\n",
            "Bad transformation embedding_similarity_qa-mpnet\n",
            "Bad transformation embedding_similarity_scincl\n",
            "Bad transformation max_max_citations_p1\n",
            "Bad transformation max_mean_citations_p1\n",
            "Bad transformation max_mean_citations_p2\n",
            "Bad transformation mean_max_citations_p1\n",
            "Bad transformation mean_max_citations_p2\n",
            "Bad transformation mean_mean_citations_p1\n",
            "Bad transformation mean_mean_citations_p2\n",
            "Bad transformation mean_paper_count_p1\n",
            "Bad transformation mean_paper_count_p2\n",
            "Bad transformation mean_total_citations_p1\n",
            "Bad transformation mean_total_citations_p2\n",
            "Bad transformation ratio_max_max_citations\n",
            "Bad transformation ratio_max_mean_citations\n",
            "Bad transformation ratio_max_paper_count\n",
            "Bad transformation ratio_max_total_citations\n",
            "Bad transformation ratio_mean_max_citations\n",
            "Bad transformation ratio_mean_mean_citations\n",
            "Bad transformation ratio_mean_paper_count\n",
            "Bad transformation ratio_mean_total_citations\n",
            "Bad transformation ratio_sum_max_citations\n",
            "Bad transformation ratio_sum_mean_citations\n",
            "Bad transformation ratio_sum_paper_count\n",
            "Bad transformation ratio_sum_total_citations\n",
            "Bad transformation scibert_similarity\n",
            "Bad transformation specterv2_similarity\n",
            "Bad transformation sum_mean_citations_p1\n",
            "Bad transformation sum_mean_citations_p2\n",
            "Bad transformation tfidf_sim\n",
            "Bad transformation title_similarity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
            "<ipython-input-32-29cf6181f74e>:19: RuntimeWarning: overflow encountered in cast\n",
            "  if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 66.33 Mb (60.8% reduction)\n",
            "Bad transformation TITLE-TinyBERT-L6-score\n",
            "Bad transformation authors_jaccard\n",
            "Bad transformation bm25_score\n",
            "Bad transformation concepts_jaccard\n",
            "Bad transformation diff_max_mean_citations\n",
            "Bad transformation diff_mean_max_citations\n",
            "Bad transformation diff_mean_mean_citations\n",
            "Bad transformation diff_mean_paper_count\n",
            "Bad transformation diff_mean_total_citations\n",
            "Bad transformation diff_sum_mean_citations\n",
            "Bad transformation embedding_similarity\n",
            "Bad transformation embedding_similarity2\n",
            "Bad transformation embedding_similarity_all-mpnet\n",
            "Bad transformation embedding_similarity_qa-mpnet\n",
            "Bad transformation embedding_similarity_scincl\n",
            "Bad transformation max_max_citations_p1\n",
            "Bad transformation max_mean_citations_p1\n",
            "Bad transformation max_mean_citations_p2\n",
            "Bad transformation mean_max_citations_p1\n",
            "Bad transformation mean_max_citations_p2\n",
            "Bad transformation mean_mean_citations_p1\n",
            "Bad transformation mean_mean_citations_p2\n",
            "Bad transformation mean_paper_count_p1\n",
            "Bad transformation mean_paper_count_p2\n",
            "Bad transformation mean_total_citations_p1\n",
            "Bad transformation mean_total_citations_p2\n",
            "Bad transformation ratio_max_max_citations\n",
            "Bad transformation ratio_max_mean_citations\n",
            "Bad transformation ratio_max_paper_count\n",
            "Bad transformation ratio_max_total_citations\n",
            "Bad transformation ratio_mean_max_citations\n",
            "Bad transformation ratio_mean_mean_citations\n",
            "Bad transformation ratio_mean_paper_count\n",
            "Bad transformation ratio_mean_total_citations\n",
            "Bad transformation ratio_sum_max_citations\n",
            "Bad transformation ratio_sum_mean_citations\n",
            "Bad transformation ratio_sum_paper_count\n",
            "Bad transformation ratio_sum_total_citations\n",
            "Bad transformation scibert_similarity\n",
            "Bad transformation specterv2_similarity\n",
            "Bad transformation sum_mean_citations_p1\n",
            "Bad transformation sum_mean_citations_p2\n",
            "Bad transformation tfidf_sim\n",
            "Bad transformation title_similarity\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 410691 entries, 0 to 410690\n",
            "Data columns (total 67 columns):\n",
            " #   Column                          Non-Null Count   Dtype  \n",
            "---  ------                          --------------   -----  \n",
            " 0   TITLE-TinyBERT-L6-score         410691 non-null  float64\n",
            " 1   author_mentioned                410691 non-null  int8   \n",
            " 2   authors_jaccard                 410691 non-null  float64\n",
            " 3   bm25_score                      410691 non-null  float64\n",
            " 4   cited_by_count_p2               410691 non-null  int32  \n",
            " 5   concepts_jaccard                410691 non-null  float64\n",
            " 6   days_diff                       410691 non-null  int32  \n",
            " 7   diff_max_max_citations          410691 non-null  float32\n",
            " 8   diff_max_mean_citations         410691 non-null  float64\n",
            " 9   diff_max_paper_count            410691 non-null  float16\n",
            " 10  diff_max_total_citations        410691 non-null  float32\n",
            " 11  diff_mean_max_citations         410691 non-null  float64\n",
            " 12  diff_mean_mean_citations        410691 non-null  float64\n",
            " 13  diff_mean_paper_count           410691 non-null  float64\n",
            " 14  diff_mean_total_citations       410691 non-null  float64\n",
            " 15  diff_sum_max_citations          410691 non-null  float32\n",
            " 16  diff_sum_mean_citations         410691 non-null  float64\n",
            " 17  diff_sum_paper_count            410691 non-null  float16\n",
            " 18  diff_sum_total_citations        410691 non-null  float32\n",
            " 19  embedding_similarity            410691 non-null  float64\n",
            " 20  embedding_similarity2           410691 non-null  float64\n",
            " 21  embedding_similarity_all-mpnet  410691 non-null  float64\n",
            " 22  embedding_similarity_qa-mpnet   410691 non-null  float64\n",
            " 23  embedding_similarity_scincl     410691 non-null  float64\n",
            " 24  matched_words_count             410691 non-null  int16  \n",
            " 25  max_max_citations_p1            410691 non-null  float64\n",
            " 26  max_max_citations_p2            410691 non-null  float32\n",
            " 27  max_mean_citations_p1           410691 non-null  float64\n",
            " 28  max_mean_citations_p2           410691 non-null  float64\n",
            " 29  max_paper_count_p1              410691 non-null  float16\n",
            " 30  max_paper_count_p2              410691 non-null  float16\n",
            " 31  max_total_citations_p1          410691 non-null  float32\n",
            " 32  max_total_citations_p2          410691 non-null  float32\n",
            " 33  mean_max_citations_p1           410691 non-null  float64\n",
            " 34  mean_max_citations_p2           410691 non-null  float64\n",
            " 35  mean_mean_citations_p1          410691 non-null  float64\n",
            " 36  mean_mean_citations_p2          410691 non-null  float64\n",
            " 37  mean_paper_count_p1             410691 non-null  float64\n",
            " 38  mean_paper_count_p2             410691 non-null  float64\n",
            " 39  mean_total_citations_p1         410691 non-null  float64\n",
            " 40  mean_total_citations_p2         410691 non-null  float64\n",
            " 41  months_diff                     410691 non-null  int16  \n",
            " 42  ratio_max_max_citations         410691 non-null  float64\n",
            " 43  ratio_max_mean_citations        410691 non-null  float64\n",
            " 44  ratio_max_paper_count           410691 non-null  float64\n",
            " 45  ratio_max_total_citations       410691 non-null  float64\n",
            " 46  ratio_mean_max_citations        410691 non-null  float64\n",
            " 47  ratio_mean_mean_citations       410691 non-null  float64\n",
            " 48  ratio_mean_paper_count          410691 non-null  float64\n",
            " 49  ratio_mean_total_citations      410691 non-null  float64\n",
            " 50  ratio_sum_max_citations         410691 non-null  float64\n",
            " 51  ratio_sum_mean_citations        410691 non-null  float64\n",
            " 52  ratio_sum_paper_count           410691 non-null  float64\n",
            " 53  ratio_sum_total_citations       410691 non-null  float64\n",
            " 54  scibert_similarity              410691 non-null  float32\n",
            " 55  specterv2_similarity            410691 non-null  float32\n",
            " 56  sum_max_citations_p1            410691 non-null  float32\n",
            " 57  sum_max_citations_p2            410691 non-null  float32\n",
            " 58  sum_mean_citations_p1           410691 non-null  float64\n",
            " 59  sum_mean_citations_p2           410691 non-null  float64\n",
            " 60  sum_paper_count_p1              410691 non-null  float16\n",
            " 61  sum_paper_count_p2              410691 non-null  float16\n",
            " 62  sum_total_citations_p1          410691 non-null  float32\n",
            " 63  sum_total_citations_p2          410691 non-null  float32\n",
            " 64  tfidf_sim                       410691 non-null  float64\n",
            " 65  title_similarity                410691 non-null  float64\n",
            " 66  year_diff                       410691 non-null  int16  \n",
            "dtypes: float16(6), float32(13), float64(42), int16(3), int32(2), int8(1)\n",
            "memory usage: 162.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation\n",
        "\n"
      ],
      "metadata": {
        "id": "Bf4CfbxuAdJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LGBM"
      ],
      "metadata": {
        "id": "wZi-wYNkfTFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "#  LGBM Cross-Validation Configuration\n",
        "lgbm_n_splits = 5  # Number of folds for cross-validation\n",
        "lgbm_random_seed = 42\n",
        "\n",
        "# Calculate scale_pos_weight for imbalance (LightGBM specific)\n",
        "lgbm_neg_count = y.value_counts()[0]\n",
        "lgbm_pos_count = y.value_counts()[1]\n",
        "lgbm_scale_pos_weight_value = lgbm_neg_count / lgbm_pos_count\n",
        "print(f\"Calculated scale_pos_weight for LightGBM: {lgbm_scale_pos_weight_value:.2f}\")\n",
        "\n",
        "# Setup Stratified K-Fold for LightGBM\n",
        "lgbm_skf = StratifiedKFold(n_splits=lgbm_n_splits, shuffle=True, random_state=lgbm_random_seed)\n",
        "\n",
        "# Arrays to store LightGBM OOF (Out-of-Fold) predictions and MCC scores\n",
        "lgbm_oof_preds_proba = np.zeros(len(X))  # Store OOF probabilities\n",
        "lgbm_oof_mcc_scores = []  # Store MCC scores per fold\n",
        "\n",
        "#  LightGBM Cross-Validation Loop\n",
        "print(f\"\\nStarting LightGBM {lgbm_n_splits}-Fold Cross-Validation (Evaluating with MCC)...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(lgbm_skf.split(X, y)):\n",
        "    print(f\" Fold {fold+1}/{lgbm_n_splits} \")\n",
        "    lgbm_X_train, lgbm_y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    lgbm_X_val, lgbm_y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Initialize LightGBM Model\n",
        "    lgbm_model = lgb.LGBMClassifier(\n",
        "        objective='binary',\n",
        "        metric='auc',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=31,\n",
        "        max_depth=-1,\n",
        "        scale_pos_weight=lgbm_scale_pos_weight_value,\n",
        "        random_state=lgbm_random_seed + fold,\n",
        "        n_jobs=-1,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        verbose=-1,\n",
        "    )\n",
        "\n",
        "    # Train LightGBM Model\n",
        "    lgbm_model.fit(lgbm_X_train, lgbm_y_train,\n",
        "                   eval_set=[(lgbm_X_val, lgbm_y_val)],\n",
        "                   eval_metric=['auc', 'average_precision'],\n",
        "                   callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "    # Prediction on Validation Set (LightGBM)\n",
        "    lgbm_val_preds_proba_raw = lgbm_model.predict_proba(lgbm_X_val)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    # Store LightGBM OOF probabilities\n",
        "    lgbm_oof_preds_proba[val_idx] = lgbm_val_preds_proba_raw\n",
        "\n",
        "    #  Calculate MCC for LightGBM\n",
        "    lgbm_temp_threshold = 0.93428094\n",
        "    lgbm_val_preds_binary = (lgbm_val_preds_proba_raw >= lgbm_temp_threshold).astype(int)\n",
        "    lgbm_fold_mcc = matthews_corrcoef(lgbm_y_val, lgbm_val_preds_binary)\n",
        "    lgbm_oof_mcc_scores.append(lgbm_fold_mcc)\n",
        "    print(f\"Fold {fold+1} LightGBM MCC (thresh={lgbm_temp_threshold}): {lgbm_fold_mcc:.5f}\")\n",
        "\n",
        "    del lgbm_X_train, lgbm_y_train, lgbm_X_val, lgbm_y_val, lgbm_model, lgbm_val_preds_proba_raw, lgbm_val_preds_binary\n",
        "    gc.collect()\n",
        "\n",
        "#  Skor CV Rata-rata\n",
        "lgbm_mean_mcc = np.mean(lgbm_oof_mcc_scores)\n",
        "print(f\"\\n Cross-Validation Summary \")\n",
        "print(f\"Mean MCC (using threshold {lgbm_temp_threshold} in each fold): {lgbm_mean_mcc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG9QJouZAeXB",
        "outputId": "daa51001-42ed-4fb8-873b-95f449b0a10f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated scale_pos_weight for LightGBM: 94.69\n",
            "\n",
            "Starting LightGBM 5-Fold Cross-Validation (Evaluating with MCC)...\n",
            " Fold 1/5 \n",
            "Fold 1 LightGBM MCC (thresh=0.93428094): 0.57030\n",
            " Fold 2/5 \n",
            "Fold 2 LightGBM MCC (thresh=0.93428094): 0.57059\n",
            " Fold 3/5 \n",
            "Fold 3 LightGBM MCC (thresh=0.93428094): 0.54976\n",
            " Fold 4/5 \n",
            "Fold 4 LightGBM MCC (thresh=0.93428094): 0.53970\n",
            " Fold 5/5 \n",
            "Fold 5 LightGBM MCC (thresh=0.93428094): 0.55410\n",
            "\n",
            " Cross-Validation Summary \n",
            "Mean MCC (using threshold 0.93428094 in each fold): 0.55689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "gQkk_iZ4fj8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  XGBoost Cross-Validation Configuration\n",
        "xgb_n_splits = 5\n",
        "xgb_random_seed = 42\n",
        "\n",
        "# Setup Stratified K-Fold for XGBoost\n",
        "xgb_skf = StratifiedKFold(n_splits=xgb_n_splits, shuffle=True, random_state=xgb_random_seed)\n",
        "\n",
        "# Arrays to store XGBoost OOF (Out-of-Fold) predictions and MCC scores\n",
        "xgb_oof_preds_proba = np.zeros(len(X))  # Store OOF probabilities\n",
        "xgb_oof_mcc_scores = []  # Store MCC scores per fold\n",
        "\n",
        "#  XGBoost Cross-Validation Loop\n",
        "print(f\"\\nStarting XGBoost {xgb_n_splits}-Fold Cross-Validation (Evaluating with MCC)...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(xgb_skf.split(X, y)):\n",
        "    print(f\" Fold {fold+1}/{xgb_n_splits} \")\n",
        "    xgb_X_train, xgb_y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    xgb_X_val, xgb_y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Initialize XGBoost Model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=xgb_random_seed + fold,\n",
        "        n_jobs=-1,\n",
        "        early_stopping_rounds=100,  # Early stopping\n",
        "    )\n",
        "\n",
        "    # Train XGBoost Model\n",
        "    xgb_model.fit(xgb_X_train, xgb_y_train,\n",
        "                   eval_set=[(xgb_X_val, xgb_y_val)],\n",
        "                   verbose=False)\n",
        "\n",
        "    # Prediction on Validation Set (XGBoost)\n",
        "    xgb_val_preds_proba_raw = xgb_model.predict_proba(xgb_X_val)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    # Store XGBoost OOF probabilities\n",
        "    xgb_oof_preds_proba[val_idx] = xgb_val_preds_proba_raw\n",
        "\n",
        "    #  Calculate MCC for XGBoost\n",
        "    # Apply best threshold to probabilities\n",
        "    xgb_temp_threshold = 0.5\n",
        "    xgb_val_preds_binary = (xgb_val_preds_proba_raw >= xgb_temp_threshold).astype(int)\n",
        "    xgb_fold_mcc = matthews_corrcoef(xgb_y_val, xgb_val_preds_binary)\n",
        "    xgb_oof_mcc_scores.append(xgb_fold_mcc)\n",
        "    print(f\"Fold {fold+1} XGBoost MCC (thresh={xgb_temp_threshold}): {xgb_fold_mcc:.5f}\")\n",
        "\n",
        "    del xgb_X_train, xgb_y_train, xgb_X_val, xgb_y_val, xgb_model, xgb_val_preds_proba_raw, xgb_val_preds_binary\n",
        "    gc.collect()\n",
        "\n",
        "xgb_mean_mcc = np.mean(xgb_oof_mcc_scores)\n",
        "print(f\"\\n Cross-Validation Summary \")\n",
        "print(f\"Mean MCC (using threshold {xgb_temp_threshold} in each fold): {xgb_mean_mcc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVNgV-e1fm96",
        "outputId": "d8af71f9-ce01-4cc3-cb4a-73edfa96821b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting XGBoost 5-Fold Cross-Validation (Evaluating with MCC)...\n",
            " Fold 1/5 \n",
            "Fold 1 XGBoost MCC (thresh=0.5): 0.55600\n",
            " Fold 2/5 \n",
            "Fold 2 XGBoost MCC (thresh=0.5): 0.57116\n",
            " Fold 3/5 \n",
            "Fold 3 XGBoost MCC (thresh=0.5): 0.55719\n",
            " Fold 4/5 \n",
            "Fold 4 XGBoost MCC (thresh=0.5): 0.51174\n",
            " Fold 5/5 \n",
            "Fold 5 XGBoost MCC (thresh=0.5): 0.51407\n",
            "\n",
            " Cross-Validation Summary \n",
            "Mean MCC (using threshold 0.5 in each fold): 0.54203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost"
      ],
      "metadata": {
        "id": "hheJBS7F1lDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  CatBoost Cross-Validation Configuration\n",
        "cb_n_splits = 5  # Number of folds for cross-validation\n",
        "cb_random_seed = 42\n",
        "\n",
        "# Setup Stratified K-Fold for CatBoost\n",
        "cb_skf = StratifiedKFold(n_splits=cb_n_splits, shuffle=True, random_state=cb_random_seed)\n",
        "\n",
        "# Arrays to store CatBoost OOF (Out-of-Fold) predictions and MCC scores\n",
        "cb_oof_preds_proba = np.zeros(len(X))  # Store OOF probabilities\n",
        "cb_oof_mcc_scores = []  # Store MCC scores per fold\n",
        "\n",
        "#  CatBoost Cross-Validation Loop\n",
        "print(f\"\\nStarting CatBoost {cb_n_splits}-Fold Cross-Validation (Evaluating with MCC)...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(cb_skf.split(X, y)):\n",
        "    print(f\" Fold {fold+1}/{cb_n_splits} \")\n",
        "    cb_X_train, cb_y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    cb_X_val, cb_y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Initialize CatBoost Model\n",
        "    cb_model = cb.CatBoostClassifier(\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='AUC',\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        random_seed=cb_random_seed + fold,\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "        thread_count=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train CatBoost Model\n",
        "    cb_model.fit(cb_X_train, cb_y_train,\n",
        "                   eval_set=[(cb_X_val, cb_y_val)],\n",
        "                   )\n",
        "\n",
        "    # Prediction on Validation Set (CatBoost)\n",
        "    cb_val_preds_proba_raw = cb_model.predict_proba(cb_X_val)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    # Store CatBoost OOF probabilities\n",
        "    cb_oof_preds_proba[val_idx] = cb_val_preds_proba_raw\n",
        "\n",
        "    #  Calculate MCC for CatBoost\n",
        "    # Apply best threshold to probabilities\n",
        "    cb_temp_threshold = 0.5\n",
        "    cb_val_preds_binary = (cb_val_preds_proba_raw >= cb_temp_threshold).astype(int)\n",
        "    cb_fold_mcc = matthews_corrcoef(cb_y_val, cb_val_preds_binary)\n",
        "    cb_oof_mcc_scores.append(cb_fold_mcc)\n",
        "    print(f\"Fold {fold+1} CatBoost MCC (thresh={cb_temp_threshold}): {cb_fold_mcc:.5f}\")\n",
        "\n",
        "    del cb_X_train, cb_y_train, cb_X_val, cb_y_val, cb_model, cb_val_preds_proba_raw, cb_val_preds_binary\n",
        "    gc.collect()\n",
        "\n",
        "cb_mean_mcc = np.mean(cb_oof_mcc_scores)\n",
        "print(f\"\\n Cross-Validation Summary \")\n",
        "print(f\"Mean MCC (using threshold {cb_temp_threshold} in each fold): {cb_mean_mcc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7VD1j-j1mXz",
        "outputId": "07858c59-c239-4292-9c0a-9f8335756850"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting CatBoost 5-Fold Cross-Validation (Evaluating with MCC)...\n",
            " Fold 1/5 \n",
            "Fold 1 CatBoost MCC (thresh=0.5): 0.53255\n",
            " Fold 2/5 \n",
            "Fold 2 CatBoost MCC (thresh=0.5): 0.56677\n",
            " Fold 3/5 \n",
            "Fold 3 CatBoost MCC (thresh=0.5): 0.53775\n",
            " Fold 4/5 \n",
            "Fold 4 CatBoost MCC (thresh=0.5): 0.52009\n",
            " Fold 5/5 \n",
            "Fold 5 CatBoost MCC (thresh=0.5): 0.50184\n",
            "\n",
            " Cross-Validation Summary \n",
            "Mean MCC (using threshold 0.5 in each fold): 0.53180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Training & Prediction"
      ],
      "metadata": {
        "id": "hyoJjvAKNT1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LGBM"
      ],
      "metadata": {
        "id": "bcBOyll8ixHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining final model on the entire training data...\")\n",
        "\n",
        "final_lgbm = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    metric='none',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    max_depth=-1,\n",
        "    scale_pos_weight=lgbm_scale_pos_weight_value,\n",
        "    random_state=lgbm_random_seed,\n",
        "    n_jobs=-1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1\n",
        ")\n",
        "\n",
        "final_lgbm.fit(X, y)\n",
        "\n",
        "print(\"Predicting probabilities on the test set...\")\n",
        "\n",
        "# Dapatkan probabilitas mentah dari model\n",
        "lgbm_test_predictions_proba_raw = final_lgbm.predict_proba(X_test)[:, 1] # Probabilitas kelas 1\n",
        "\n",
        "print(\"Raw probability prediction finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XhM8lWfNVVn",
        "outputId": "eba9bfb3-e96a-4ed0-faa7-9ec68e0f5091"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model on the entire training data...\n",
            "Predicting probabilities on the test set...\n",
            "Raw probability prediction finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/gammafest25/models/final_lgbm.pkl', 'wb') as f:\n",
        "    pickle.dump(final_lgbm, f)"
      ],
      "metadata": {
        "id": "oIqXzv1t-yRN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XBGoost"
      ],
      "metadata": {
        "id": "oxyf3fZui0J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining final model on the entire training data...\")\n",
        "\n",
        "final_xgb = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=xgb_random_seed,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "final_xgb.fit(X, y)\n",
        "\n",
        "print(\"Predicting probabilities on the test set...\")\n",
        "\n",
        "# Get raw probabilities from the model\n",
        "xgb_test_predictions_proba_raw = final_xgb.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "print(\"Raw probability prediction finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoiearPWi1KT",
        "outputId": "ef40b08d-9bc6-4f73-9bdf-b3dfc8406c01"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model on the entire training data...\n",
            "Predicting probabilities on the test set...\n",
            "Raw probability prediction finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/gammafest25/models/final_xgb.pkl', 'wb') as f:\n",
        "    pickle.dump(final_xgb, f)"
      ],
      "metadata": {
        "id": "SQf96ERI_X1u"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost"
      ],
      "metadata": {
        "id": "hAL_7cDC1uet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining final model on the entire training data...\")\n",
        "\n",
        "final_cb = cb.CatBoostClassifier(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    random_seed=cb_random_seed,\n",
        "    verbose=False,\n",
        "    thread_count=-1,\n",
        ")\n",
        "\n",
        "final_cb.fit(X, y)\n",
        "\n",
        "print(\"Predicting probabilities on the test set...\")\n",
        "\n",
        "# Get raw probabilities from the model\n",
        "cb_test_predictions_proba_raw = final_cb.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "print(\"Raw probability prediction finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMNs4Ri91vdf",
        "outputId": "cb3a3895-2d62-4b8f-d102-0faebc332fd1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model on the entire training data...\n",
            "Predicting probabilities on the test set...\n",
            "Raw probability prediction finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/gammafest25/models/final_cb.pkl', 'wb') as f:\n",
        "    pickle.dump(final_cb, f)"
      ],
      "metadata": {
        "id": "IersP1CN_dOJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Prediction with MCC Threshold Tuning"
      ],
      "metadata": {
        "id": "9zYPs67GNYcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thresholding"
      ],
      "metadata": {
        "id": "e3T2qvMytqe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = np.linspace(0.01, 0.99, 300)"
      ],
      "metadata": {
        "id": "l5GXFnHbBMM7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LGBM Threshold"
      ],
      "metadata": {
        "id": "p9ojVOw7-kDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari Threshold Optimal berdasarkan MCC pada OOF\n",
        "print(\"\\nFinding best threshold based on OOF MCC score\")\n",
        "\n",
        "y_oof = y\n",
        "lgbm_mcc_scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    lgbm_oof_preds_binary = (lgbm_oof_preds_proba >= t).astype(int)\n",
        "    mcc = matthews_corrcoef(y_oof, lgbm_oof_preds_binary)\n",
        "    lgbm_mcc_scores.append(mcc)\n",
        "\n",
        "# Cari threshold terbaik\n",
        "lgbm_best_threshold_idx = np.argmax(lgbm_mcc_scores)\n",
        "lgbm_best_threshold_mcc = thresholds[lgbm_best_threshold_idx]\n",
        "lgbm_best_mcc_score = lgbm_mcc_scores[lgbm_best_threshold_idx]\n",
        "\n",
        "print(f\"Best Threshold based on OOF MCC: {lgbm_best_threshold_mcc:.8f}\")\n",
        "print(f\"Corresponding OOF MCC Score: {lgbm_best_mcc_score:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVDn7fiB-oP4",
        "outputId": "8fff6658-abc4-4b96-8169-9ce54b1d4b7a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding best threshold based on OOF MCC score\n",
            "Best Threshold based on OOF MCC: 0.94083612\n",
            "Corresponding OOF MCC Score: 0.55935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost Threshold"
      ],
      "metadata": {
        "id": "ZXGPtR7O_ScV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari Threshold Optimal berdasarkan MCC pada OOF\n",
        "print(\"\\nFinding best threshold based on OOF MCC score\")\n",
        "\n",
        "y_oof = y\n",
        "xgb_mcc_scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    xgb_oof_preds_binary = (xgb_oof_preds_proba >= t).astype(int)\n",
        "    mcc = matthews_corrcoef(y_oof, xgb_oof_preds_binary)\n",
        "    xgb_mcc_scores.append(mcc)\n",
        "\n",
        "# Cari threshold terbaik\n",
        "xgb_best_threshold_idx = np.argmax(xgb_mcc_scores)\n",
        "xgb_best_threshold_mcc = thresholds[xgb_best_threshold_idx]\n",
        "xgb_best_mcc_score = xgb_mcc_scores[xgb_best_threshold_idx]\n",
        "\n",
        "print(f\"Best Threshold based on OOF MCC: {xgb_best_threshold_mcc:.8f}\")\n",
        "print(f\"Corresponding OOF MCC Score: {xgb_best_mcc_score:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXsC3fjA_VLp",
        "outputId": "fcb564a1-2f0d-4349-ba3b-6779022f2371"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding best threshold based on OOF MCC score\n",
            "Best Threshold based on OOF MCC: 0.19354515\n",
            "Corresponding OOF MCC Score: 0.57256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CatBoost Threshold"
      ],
      "metadata": {
        "id": "50ZTUieQ_YcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari Threshold Optimal berdasarkan MCC pada OOF\n",
        "print(\"\\nFinding best threshold based on OOF MCC score\")\n",
        "\n",
        "y_oof = y\n",
        "cb_mcc_scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    cb_oof_preds_binary = (cb_oof_preds_proba >= t).astype(int)\n",
        "    mcc = matthews_corrcoef(y_oof, cb_oof_preds_binary)\n",
        "    cb_mcc_scores.append(mcc)\n",
        "\n",
        "# Cari threshold terbaik\n",
        "cb_best_threshold_idx = np.argmax(cb_mcc_scores)\n",
        "cb_best_threshold_mcc = thresholds[cb_best_threshold_idx]\n",
        "cb_best_mcc_score = cb_mcc_scores[cb_best_threshold_idx]\n",
        "\n",
        "print(f\"Best Threshold based on OOF MCC: {cb_best_threshold_mcc:.8f}\")\n",
        "print(f\"Corresponding OOF MCC Score: {cb_best_mcc_score:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv56vzIx_aQa",
        "outputId": "e9850de7-3434-4b60-9180-f48c053c9f11"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding best threshold based on OOF MCC score\n",
            "Best Threshold based on OOF MCC: 0.21648829\n",
            "Corresponding OOF MCC Score: 0.57133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weighted Thresholding"
      ],
      "metadata": {
        "id": "KCHiN0FkA4j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari Threshold Optimal berdasarkan MCC pada OOF\n",
        "print(\"\\nFinding best threshold based on OOF MCC score\")\n",
        "\n",
        "y_oof = y\n",
        "\n",
        "# Define weights based on MCC scores (higher MCC gets higher weight)\n",
        "lgbm_weight = lgbm_best_mcc_score  # Weight for LGBM predictions\n",
        "xgb_weight = xgb_best_mcc_score  # Weight for XGBoost predictions\n",
        "cb_weight = cb_best_mcc_score  # Weight for CatBoost predictions\n",
        "\n",
        "# Normalize weights to sum to 1\n",
        "total_weight = lgbm_weight + xgb_weight + cb_weight\n",
        "lgbm_weight /= total_weight\n",
        "xgb_weight /= total_weight\n",
        "cb_weight /= total_weight\n",
        "\n",
        "# Aggregate predictions using weighted average\n",
        "oof_preds_proba = (lgbm_weight * lgbm_oof_preds_proba +\n",
        "                    xgb_weight * xgb_oof_preds_proba +\n",
        "                    cb_weight * cb_oof_preds_proba)\n",
        "\n",
        "mcc_scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    oof_preds_binary = (oof_preds_proba >= t).astype(int)\n",
        "    mcc = matthews_corrcoef(y_oof, oof_preds_binary)\n",
        "    mcc_scores.append(mcc)\n",
        "\n",
        "# Cari threshold terbaik\n",
        "best_threshold_idx = np.argmax(mcc_scores)\n",
        "best_threshold_mcc = thresholds[best_threshold_idx]\n",
        "best_mcc_score = mcc_scores[best_threshold_idx]\n",
        "\n",
        "print(f\"Best Threshold based on OOF MCC: {best_threshold_mcc:.8f}\")\n",
        "print(f\"Corresponding OOF MCC Score: {best_mcc_score:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9TfBZb7NZl4",
        "outputId": "1e398b07-580f-4c36-bf62-1991b7a85798"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding best threshold based on OOF MCC score\n",
            "Best Threshold based on OOF MCC: 0.47869565\n",
            "Corresponding OOF MCC Score: 0.57632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot MCC vs Threshold (Optional) - menggunakan oof_preds_proba yang sudah diproses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(thresholds, mcc_scores, marker='.')\n",
        "plt.title('MCC Score vs. Prediction Threshold on OOF Data')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('MCC Score')\n",
        "plt.vlines(best_threshold_mcc, plt.ylim()[0], plt.ylim()[1], color='red', linestyle='--', label=f'Best Threshold ({best_threshold_mcc:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "gjcSzKtvNcWc",
        "outputId": "d3c71352-0f39-41c0-b57d-4d6c10acbc38"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "vwrvY9zutnSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi biner pada data training menggunakan best threshold (untuk confusion matrix)\n",
        "oof_preds_binary_best_thresh = (oof_preds_proba >= best_threshold_mcc).astype(int)\n",
        "\n",
        "# Buat Confusion Matrix\n",
        "cm = confusion_matrix(y, oof_preds_binary_best_thresh) # y adalah target asli\n",
        "\n",
        "# Hitung nilai-nilai\n",
        "accuracy = accuracy_score(y, oof_preds_binary_best_thresh)\n",
        "precision = precision_score(y, oof_preds_binary_best_thresh)\n",
        "recall = recall_score(y, oof_preds_binary_best_thresh)\n",
        "f1 = f1_score(y, oof_preds_binary_best_thresh)\n",
        "mcc = matthews_corrcoef(y, oof_preds_binary_best_thresh)\n",
        "\n",
        "# Tampilkan Confusion Matrix dan nilai-nilai\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"MCC:\", mcc)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title(\"Confusion Matrix (using best threshold on OOF data)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "ciw2c2NhpBcs",
        "outputId": "a0672e18-11ab-44f5-f540-ad9d28e6c2a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9910224475335471\n",
            "Precision: 0.5672072872694957\n",
            "Recall: 0.5948275862068966\n",
            "F1-Score: 0.580689184578642\n",
            "MCC: 0.5763207902901686\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### False Positives & Negatives"
      ],
      "metadata": {
        "id": "J6TtBepPs_ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get incorrectly predicted indices\n",
        "incorrect_indices = np.where(y != (oof_preds_proba >= best_threshold_mcc).astype(int))[0]\n",
        "\n",
        "# Separate false positives and false negatives\n",
        "false_positives = incorrect_indices[np.where(y[incorrect_indices] == 0)[0]]\n",
        "false_negatives = incorrect_indices[np.where(y[incorrect_indices] == 1)[0]]\n",
        "\n",
        "# Visualize the oof_preds_proba distribution for false positives and false negatives\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(oof_preds_proba[false_positives], label='False Positives', color='red')\n",
        "sns.histplot(oof_preds_proba[false_negatives], label='False Negatives', color='blue')\n",
        "plt.xlabel('OOF Predicted Probabilities')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of OOF Probabilities for Incorrect Predictions')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "zlXA4XvSZACl",
        "outputId": "fa570514-36fe-4660-ebc6-f85ad2451150"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUBMISSION"
      ],
      "metadata": {
        "id": "u-6A5esioq2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Membuat File Submission\n",
        "\n",
        "# Terapkan Threshold Terbaik pada Probabilitas Test\n",
        "print(f\"Applying best threshold ({best_threshold_mcc:.4f}) to test predictions...\")\n",
        "test_predictions_proba_raw = (lgbm_weight * lgbm_test_predictions_proba_raw +\n",
        "                                xgb_weight * xgb_test_predictions_proba_raw +\n",
        "                                cb_weight * cb_test_predictions_proba_raw)\n",
        "\n",
        "\n",
        "test_predictions_binary = (test_predictions_proba_raw >= best_threshold_mcc).astype(int)\n",
        "\n",
        "\n",
        "# Buat DataFrame submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_merged['id'], # Ambil ID dari test_merged\n",
        "    'is_referenced': test_predictions_binary\n",
        "})\n",
        "\n",
        "# Cek hasil submission\n",
        "print(\"\\n Submission File Head \")\n",
        "display(submission_df.head())\n",
        "print(\"\\n Submission Value Counts \")\n",
        "submission_df['is_referenced'].value_counts()"
      ],
      "metadata": {
        "id": "6RpNDCmvuddQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "066c777e-6d62-4eea-8f05-053b802029b9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying best threshold (0.4787) to test predictions...\n",
            "\n",
            " Submission File Head \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id  is_referenced\n",
              "0   0              0\n",
              "1   1              0\n",
              "2   2              0\n",
              "3   3              0\n",
              "4   4              0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95048b4b-5156-45ff-baa1-a568b4d73240\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>is_referenced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95048b4b-5156-45ff-baa1-a568b4d73240')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95048b4b-5156-45ff-baa1-a568b4d73240 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95048b4b-5156-45ff-baa1-a568b4d73240');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ad3ef85-93e2-44f7-91dd-bf72caedf7da\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ad3ef85-93e2-44f7-91dd-bf72caedf7da')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ad3ef85-93e2-44f7-91dd-bf72caedf7da button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"submission_df['is_referenced']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_referenced\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Submission Value Counts \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "is_referenced\n",
              "0    332785\n",
              "1      3236\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_referenced</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3236</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan ke CSV\n",
        "output_filename = f'submission.csv'\n",
        "submission_df.to_csv(output_filename, index=False)\n",
        "print(f\"\\nSubmission file saved as {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbF5xsNr_WLm",
        "outputId": "c13bef66-3471-4372-f7f1-766ff1578574"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv(f'{base_path}/{output_filename}', index=False)"
      ],
      "metadata": {
        "id": "njubrRRy_c_t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKPs0_i7A0Yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}